[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using R with Databricks",
    "section": "",
    "text": "Using R with Databricks",
    "crumbs": [
      "Using R with Databricks"
    ]
  },
  {
    "objectID": "index.html#positconf2024",
    "href": "index.html#positconf2024",
    "title": "Using R with Databricks",
    "section": "posit::conf2024",
    "text": "posit::conf2024\n\n\n\nPresentation Slides\n\n\n\n\n\n\n\n\n\n\nWorkbook\n\n\n\n\n\n\n\n\n\n\nGitHub Repository",
    "crumbs": [
      "Using R with Databricks"
    ]
  },
  {
    "objectID": "connecting.html",
    "href": "connecting.html",
    "title": "1  Connecting and interacting",
    "section": "",
    "text": "1.1 Connect to a database",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "connecting.html#connect-to-a-database",
    "href": "connecting.html#connect-to-a-database",
    "title": "1  Connecting and interacting",
    "section": "",
    "text": "Click on the Connections tab\nClick on the New Connection button\nSelect warehouse\nClick OK",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "connecting.html#explore-the-database-using-the-rstudio-ide",
    "href": "connecting.html#explore-the-database-using-the-rstudio-ide",
    "title": "1  Connecting and interacting",
    "section": "1.2 Explore the database using the RStudio IDE",
    "text": "1.2 Explore the database using the RStudio IDE\n\nExpand the hive_metastore catalog\nExpand the defaults schema\nExpand the cars table\nClick on the table icon to the right of the cars table\n(Optional) Expand and explore the other tables\nClick on the disconnect icon to close the connection",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "connecting.html#connecting-using-odbcodbc",
    "href": "connecting.html#connecting-using-odbcodbc",
    "title": "1  Connecting and interacting",
    "section": "1.3 Connecting using odbc::odbc()",
    "text": "1.3 Connecting using odbc::odbc()\nhttps://solutions.posit.co/connections/db/databases/databricks/#using-the-odbcodbc-function\n\nUse the following code to start a new connection\n\n\nlibrary(DBI)\n\ncon &lt;- dbConnect(\n  odbc::odbc(),\n  Driver = \"/Library/simba/spark/lib/libsparkodbc_sb64-universal.dylib\",\n  Host = Sys.getenv(\"DATABRICKS_HOST\"),\n  Port = 443,\n  AuthMech = 3,\n  HTTPPath = \"/sql/1.0/warehouses/300bd24ba12adf8e\",\n  Protocol = \"https\",\n  ThriftTransport = 2,\n  SSL = 1,\n  UID = \"token\",\n  PWD = Sys.getenv(\"DATABRICKS_TOKEN\")\n)\n\n\nDisconnect from the database using dbDisconnect()\n\n\ndbDisconnect(con)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "connecting.html#connecting-using-odbcdatabricks",
    "href": "connecting.html#connecting-using-odbcdatabricks",
    "title": "1  Connecting and interacting",
    "section": "1.4 Connecting using odbc::databricks()",
    "text": "1.4 Connecting using odbc::databricks()\nhttps://solutions.posit.co/connections/db/databases/databricks/#using-the-new-odbcdatabricks-function\n\nUse the newer odbc::databricks() as the driver, which automatically sets the defaults for you. Also, it will automatically look for the Databricks Host and Token, so you won’t to specify it in the code.\n\n\ncon &lt;- dbConnect(\n  odbc::databricks(),\n  HTTPPath = \"/sql/1.0/warehouses/300bd24ba12adf8e\"\n)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "connecting.html#interact-with-database",
    "href": "connecting.html#interact-with-database",
    "title": "1  Connecting and interacting",
    "section": "1.5 Interact with database",
    "text": "1.5 Interact with database\n\nUse dbListTables() to retrieve a list of tables\n\n\ndbListTables(con)[1:4]\n#&gt; [1] \"allergies\"   \"cars\"        \"conditions\"  \"covid_stats\"\n\n\nUse dbListFields() to get the names of the fields of a given table\n\n\ndbListFields(con, \"cars\")\n#&gt;  [1] \"row_names\" \"mpg\"       \"cyl\"       \"disp\"      \"hp\"        \"drat\"     \n#&gt;  [7] \"wt\"        \"qsec\"      \"vs\"        \"am\"        \"gear\"      \"carb\"\n\n\nUse dbGetQuery() to run a quick query\n\n\ndbGetQuery(con, \"SELECT * FROM cars LIMIT 5\")\n#&gt;           row_names  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "connecting.html#using-knitr-chunks",
    "href": "connecting.html#using-knitr-chunks",
    "title": "1  Connecting and interacting",
    "section": "1.6 Using knitr chunks",
    "text": "1.6 Using knitr chunks\n\nUse the SQL chunk\n\n\nSELECT * FROM cars LIMIT 5\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrow_names\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\nUse the output.var option to load results to a variable\n\n\nSELECT * FROM cars LIMIT 5\n\n\nTest the variable\n\n\nsql_top5\n#&gt;           row_names  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; 1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; 2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; 3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; 4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; 5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n\nDisconnect from the database using dbDisconnect()\n\n\ndbDisconnect(con)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "connecting.html#rstudio-sql-script",
    "href": "connecting.html#rstudio-sql-script",
    "title": "1  Connecting and interacting",
    "section": "1.7 RStudio SQL Script",
    "text": "1.7 RStudio SQL Script\nTry out the new SQL Script support in RStudio\n\nOpen the example-connecting.sql file\nClick the Preview button. It is located in the top-right area of the script\nIn the script, change customer to orders\nClick on Preview again",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting and interacting</span>"
    ]
  },
  {
    "objectID": "remote-processing.html",
    "href": "remote-processing.html",
    "title": "2  Remote processing",
    "section": "",
    "text": "2.1 Create a table variable\nBasics to how to point a variable in R to a table or view inside the database\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\ncon &lt;- dbConnect(\n  odbc::databricks(),\n  HTTPPath = \"/sql/1.0/warehouses/300bd24ba12adf8e\"\n)\ndbGetQuery(con, \"select * from diamonds\")\ntbl(con, \"diamonds\")\n#&gt; # Source:   table&lt;`diamonds`&gt; [?? x 11]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    `_c0` carat cut       color clarity depth table price     x     y     z\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1     1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt;  2     2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt;  3     3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt;  4     4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt;  5     5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt;  6     6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt;  7     7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n#&gt;  8     8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n#&gt;  9     9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n#&gt; 10    10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n#&gt; # ℹ more rows\ntbl_diamonds &lt;- tbl(con, \"diamonds\")\ntbl_diamonds\n#&gt; # Source:   table&lt;`diamonds`&gt; [?? x 11]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    `_c0` carat cut       color clarity depth table price     x     y     z\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1     1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt;  2     2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt;  3     3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt;  4     4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt;  5     5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt;  6     6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt;  7     7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n#&gt;  8     8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n#&gt;  9     9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n#&gt; 10    10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n#&gt; # ℹ more rows\ntbl_diamonds |&gt; \n  count()\n#&gt; # Source:   SQL [1 x 1]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;         n\n#&gt;   &lt;int64&gt;\n#&gt; 1   53940\ntbl_diamonds |&gt; \n  count(cut)\n#&gt; # Source:   SQL [5 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   cut             n\n#&gt;   &lt;chr&gt;     &lt;int64&gt;\n#&gt; 1 Ideal       21551\n#&gt; 2 Premium     13791\n#&gt; 3 Very Good   12082\n#&gt; 4 Good         4906\n#&gt; 5 Fair         1610\ntbl_diamonds |&gt; \n  count(cut) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT `cut`, COUNT(*) AS `n`\n#&gt; FROM `diamonds`\n#&gt; GROUP BY `cut`",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote processing</span>"
    ]
  },
  {
    "objectID": "remote-processing.html#create-a-table-variable",
    "href": "remote-processing.html#create-a-table-variable",
    "title": "2  Remote processing",
    "section": "",
    "text": "Load the dplyr, DBI and dbplyr libraries\n\n\n\n(Optional) Open a connection to the database if it’s currently closed\n\n\n\nUsing dbGetQuery() create a query to pull the diamonds table\n\n\n\nUse the tbl() to perform the same\n\n\n\nLoad the reference, not the table data, into a variable\n\n\n\nCall the variable to see preview the data in the table\n\n\n\nAdd count() to easily get the number of rows\n\n\n\nAdd cut as an argument to count() to see the count by that field\n\n\n\nAdd show_query() to see the how dplyr translates your code to SQL",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote processing</span>"
    ]
  },
  {
    "objectID": "remote-processing.html#easily-aggretate-data",
    "href": "remote-processing.html#easily-aggretate-data",
    "title": "2  Remote processing",
    "section": "2.2 Easily aggretate data",
    "text": "2.2 Easily aggretate data\nAn example of how we can use the same code against a local R data frame and a remote table\n\nUsing dplyr, get the average price for each cut, and sort it by the average for diamonds, from the ggplot2 package\n\n\nggplot2::diamonds |&gt; \n  group_by(cut) |&gt; \n  summarise(avg_price = mean(price, na.rm = TRUE)) |&gt; \n  arrange(desc(avg_price))\n#&gt; # A tibble: 5 × 2\n#&gt;   cut       avg_price\n#&gt;   &lt;ord&gt;         &lt;dbl&gt;\n#&gt; 1 Premium       4584.\n#&gt; 2 Fair          4359.\n#&gt; 3 Very Good     3982.\n#&gt; 4 Good          3929.\n#&gt; 5 Ideal         3458.\n\n\nUse tbl_diamonds to perform the exact same operation\n\n\ntbl_diamonds |&gt; \n  group_by(cut) |&gt; \n  summarise(avg_price = mean(price, na.rm = TRUE)) |&gt; \n  arrange(desc(avg_price))\n#&gt; # Source:     SQL [5 x 2]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: desc(avg_price)\n#&gt;   cut       avg_price\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 Premium       4584.\n#&gt; 2 Fair          4359.\n#&gt; 3 Very Good     3982.\n#&gt; 4 Good          3929.\n#&gt; 5 Ideal         3458.\n\n\nLoad code into a variable named price_by_cut\n\n\nprice_by_cut &lt;- tbl_diamonds |&gt; \n  group_by(cut) |&gt; \n  summarise(avg_price = mean(price, na.rm = TRUE)) |&gt; \n  arrange(desc(avg_price))\n\n\nCall price_by_cut\n\n\nprice_by_cut\n#&gt; # Source:     SQL [5 x 2]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: desc(avg_price)\n#&gt;   cut       avg_price\n#&gt;   &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 Premium       4584.\n#&gt; 2 Fair          4359.\n#&gt; 3 Very Good     3982.\n#&gt; 4 Good          3929.\n#&gt; 5 Ideal         3458.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote processing</span>"
    ]
  },
  {
    "objectID": "remote-processing.html#data-collection",
    "href": "remote-processing.html#data-collection",
    "title": "2  Remote processing",
    "section": "2.3 Data collection",
    "text": "2.3 Data collection\nUnderstand the difference between printing and collecting\n\nCall the tbl_diamonds variable directly\n\n\ntbl_diamonds\n#&gt; # Source:   table&lt;`diamonds`&gt; [?? x 11]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    `_c0` carat cut       color clarity depth table price     x     y     z\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1     1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt;  2     2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt;  3     3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt;  4     4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt;  5     5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt;  6     6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt;  7     7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n#&gt;  8     8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n#&gt;  9     9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n#&gt; 10    10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n#&gt; # ℹ more rows\n\n\nCall tbl_diamonds via print().\n\n\nprint(tbl_diamonds)\n#&gt; # Source:   table&lt;`diamonds`&gt; [?? x 11]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    `_c0` carat cut       color clarity depth table price     x     y     z\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1     1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt;  2     2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt;  3     3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt;  4     4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt;  5     5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt;  6     6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt;  7     7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n#&gt;  8     8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n#&gt;  9     9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n#&gt; 10    10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n#&gt; # ℹ more rows\n\n\nCall tbl_diamonds via collect(). Notice it prints the entire table\n\n\ncollect(tbl_diamonds)\n#&gt; # A tibble: 53,940 × 11\n#&gt;    `_c0` carat cut       color clarity depth table price     x     y     z\n#&gt;    &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1     1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#&gt;  2     2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#&gt;  3     3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#&gt;  4     4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#&gt;  5     5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#&gt;  6     6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n#&gt;  7     7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n#&gt;  8     8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n#&gt;  9     9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n#&gt; 10    10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n#&gt; # ℹ 53,930 more rows\n\n\nLoad the previous operation to a variable named local_diamonds\n\n\nlocal_diamonds &lt;- collect(tbl_diamonds)\n\n\nUse pull() to extract the values from price only\n\n\ntbl_diamonds |&gt; \n  pull(price)\n\n\nLoad the previous operation to a variable named price. Notice that this time, the variable is a vector, not a data frame.\n\n\nprice &lt;- tbl_diamonds |&gt; \n  pull(price)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote processing</span>"
    ]
  },
  {
    "objectID": "remote-processing.html#referring-to-non-default-catalog-tables",
    "href": "remote-processing.html#referring-to-non-default-catalog-tables",
    "title": "2  Remote processing",
    "section": "2.4 Referring to non-default catalog tables",
    "text": "2.4 Referring to non-default catalog tables\nUsing I() to create non-default table references\n\nCreate a reference to the customer table, which is under the tpch schema, inside the samples catalog\n\n\ntbl(con, I(\"workshops.tpch.customer\"))\n#&gt; # Source:   table&lt;workshops.tpch.customer&gt; [?? x 8]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    c_custkey c_name         c_address c_nationkey c_phone c_acctbal c_mktsegment\n#&gt;        &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       \n#&gt;  1         1 Customer#0000… IVhzIApe…          15 25-989…      712. BUILDING    \n#&gt;  2         2 Customer#0000… XSTf4,NC…          13 23-768…      122. AUTOMOBILE  \n#&gt;  3         3 Customer#0000… MG9kdTD2…           1 11-719…     7498. AUTOMOBILE  \n#&gt;  4         4 Customer#0000… XxVSJsLA…           4 14-128…     2867. MACHINERY   \n#&gt;  5         5 Customer#0000… KvpyuHCp…           3 13-750…      794. HOUSEHOLD   \n#&gt;  6         6 Customer#0000… sKZz0Csn…          20 30-114…     7639. AUTOMOBILE  \n#&gt;  7         7 Customer#0000… TcGe5gaZ…          18 28-190…     9562. AUTOMOBILE  \n#&gt;  8         8 Customer#0000… I0B10bB0…          17 27-147…     6820. BUILDING    \n#&gt;  9         9 Customer#0000… xKiAFTjU…           8 18-338…     8324. FURNITURE   \n#&gt; 10        10 Customer#0000… 6LrEaV6K…           5 15-741…     2754. HOUSEHOLD   \n#&gt; # ℹ more rows\n#&gt; # ℹ 1 more variable: c_comment &lt;chr&gt;\n\n\nCreate a reference to the sales_order_in_la table, under the ferit schema\n\n\ntbl(con, I(\"ferit.sales_order_in_la\"))\n#&gt; # Source:   table&lt;ferit.sales_order_in_la&gt; [?? x 7]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    order_date city        customer_id customer_name      currency sales quantity\n#&gt;    &lt;date&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;              &lt;chr&gt;    &lt;int&gt;  &lt;int64&gt;\n#&gt;  1 2019-09-27 Los Angeles 14120673    DEL MONICO,  DOMI… USD       1166        2\n#&gt;  2 2019-08-30 Los Angeles 12593702    SAUTKUS,  LAURA    USD       1250        8\n#&gt;  3 2019-11-12 Los Angeles 14159223    ROBERTS,  ALAN E   USD         98        2\n#&gt;  4 2019-10-15 Los Angeles 13005776    SAUTKUS,  LAURA    USD        731        4\n#&gt;  5 2019-08-23 Los Angeles 14501552    osborn paper co    USD       2180        3\n#&gt;  6 2019-09-03 Los Angeles 13265811    RYCHTANEK,  NICOLE USD         72        3\n#&gt;  7 2019-10-16 Los Angeles 12947406    epikos church      USD       2529        3\n#&gt;  8 2019-08-08 Los Angeles 15171127    GORZEN,  WALDEMAR… USD       1207        6\n#&gt;  9 2019-08-16 Los Angeles 14065091    VARGAS,  DAVID     USD        349        2\n#&gt; 10 2019-10-08 Los Angeles 13999146    SEGOVIA,  VICTOR M USD       2092        5\n#&gt; # ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Remote processing</span>"
    ]
  },
  {
    "objectID": "preparing-exploring.html",
    "href": "preparing-exploring.html",
    "title": "3  Preparing and exploring Data",
    "section": "",
    "text": "Catch up\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\n\ncon &lt;- dbConnect(\n  odbc::databricks(),\n  HTTPPath = \"/sql/1.0/warehouses/300bd24ba12adf8e\"\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing and exploring Data</span>"
    ]
  },
  {
    "objectID": "preparing-exploring.html#selecting-variables",
    "href": "preparing-exploring.html#selecting-variables",
    "title": "3  Preparing and exploring Data",
    "section": "3.1 Selecting variables",
    "text": "3.1 Selecting variables\nSimple strategies to order, and reduce, data to work with\n\nLoad the customer table to a variable called customer\n\n\ncustomer &lt;- tbl(con, I(\"workshops.tpch.customer\"))\n\n\nSelect all columns that end with “key”\n\n\ncustomer |&gt; \n  select(ends_with(\"key\"))\n#&gt; # Source:   SQL [?? x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    c_custkey c_nationkey\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1         1          15\n#&gt;  2         2          13\n#&gt;  3         3           1\n#&gt;  4         4           4\n#&gt;  5         5           3\n#&gt;  6         6          20\n#&gt;  7         7          18\n#&gt;  8         8          17\n#&gt;  9         9           8\n#&gt; 10        10           5\n#&gt; # ℹ more rows\n\n\nMove all columns that end with “key” to the front\n\n\ncustomer |&gt; \n  select(ends_with(\"key\"), everything())\n#&gt; # Source:   SQL [?? x 8]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    c_custkey c_nationkey c_name         c_address c_phone c_acctbal c_mktsegment\n#&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       \n#&gt;  1         1          15 Customer#0000… IVhzIApe… 25-989…      712. BUILDING    \n#&gt;  2         2          13 Customer#0000… XSTf4,NC… 23-768…      122. AUTOMOBILE  \n#&gt;  3         3           1 Customer#0000… MG9kdTD2… 11-719…     7498. AUTOMOBILE  \n#&gt;  4         4           4 Customer#0000… XxVSJsLA… 14-128…     2867. MACHINERY   \n#&gt;  5         5           3 Customer#0000… KvpyuHCp… 13-750…      794. HOUSEHOLD   \n#&gt;  6         6          20 Customer#0000… sKZz0Csn… 30-114…     7639. AUTOMOBILE  \n#&gt;  7         7          18 Customer#0000… TcGe5gaZ… 28-190…     9562. AUTOMOBILE  \n#&gt;  8         8          17 Customer#0000… I0B10bB0… 27-147…     6820. BUILDING    \n#&gt;  9         9           8 Customer#0000… xKiAFTjU… 18-338…     8324. FURNITURE   \n#&gt; 10        10           5 Customer#0000… 6LrEaV6K… 15-741…     2754. HOUSEHOLD   \n#&gt; # ℹ more rows\n#&gt; # ℹ 1 more variable: c_comment &lt;chr&gt;\n\n\nSelect all columns that do not end with “key”\n\n\ncustomer |&gt; \n  select(-ends_with(\"key\"))\n#&gt; # Source:   SQL [?? x 6]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    c_name             c_address         c_phone c_acctbal c_mktsegment c_comment\n#&gt;    &lt;chr&gt;              &lt;chr&gt;             &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;    \n#&gt;  1 Customer#000000001 IVhzIApeRb ot,c,E 25-989…      712. BUILDING     \"to the …\n#&gt;  2 Customer#000000002 XSTf4,NCwDVaWNe6… 23-768…      122. AUTOMOBILE   \"l accou…\n#&gt;  3 Customer#000000003 MG9kdTD2WBHm      11-719…     7498. AUTOMOBILE   \" deposi…\n#&gt;  4 Customer#000000004 XxVSJsLAGtn       14-128…     2867. MACHINERY    \" reques…\n#&gt;  5 Customer#000000005 KvpyuHCplrB84WgA… 13-750…      794. HOUSEHOLD    \"n accou…\n#&gt;  6 Customer#000000006 sKZz0CsnMD7mp4Xd… 30-114…     7639. AUTOMOBILE   \"tions. …\n#&gt;  7 Customer#000000007 TcGe5gaZNgVePxU5… 28-190…     9562. AUTOMOBILE   \"ainst t…\n#&gt;  8 Customer#000000008 I0B10bB0AymmC, 0… 27-147…     6820. BUILDING     \"among t…\n#&gt;  9 Customer#000000009 xKiAFTjUsCuxfele… 18-338…     8324. FURNITURE    \"r theod…\n#&gt; 10 Customer#000000010 6LrEaV6KR6PLVcgl… 15-741…     2754. HOUSEHOLD    \"es regu…\n#&gt; # ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing and exploring Data</span>"
    ]
  },
  {
    "objectID": "preparing-exploring.html#join-to-tables",
    "href": "preparing-exploring.html#join-to-tables",
    "title": "3  Preparing and exploring Data",
    "section": "3.2 Join to tables",
    "text": "3.2 Join to tables\nUsing left_join() to relate two tables\n\nLoad the nation table into a variable called the same\n\n\nnation &lt;- tbl(con, I(\"workshops.tpch.nation\"))\n\n\nUse left_join to relate customer with nation using the nation key\n\n\ncustomer |&gt; \n  left_join(nation, by = c(\"c_nationkey\" = \"n_nationkey\"))\n#&gt; # Source:   SQL [?? x 11]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    c_custkey c_name         c_address c_nationkey c_phone c_acctbal c_mktsegment\n#&gt;        &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;       \n#&gt;  1         1 Customer#0000… IVhzIApe…          15 25-989…      712. BUILDING    \n#&gt;  2         2 Customer#0000… XSTf4,NC…          13 23-768…      122. AUTOMOBILE  \n#&gt;  3         3 Customer#0000… MG9kdTD2…           1 11-719…     7498. AUTOMOBILE  \n#&gt;  4         4 Customer#0000… XxVSJsLA…           4 14-128…     2867. MACHINERY   \n#&gt;  5         5 Customer#0000… KvpyuHCp…           3 13-750…      794. HOUSEHOLD   \n#&gt;  6         6 Customer#0000… sKZz0Csn…          20 30-114…     7639. AUTOMOBILE  \n#&gt;  7         7 Customer#0000… TcGe5gaZ…          18 28-190…     9562. AUTOMOBILE  \n#&gt;  8         8 Customer#0000… I0B10bB0…          17 27-147…     6820. BUILDING    \n#&gt;  9         9 Customer#0000… xKiAFTjU…           8 18-338…     8324. FURNITURE   \n#&gt; 10        10 Customer#0000… 6LrEaV6K…           5 15-741…     2754. HOUSEHOLD   \n#&gt; # ℹ more rows\n#&gt; # ℹ 4 more variables: c_comment &lt;chr&gt;, n_name &lt;chr&gt;, n_regionkey &lt;dbl&gt;,\n#&gt; #   n_comment &lt;chr&gt;\n\n\nWhat are the 5 countries with the most customers?\n\n\ncustomer |&gt; \n  left_join(nation, by = c(\"c_nationkey\" = \"n_nationkey\")) |&gt; \n  count(n_name, sort = TRUE) |&gt; \n  head(5)\n#&gt; # Source:     SQL [5 x 2]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: desc(n)\n#&gt;   n_name              n\n#&gt;   &lt;chr&gt;         &lt;int64&gt;\n#&gt; 1 UNITED STATES    1260\n#&gt; 2 IRAQ             1249\n#&gt; 3 IRAN             1248\n#&gt; 4 CHINA            1244\n#&gt; 5 EGYPT            1225",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing and exploring Data</span>"
    ]
  },
  {
    "objectID": "preparing-exploring.html#prepare-base",
    "href": "preparing-exploring.html#prepare-base",
    "title": "3  Preparing and exploring Data",
    "section": "3.3 Prepare base",
    "text": "3.3 Prepare base\nBuilding the base variable/query\n\nLoad the orders table in a variable called orders\n\n\norders &lt;- tbl(con, I(\"workshops.tpch.orders\"))\n\n\nJoin orders to the customer variable (table). Relate them on the o_custkey and c_custkey fields.\n\n\norders |&gt; \n  left_join(customer, by = c(\"o_custkey\" = \"c_custkey\"))\n#&gt; # Source:   SQL [?? x 16]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    o_orderkey o_custkey o_orderstatus o_totalprice o_orderdate order_priority \n#&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;date&gt;      &lt;chr&gt;          \n#&gt;  1          1      7381 O                  181585. 1996-01-02  5-LOW          \n#&gt;  2          2     15601 O                   46094. 1996-12-01  1-URGENT       \n#&gt;  3          3     24664 F                  271423. 1993-10-14  5-LOW          \n#&gt;  4          4     27356 O                   47915. 1995-10-11  5-LOW          \n#&gt;  5          5      8897 F                  136702. 1994-07-30  5-LOW          \n#&gt;  6          6     11125 F                   65071. 1992-02-21  4-NOT SPECIFIED\n#&gt;  7          7      7828 O                  263865. 1996-01-10  2-HIGH         \n#&gt;  8         32     26012 O                  153480. 1995-07-16  2-HIGH         \n#&gt;  9         33     13393 F                  138123. 1993-10-27  3-MEDIUM       \n#&gt; 10         34     12202 O                   72249. 1998-07-21  3-MEDIUM       \n#&gt; # ℹ more rows\n#&gt; # ℹ 10 more variables: o_clerk &lt;chr&gt;, o_shippriority &lt;dbl&gt;, o_comment &lt;chr&gt;,\n#&gt; #   c_name &lt;chr&gt;, c_address &lt;chr&gt;, c_nationkey &lt;dbl&gt;, c_phone &lt;chr&gt;,\n#&gt; #   c_acctbal &lt;dbl&gt;, c_mktsegment &lt;chr&gt;, c_comment &lt;chr&gt;\n\n\nJoin the nation variable/table to the orders and customer variables. Use the c_nationkey and the n_nationkey to relate them.\n\n\norders |&gt; \n  left_join(customer, by = c(\"o_custkey\" = \"c_custkey\")) |&gt; \n  left_join(nation, by = c(\"c_nationkey\" = \"n_nationkey\"))\n#&gt; # Source:   SQL [?? x 19]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    o_orderkey o_custkey o_orderstatus o_totalprice o_orderdate order_priority \n#&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;date&gt;      &lt;chr&gt;          \n#&gt;  1          1      7381 O                  181585. 1996-01-02  5-LOW          \n#&gt;  2          2     15601 O                   46094. 1996-12-01  1-URGENT       \n#&gt;  3          3     24664 F                  271423. 1993-10-14  5-LOW          \n#&gt;  4          4     27356 O                   47915. 1995-10-11  5-LOW          \n#&gt;  5          5      8897 F                  136702. 1994-07-30  5-LOW          \n#&gt;  6          6     11125 F                   65071. 1992-02-21  4-NOT SPECIFIED\n#&gt;  7          7      7828 O                  263865. 1996-01-10  2-HIGH         \n#&gt;  8         32     26012 O                  153480. 1995-07-16  2-HIGH         \n#&gt;  9         33     13393 F                  138123. 1993-10-27  3-MEDIUM       \n#&gt; 10         34     12202 O                   72249. 1998-07-21  3-MEDIUM       \n#&gt; # ℹ more rows\n#&gt; # ℹ 13 more variables: o_clerk &lt;chr&gt;, o_shippriority &lt;dbl&gt;, o_comment &lt;chr&gt;,\n#&gt; #   c_name &lt;chr&gt;, c_address &lt;chr&gt;, c_nationkey &lt;dbl&gt;, c_phone &lt;chr&gt;,\n#&gt; #   c_acctbal &lt;dbl&gt;, c_mktsegment &lt;chr&gt;, c_comment &lt;chr&gt;, n_name &lt;chr&gt;,\n#&gt; #   n_regionkey &lt;dbl&gt;, n_comment &lt;chr&gt;\n\n\nLoad the resulting code into a variable called rel_orders. We do this so to get autocomplete working\n\n\nrel_orders &lt;- orders |&gt; \n  left_join(customer, by = c(\"o_custkey\" = \"c_custkey\")) |&gt; \n  left_join(nation, by = c(\"c_nationkey\" = \"n_nationkey\"))\n\n\nCreate new columns for the year of the order date, and another for the month of the order date. Name them order_year and order_month respectively.\n\n\nrel_orders |&gt; \n  mutate(order_year = year(o_orderdate), order_month = month(o_orderdate))\n#&gt; # Source:   SQL [?? x 21]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    o_orderkey o_custkey o_orderstatus o_totalprice o_orderdate order_priority \n#&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;date&gt;      &lt;chr&gt;          \n#&gt;  1          1      7381 O                  181585. 1996-01-02  5-LOW          \n#&gt;  2          2     15601 O                   46094. 1996-12-01  1-URGENT       \n#&gt;  3          3     24664 F                  271423. 1993-10-14  5-LOW          \n#&gt;  4          4     27356 O                   47915. 1995-10-11  5-LOW          \n#&gt;  5          5      8897 F                  136702. 1994-07-30  5-LOW          \n#&gt;  6          6     11125 F                   65071. 1992-02-21  4-NOT SPECIFIED\n#&gt;  7          7      7828 O                  263865. 1996-01-10  2-HIGH         \n#&gt;  8         32     26012 O                  153480. 1995-07-16  2-HIGH         \n#&gt;  9         33     13393 F                  138123. 1993-10-27  3-MEDIUM       \n#&gt; 10         34     12202 O                   72249. 1998-07-21  3-MEDIUM       \n#&gt; # ℹ more rows\n#&gt; # ℹ 15 more variables: o_clerk &lt;chr&gt;, o_shippriority &lt;dbl&gt;, o_comment &lt;chr&gt;,\n#&gt; #   c_name &lt;chr&gt;, c_address &lt;chr&gt;, c_nationkey &lt;dbl&gt;, c_phone &lt;chr&gt;,\n#&gt; #   c_acctbal &lt;dbl&gt;, c_mktsegment &lt;chr&gt;, c_comment &lt;chr&gt;, n_name &lt;chr&gt;,\n#&gt; #   n_regionkey &lt;dbl&gt;, n_comment &lt;chr&gt;, order_year &lt;int&gt;, order_month &lt;int&gt;\n\n\nRemove any columns that end in “comment”, and end in “key”\n\n\nrel_orders |&gt; \n  mutate(order_year = year(o_orderdate), order_month = month(o_orderdate)) |&gt; \n  select(-ends_with(\"comment\"), -ends_with(\"key\")) \n#&gt; # Source:   SQL [?? x 14]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    o_orderstatus o_totalprice o_orderdate order_priority  o_clerk o_shippriority\n#&gt;    &lt;chr&gt;                &lt;dbl&gt; &lt;date&gt;      &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;\n#&gt;  1 O                  181585. 1996-01-02  5-LOW           Clerk#…              0\n#&gt;  2 O                   46094. 1996-12-01  1-URGENT        Clerk#…              0\n#&gt;  3 F                  271423. 1993-10-14  5-LOW           Clerk#…              0\n#&gt;  4 O                   47915. 1995-10-11  5-LOW           Clerk#…              0\n#&gt;  5 F                  136702. 1994-07-30  5-LOW           Clerk#…              0\n#&gt;  6 F                   65071. 1992-02-21  4-NOT SPECIFIED Clerk#…              0\n#&gt;  7 O                  263865. 1996-01-10  2-HIGH          Clerk#…              0\n#&gt;  8 O                  153480. 1995-07-16  2-HIGH          Clerk#…              0\n#&gt;  9 F                  138123. 1993-10-27  3-MEDIUM        Clerk#…              0\n#&gt; 10 O                   72249. 1998-07-21  3-MEDIUM        Clerk#…              0\n#&gt; # ℹ more rows\n#&gt; # ℹ 8 more variables: c_name &lt;chr&gt;, c_address &lt;chr&gt;, c_phone &lt;chr&gt;,\n#&gt; #   c_acctbal &lt;dbl&gt;, c_mktsegment &lt;chr&gt;, n_name &lt;chr&gt;, order_year &lt;int&gt;,\n#&gt; #   order_month &lt;int&gt;\n\n\nRename o_custkey to customer, insert code before the selection\n\n\nrel_orders |&gt; \n  mutate(order_year = year(o_orderdate), order_month = month(o_orderdate)) |&gt; \n  rename(customer = o_custkey) |&gt; \n  select(-ends_with(\"comment\"), -ends_with(\"key\")) \n#&gt; # Source:   SQL [?? x 15]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;    customer o_orderstatus o_totalprice o_orderdate order_priority  o_clerk      \n#&gt;       &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;date&gt;      &lt;chr&gt;           &lt;chr&gt;        \n#&gt;  1     7381 O                  181585. 1996-01-02  5-LOW           Clerk#000000…\n#&gt;  2    15601 O                   46094. 1996-12-01  1-URGENT        Clerk#000000…\n#&gt;  3    24664 F                  271423. 1993-10-14  5-LOW           Clerk#000000…\n#&gt;  4    27356 O                   47915. 1995-10-11  5-LOW           Clerk#000000…\n#&gt;  5     8897 F                  136702. 1994-07-30  5-LOW           Clerk#000000…\n#&gt;  6    11125 F                   65071. 1992-02-21  4-NOT SPECIFIED Clerk#000000…\n#&gt;  7     7828 O                  263865. 1996-01-10  2-HIGH          Clerk#000000…\n#&gt;  8    26012 O                  153480. 1995-07-16  2-HIGH          Clerk#000000…\n#&gt;  9    13393 F                  138123. 1993-10-27  3-MEDIUM        Clerk#000000…\n#&gt; 10    12202 O                   72249. 1998-07-21  3-MEDIUM        Clerk#000000…\n#&gt; # ℹ more rows\n#&gt; # ℹ 9 more variables: o_shippriority &lt;dbl&gt;, c_name &lt;chr&gt;, c_address &lt;chr&gt;,\n#&gt; #   c_phone &lt;chr&gt;, c_acctbal &lt;dbl&gt;, c_mktsegment &lt;chr&gt;, n_name &lt;chr&gt;,\n#&gt; #   order_year &lt;int&gt;, order_month &lt;int&gt;\n\n\nLoad resulting code to a variable called prep_orders\n\n\nprep_orders &lt;- rel_orders |&gt; \n  mutate(order_year = year(o_orderdate), order_month = month(o_orderdate)) |&gt; \n  rename(customer = o_custkey) |&gt; \n  select(-ends_with(\"comment\"), -ends_with(\"key\")) \n\n\nPreview prep_orders using glimpse()\n\n\nprep_orders |&gt; \n  glimpse()\n#&gt; Rows: ??\n#&gt; Columns: 15\n#&gt; Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; $ customer       &lt;dbl&gt; 7381, 15601, 24664, 27356, 8897, 11125, 7828, 26012, 13…\n#&gt; $ o_orderstatus  &lt;chr&gt; \"O\", \"O\", \"F\", \"O\", \"F\", \"F\", \"O\", \"O\", \"F\", \"O\", \"O\", …\n#&gt; $ o_totalprice   &lt;dbl&gt; 181585.13, 46093.67, 271422.96, 47915.12, 136701.72, 65…\n#&gt; $ o_orderdate    &lt;date&gt; 1996-01-02, 1996-12-01, 1993-10-14, 1995-10-11, 1994-0…\n#&gt; $ order_priority &lt;chr&gt; \"5-LOW\", \"1-URGENT\", \"5-LOW\", \"5-LOW\", \"5-LOW\", \"4-NOT …\n#&gt; $ o_clerk        &lt;chr&gt; \"Clerk#000000951\", \"Clerk#000000880\", \"Clerk#000000955\"…\n#&gt; $ o_shippriority &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#&gt; $ c_name         &lt;chr&gt; \"Customer#000007381\", \"Customer#000015601\", \"Customer#0…\n#&gt; $ c_address      &lt;chr&gt; \"uAT93aEOFo7IUvh BMFgRm48hsr7LtPfQJ\", \"T2ALSWGjgRFWLxP4…\n#&gt; $ c_phone        &lt;chr&gt; \"30-666-139-1602\", \"32-397-926-3405\", \"14-383-701-6221\"…\n#&gt; $ c_acctbal      &lt;dbl&gt; 73.39, 7589.86, 570.97, -254.76, 2725.15, 2998.55, 3706…\n#&gt; $ c_mktsegment   &lt;chr&gt; \"BUILDING\", \"HOUSEHOLD\", \"FURNITURE\", \"MACHINERY\", \"BUI…\n#&gt; $ n_name         &lt;chr&gt; \"SAUDI ARABIA\", \"RUSSIA\", \"EGYPT\", \"ETHIOPIA\", \"ARGENTI…\n#&gt; $ order_year     &lt;int&gt; 1996, 1996, 1993, 1995, 1994, 1992, 1996, 1995, 1993, 1…\n#&gt; $ order_month    &lt;int&gt; 1, 12, 10, 10, 7, 2, 1, 7, 10, 7, 10, 11, 6, 8, 9, 7, 3…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing and exploring Data</span>"
    ]
  },
  {
    "objectID": "preparing-exploring.html#answering-questions",
    "href": "preparing-exploring.html#answering-questions",
    "title": "3  Preparing and exploring Data",
    "section": "3.4 Answering questions",
    "text": "3.4 Answering questions\nUsing the base query to answer more complex questions\n\nWhat are the top 5 countries for total amount ordered?\n\n\nprep_orders |&gt; \n  group_by(n_name) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(total_price)) |&gt; \n  head(5)\n#&gt; # Source:     SQL [5 x 2]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: desc(total_price)\n#&gt;   n_name        total_price\n#&gt;   &lt;chr&gt;               &lt;dbl&gt;\n#&gt; 1 CHINA         1801806199.\n#&gt; 2 UNITED STATES 1774171034.\n#&gt; 3 EGYPT         1771552951.\n#&gt; 4 IRAN          1771537297.\n#&gt; 5 VIETNAM       1771445652.\n\n\nWhat are the top 5 countries for total amount ordered for 1998?\n\n\nprep_orders |&gt; \n  filter(order_year == 1998) |&gt; \n  group_by(n_name) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt; \n  arrange(desc(total_price)) |&gt; \n  head(5)\n#&gt; # Source:     SQL [5 x 2]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: desc(total_price)\n#&gt;   n_name         total_price\n#&gt;   &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1 CHINA           171553459.\n#&gt; 2 UNITED STATES   171380901.\n#&gt; 3 UNITED KINGDOM  162354578.\n#&gt; 4 IRAN            160671344.\n#&gt; 5 ARGENTINA       160164705.\n\n\nWhat has been the top (1) country, in orders, by year?\n\n\nprep_orders |&gt; \n  group_by(n_name, order_year) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt; \n  group_by(order_year) |&gt; \n  filter(total_price == max(total_price))\n#&gt; # Source:   SQL [7 x 3]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Groups:   order_year\n#&gt;   n_name         order_year total_price\n#&gt;   &lt;chr&gt;               &lt;int&gt;       &lt;dbl&gt;\n#&gt; 1 UNITED KINGDOM       1992  274006542.\n#&gt; 2 VIETNAM              1993  272855530.\n#&gt; 3 INDONESIA            1994  286622681.\n#&gt; 4 EGYPT                1995  281068319.\n#&gt; 5 VIETNAM              1996  281019062.\n#&gt; 6 UNITED STATES        1997  276025595.\n#&gt; 7 CHINA                1998  171553459.\n\n\nWho are the top 5 customers by amount ordered?\n\n\nprep_orders |&gt; \n  group_by(customer) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n    ) |&gt; \n  arrange(desc(total_price)) |&gt; \n  head(5)\n#&gt; # Source:     SQL [5 x 2]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: desc(total_price)\n#&gt;   customer total_price\n#&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1      691    6311210.\n#&gt; 2    25936    5917968.\n#&gt; 3    20959    5884257.\n#&gt; 4     4351    5830651.\n#&gt; 5      979    5820310.\n\n\nWhat is the country, and market segment, of the top 5 customers by amount ordered?\n\n\nprep_orders |&gt; \n  group_by(customer) |&gt; \n  summarise(\n    country = first(n_name), \n    segment = first(c_mktsegment),\n    total_price = sum(o_totalprice, na.rm = TRUE)\n    ) |&gt; \n  arrange(desc(total_price)) |&gt; \n  head(5)\n#&gt; # Source:     SQL [5 x 4]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: desc(total_price)\n#&gt;   customer country       segment    total_price\n#&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;\n#&gt; 1      691 MOZAMBIQUE    MACHINERY     6311210.\n#&gt; 2    25936 UNITED STATES HOUSEHOLD     5917968.\n#&gt; 3    20959 GERMANY       BUILDING      5884257.\n#&gt; 4     4351 IRAN          AUTOMOBILE    5830651.\n#&gt; 5      979 CHINA         MACHINERY     5820310.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing and exploring Data</span>"
    ]
  },
  {
    "objectID": "visualizations.html",
    "href": "visualizations.html",
    "title": "4  Visualizations",
    "section": "",
    "text": "Catch up\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\n\ncon &lt;- dbConnect(\n  odbc::databricks(),\n  HTTPPath = \"/sql/1.0/warehouses/300bd24ba12adf8e\"\n)\n\norders &lt;- tbl(con, I(\"workshops.tpch.orders\"))\ncustomers &lt;- tbl(con, I(\"workshops.tpch.customer\"))\nnation &lt;- tbl(con, I(\"workshops.tpch.nation\"))\n\nprep_orders &lt;- orders |&gt; \n  left_join(customers, by = c(\"o_custkey\" = \"c_custkey\")) |&gt; \n  left_join(nation, by = c(\"c_nationkey\" = \"n_nationkey\")) |&gt; \n  mutate(\n    order_year = year(o_orderdate), \n    order_month = month(o_orderdate)\n  ) |&gt; \n  rename(customer = o_custkey) |&gt; \n  select(-ends_with(\"comment\"),  -ends_with(\"key\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#auto-collect",
    "href": "visualizations.html#auto-collect",
    "title": "4  Visualizations",
    "section": "4.1 Auto-collect",
    "text": "4.1 Auto-collect\nSee how ggplot2 auto-collects data before plotting\n\nLoad ggplot2\n\n\nlibrary(ggplot2)\n\n\nPlot the n_name over n_region_key from the nation table. Use the column geom.\n\n\nnation |&gt; \n  ggplot() +\n  geom_col(aes(n_name, n_regionkey))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#plot-data",
    "href": "visualizations.html#plot-data",
    "title": "4  Visualizations",
    "section": "4.2 Plot data",
    "text": "4.2 Plot data\n\nUsing prep_order, pull the total sales by year (o_totalprice)\n\n\nprep_orders |&gt; \n  group_by(order_year) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt; \n  arrange(order_year)\n#&gt; # Source:     SQL [7 x 2]\n#&gt; # Database:   Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt; # Ordered by: order_year\n#&gt;   order_year total_price\n#&gt;        &lt;int&gt;       &lt;dbl&gt;\n#&gt; 1       1992 6543025198.\n#&gt; 2       1993 6444226635.\n#&gt; 3       1994 6554756505.\n#&gt; 4       1995 6568883526.\n#&gt; 5       1996 6514961386.\n#&gt; 6       1997 6470760974.\n#&gt; 7       1998 3871541964.\n\n\nAdd to the code, a step to plot the data. Use a column geom\n\n\nprep_orders |&gt; \n  group_by(order_year) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt; \n  arrange(order_year) |&gt; \n  ggplot() +\n  geom_col(aes(order_year, total_price)) \n\n\n\n\n\n\n\n\n\nDownload the results to R to a variable called sales_by_year\n\n\nsales_by_year &lt;- prep_orders |&gt; \n  group_by(order_year) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt; \n  collect()\n\n\nPreview sales_by_year\n\n\nsales_by_year\n#&gt; # A tibble: 7 × 2\n#&gt;   order_year total_price\n#&gt;        &lt;int&gt;       &lt;dbl&gt;\n#&gt; 1       1994 6554756505.\n#&gt; 2       1997 6470760974.\n#&gt; 3       1995 6568883526.\n#&gt; 4       1992 6543025198.\n#&gt; 5       1993 6444226635.\n#&gt; 6       1996 6514961386.\n#&gt; 7       1998 3871541964.\n\n\nUse sales_by_year to create the same plot\n\n\nsales_by_year |&gt; \n  ggplot() +\n  geom_col(aes(order_year, total_price)) \n\n\n\n\n\n\n\n\n\nAn example of what multiple iterations of the plot would result in\n\n\nbreaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n\nbreaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\nsales_by_year |&gt; \n  ggplot() +\n  geom_col(aes(order_year, total_price)) +\n  scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n  scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n  xlab(\"Year\") +\n  ylab(\"Total Sales\") +\n  labs(title = \"Sales by year\") +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#plot-data-by-country",
    "href": "visualizations.html#plot-data-by-country",
    "title": "4  Visualizations",
    "section": "4.3 Plot data by country",
    "text": "4.3 Plot data by country\n\nCreate a variable called country, with the value “FRANCE”\n\n\ncountry &lt;- \"FRANCE\"\n\n\nModify sales_by_year, by adding a filter step to have the n_name match the value of country\n\n\nsales_by_year &lt;- prep_orders |&gt; \n  filter(n_name == country) |&gt; \n  group_by(order_year) |&gt; \n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt; \n  collect()\n\n\nCopy and use the same code from the finalized plot. Add a subtitle with the value of country\n\n\nbreaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n\nbreaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\nsales_by_year |&gt; \n  ggplot() +\n  geom_col(aes(order_year, total_price)) +\n  scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n  scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n  xlab(\"Year\") +\n  ylab(\"Total Sales\") +\n  labs(title = \"Sales by year\", subtitle = country) +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#plot-data-by-month",
    "href": "visualizations.html#plot-data-by-month",
    "title": "4  Visualizations",
    "section": "4.4 Plot data by month",
    "text": "4.4 Plot data by month\n\nCreate a new variable called year, load it with the value of 1998\n\n\nyear &lt;- 1998\n\n\nUsing the same structure, create a new variable called sales_by_month. In addition to country, the filter should include the order_year. Group by order_month\n\n\nsales_by_month &lt;- prep_orders |&gt;\n  filter(n_name == country, order_year == year) |&gt;\n  group_by(order_month) |&gt;\n  summarise(\n    total_price = sum(o_totalprice, na.rm = TRUE)\n  ) |&gt;\n  collect()\n\n\nCreate the same finalized plot, but using sales_by_month. Make sure to update the the axis, and aesthetics. Also, expand the subtitle to include the year as well.\n\n\nbreaks &lt;- as.double(quantile(c(0, max(sales_by_month$total_price))))\nbreaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\nsales_by_month |&gt;\n  ggplot() +\n  geom_col(aes(order_month, total_price)) +\n  scale_x_continuous(breaks = unique(sales_by_month$order_month)) +\n  scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n  xlab(\"Year\") +\n  ylab(\"Total Sales\") +\n  labs(title = \"Sales by month\", subtitle = paste0(country, \" - \", year)) +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "databricks.html",
    "href": "databricks.html",
    "title": "5  Databricks Connect",
    "section": "",
    "text": "5.1 Connect to Databricks Connect cluster\nCopy a simple table into the Spark session\nlibrary(sparklyr)\nlibrary(dplyr)\nsc &lt;- spark_connect(method = \"databricks_connect\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Databricks Connect</span>"
    ]
  },
  {
    "objectID": "databricks.html#uploading-data-from-r",
    "href": "databricks.html#uploading-data-from-r",
    "title": "5  Databricks Connect",
    "section": "5.2 Uploading data from R",
    "text": "5.2 Uploading data from R\n\nLoad the nycflights13 library\n\n\nlibrary(nycflights13)\n\n\nUse copy_to() to upload planes to your Spark session. Assign to a variable called tbl_planes\n\n\ntbl_planes &lt;- copy_to(sc, planes)\n\n\nUse glimpse() to preview the data\n\n\ntbl_planes |&gt; \n  glimpse()\n#&gt; Rows: ??\n#&gt; Columns: 9\n#&gt; Database: spark_connection\n#&gt; $ tailnum      &lt;chr&gt; \"N10156\", \"N102UW\", \"N103US\", \"N104UW\", \"N10575\", \"N105UW…\n#&gt; $ year         &lt;int&gt; 2004, 1998, 1999, 1999, 2002, 1999, 1999, 1999, 1999, 199…\n#&gt; $ type         &lt;chr&gt; \"Fixed wing multi engine\", \"Fixed wing multi engine\", \"Fi…\n#&gt; $ manufacturer &lt;chr&gt; \"EMBRAER\", \"AIRBUS INDUSTRIE\", \"AIRBUS INDUSTRIE\", \"AIRBU…\n#&gt; $ model        &lt;chr&gt; \"EMB-145XR\", \"A320-214\", \"A320-214\", \"A320-214\", \"EMB-145…\n#&gt; $ engines      &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n#&gt; $ seats        &lt;int&gt; 55, 182, 182, 182, 55, 182, 182, 182, 182, 182, 55, 55, 5…\n#&gt; $ speed        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n#&gt; $ engine       &lt;chr&gt; \"Turbo-fan\", \"Turbo-fan\", \"Turbo-fan\", \"Turbo-fan\", \"Turb…\n\n\nUse show_query() to see how Spark refers to the data you just uploaded\n\n\ntbl_planes |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM `planes`",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Databricks Connect</span>"
    ]
  },
  {
    "objectID": "databricks.html#caching-data",
    "href": "databricks.html#caching-data",
    "title": "5  Databricks Connect",
    "section": "5.3 Caching data",
    "text": "5.3 Caching data\nCreate a table from elements of another table\n\nUsing tbl(), create a reference to the “diamonds” table. Assign it to a variable called tbl_diamonds\n\n\ntbl_diamonds &lt;- tbl(sc, \"diamonds\")\n\n\nSelect the “cut”, “color”, “clarity” and “price” fields from tbl_diamonds and assign to a variable called tbl_temp\n\n\ntbl_temp &lt;- tbl_diamonds |&gt; \n  select(cut, color, clarity, price)\n\n\nUse show_query() to see the underlying query of tbl_temp\n\n\ntbl_temp |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT `cut`, `color`, `clarity`, `price`\n#&gt; FROM `diamonds`\n\n\nPass tbl_temp to compute(), and reassign the operation to tbl_temp\n\n\ntbl_temp &lt;- tbl_temp |&gt; \n  compute()\n\n\nUse show_query() again to see the new underlying query of tbl_temp\n\n\ntbl_temp  |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM `table_2079291d_ef80_4ad4_bb2c_7f8d9dc942ef`\n\n\nPreview tbl_temp\n\n\ntbl_temp \n#&gt; # Source:   table&lt;`table_2079291d_ef80_4ad4_bb2c_7f8d9dc942ef`&gt; [?? x 4]\n#&gt; # Database: spark_connection\n#&gt;    cut       color clarity price\n#&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n#&gt;  1 Ideal     E     SI2       326\n#&gt;  2 Premium   E     SI1       326\n#&gt;  3 Good      E     VS1       327\n#&gt;  4 Premium   I     VS2       334\n#&gt;  5 Good      J     SI2       335\n#&gt;  6 Very Good J     VVS2      336\n#&gt;  7 Very Good I     VVS1      336\n#&gt;  8 Very Good H     SI1       337\n#&gt;  9 Fair      E     VS2       337\n#&gt; 10 Very Good H     VS1       338\n#&gt; # ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Databricks Connect</span>"
    ]
  },
  {
    "objectID": "databricks.html#reading-files",
    "href": "databricks.html#reading-files",
    "title": "5  Databricks Connect",
    "section": "5.4 Reading files",
    "text": "5.4 Reading files\nUpload a CSV file that is located inside Databricks, into your Spark session\n\nUse spark_read_csv() to upload the airports CSV file to Spark. The path should be /Volumes/workshops/nycflights/2013/airports.csv, and name should be airports_csv. Assign it to a variable called tbl_airports\n\n\ntbl_airports &lt;- spark_read_csv(\n  sc = sc,\n  name = \"airports_csv\",\n  path = \"/Volumes/workshops/nycflights/2013/airports.csv\"\n  )\n\n\nPreview tbl_airports\n\n\ntbl_airports\n#&gt; # Source:   table&lt;`airports_csv`&gt; [?? x 8]\n#&gt; # Database: spark_connection\n#&gt;    faa   name                             lat    lon   alt    tz dst   tzone    \n#&gt;    &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    \n#&gt;  1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n#&gt;  2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n#&gt;  3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n#&gt;  4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n#&gt;  5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n#&gt;  6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n#&gt;  7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n#&gt;  8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n#&gt;  9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n#&gt; 10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…\n#&gt; # ℹ more rows\n\n\nPass tbl_airports to show_query() to see the underlying query\n\n\ntbl_airports |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM `airports_csv`\n\n\nUse sdf_sql() to access the top 10 rows of “airports_csv”\n\n\nsdf_sql(sc, \"select * from airports_csv limit 10\")\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_7edd58f9_2813_4a80_a191_0ddf254d4e05`&gt; [10 x 8]\n#&gt; # Database: spark_connection\n#&gt;    faa   name                             lat    lon   alt    tz dst   tzone    \n#&gt;    &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    \n#&gt;  1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n#&gt;  2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n#&gt;  3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n#&gt;  4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n#&gt;  5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n#&gt;  6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n#&gt;  7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n#&gt;  8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n#&gt;  9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n#&gt; 10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Databricks Connect</span>"
    ]
  },
  {
    "objectID": "databricks.html#mapping-files",
    "href": "databricks.html#mapping-files",
    "title": "5  Databricks Connect",
    "section": "5.5 “Mapping” files",
    "text": "5.5 “Mapping” files\nMap the flights data without caching the data\n\nUse spark_read_csv() to upload the airports CSV file to Spark. The path should be /Volumes/workshops/nycflights/2013/nycflights.csv, and name should be mapped. Important, make sure to set memory to false. Assign it to a variable called flights_mapped\n\n\nflights_mapped &lt;- spark_read_csv(\n  sc = sc,\n  name = \"mapped\",\n  path = \"/Volumes/workshops/nycflights/2013/nycflights.csv\",\n  memory = FALSE\n  )\n\n\nUse glimpse() to preview the data from flights_mapped\n\n\nflights_mapped |&gt; \n  glimpse()\n#&gt; Rows: ??\n#&gt; Columns: 16\n#&gt; Database: spark_connection\n#&gt; $ year      &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, …\n#&gt; $ month     &lt;int&gt; 6, 5, 12, 5, 7, 1, 12, 8, 9, 4, 6, 11, 4, 3, 10, 1, 2, 8, 10…\n#&gt; $ day       &lt;int&gt; 30, 7, 8, 14, 21, 1, 9, 13, 26, 30, 17, 22, 26, 25, 21, 23, …\n#&gt; $ dep_time  &lt;int&gt; 940, 1657, 859, 1841, 1102, 1817, 1259, 1920, 725, 1323, 940…\n#&gt; $ dep_delay &lt;int&gt; 15, -3, -1, -4, -3, -3, 14, 85, -10, 62, 5, 5, -2, 115, -4, …\n#&gt; $ arr_time  &lt;int&gt; 1216, 2104, 1238, 2122, 1230, 2008, 1617, 2032, 1027, 1549, …\n#&gt; $ arr_delay &lt;int&gt; -4, 10, 11, -34, -8, 3, 22, 71, -8, 60, -4, -2, 22, 91, -6, …\n#&gt; $ carrier   &lt;chr&gt; \"VX\", \"DL\", \"DL\", \"DL\", \"9E\", \"AA\", \"WN\", \"B6\", \"AA\", \"EV\", …\n#&gt; $ tailnum   &lt;chr&gt; \"N626VA\", \"N3760C\", \"N712TW\", \"N914DL\", \"N823AY\", \"N3AXAA\", …\n#&gt; $ flight    &lt;int&gt; 407, 329, 422, 2391, 3652, 353, 1428, 1407, 2279, 4162, 20, …\n#&gt; $ origin    &lt;chr&gt; \"JFK\", \"JFK\", \"JFK\", \"JFK\", \"LGA\", \"LGA\", \"EWR\", \"JFK\", \"LGA…\n#&gt; $ dest      &lt;chr&gt; \"LAX\", \"SJU\", \"LAX\", \"TPA\", \"ORF\", \"ORD\", \"HOU\", \"IAD\", \"MIA…\n#&gt; $ air_time  &lt;int&gt; 313, 216, 376, 135, 50, 138, 240, 48, 148, 110, 50, 161, 87,…\n#&gt; $ distance  &lt;int&gt; 2475, 1598, 2475, 1005, 296, 733, 1411, 228, 1096, 820, 264,…\n#&gt; $ hour      &lt;int&gt; 9, 16, 8, 18, 11, 18, 12, 19, 7, 13, 9, 13, 8, 20, 12, 20, 6…\n#&gt; $ minute    &lt;int&gt; 40, 57, 59, 41, 2, 17, 59, 20, 25, 23, 40, 20, 9, 54, 17, 24…\n\n\nRun a quick count by the carrier field against flights_mapped\n\n\nflights_mapped |&gt; \n  count(carrier)\n#&gt; # Source:   SQL [?? x 2]\n#&gt; # Database: spark_connection\n#&gt;    carrier     n\n#&gt;    &lt;chr&gt;   &lt;dbl&gt;\n#&gt;  1 UA       5770\n#&gt;  2 AA       3188\n#&gt;  3 EV       5142\n#&gt;  4 B6       5376\n#&gt;  5 DL       4751\n#&gt;  6 OO          3\n#&gt;  7 F9         69\n#&gt;  8 YV         53\n#&gt;  9 US       2015\n#&gt; 10 MQ       2507\n#&gt; # ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Databricks Connect</span>"
    ]
  },
  {
    "objectID": "databricks.html#partial-cache",
    "href": "databricks.html#partial-cache",
    "title": "5  Databricks Connect",
    "section": "5.6 Partial cache",
    "text": "5.6 Partial cache\nCache a section of the flights data into Spark\n\nFilter flights_mapped to only show flights destined to Orlando (code “ORD”)\n\n\nflights_mapped |&gt; \n  filter(dest == \"ORD\")\n#&gt; # Source:   SQL [?? x 16]\n#&gt; # Database: spark_connection\n#&gt;     year month   day dep_time dep_delay arr_time arr_delay carrier tailnum\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  \n#&gt;  1  2013     1     1     1817        -3     2008         3 AA      N3AXAA \n#&gt;  2  2013    10    21      859        -1     1036        11 UA      N57852 \n#&gt;  3  2013     9    18      933        -7     1037       -43 AA      N514AA \n#&gt;  4  2013     7    25      752        -3      917         2 MQ      N6EAMQ \n#&gt;  5  2013     7     1     2007        82     2123        53 AA      N547AA \n#&gt;  6  2013     9    29     2003        -2     2108       -22 MQ      N509MQ \n#&gt;  7  2013     4    15      625        -5      752       -13 AA      N589AA \n#&gt;  8  2013     8    24     1017        12     1113        -7 MQ      N517MQ \n#&gt;  9  2013     9    20     1948       -12     2112       -20 UA      N814UA \n#&gt; 10  2013     1    25     1814        54     2020        65 AA      N3CXAA \n#&gt; # ℹ more rows\n#&gt; # ℹ 7 more variables: flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;,\n#&gt; #   distance &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;\n\n\nAssign the previous operation to a variable called flights_ord\n\n\nflights_ord &lt;- flights_mapped |&gt; \n  filter(dest == \"ORD\")\n\n\nPass flights_ord to compute(), and reassign to flights_ord\n\n\nflights_ord &lt;- flights_ord |&gt; \n  compute()\n\n\nPreview flights_ord\n\n\nflights_ord\n#&gt; # Source:   table&lt;`table_6c908dd8_b420_4046_959b_b74d79da8e42`&gt; [?? x 16]\n#&gt; # Database: spark_connection\n#&gt;     year month   day dep_time dep_delay arr_time arr_delay carrier tailnum\n#&gt;    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;    &lt;int&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  \n#&gt;  1  2013     1     1     1817        -3     2008         3 AA      N3AXAA \n#&gt;  2  2013    10    21      859        -1     1036        11 UA      N57852 \n#&gt;  3  2013     9    18      933        -7     1037       -43 AA      N514AA \n#&gt;  4  2013     7    25      752        -3      917         2 MQ      N6EAMQ \n#&gt;  5  2013     7     1     2007        82     2123        53 AA      N547AA \n#&gt;  6  2013     9    29     2003        -2     2108       -22 MQ      N509MQ \n#&gt;  7  2013     4    15      625        -5      752       -13 AA      N589AA \n#&gt;  8  2013     8    24     1017        12     1113        -7 MQ      N517MQ \n#&gt;  9  2013     9    20     1948       -12     2112       -20 UA      N814UA \n#&gt; 10  2013     1    25     1814        54     2020        65 AA      N3CXAA \n#&gt; # ℹ more rows\n#&gt; # ℹ 7 more variables: flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;,\n#&gt; #   distance &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;\n\n\nPass flights_ord to show_query() to see the new query\n\n\nflights_ord |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM `table_6c908dd8_b420_4046_959b_b74d79da8e42`",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Databricks Connect</span>"
    ]
  },
  {
    "objectID": "databricks.html#end-game",
    "href": "databricks.html#end-game",
    "title": "5  Databricks Connect",
    "section": "5.7 End game",
    "text": "5.7 End game\nPrepare a working data set using several of the techniques covered in this unit\n\nFrom tbl_airports, select “faa” and “name”. Rename the variables to “dest” and “dest_name” respectively. Assign it to a variable called dest_airports\n\n\ndest_airports &lt;- tbl_airports |&gt; \n  select(dest = faa, dest_name = name)\n\n\nPreview dest_airports\n\n\ndest_airports\n#&gt; # Source:   SQL [?? x 2]\n#&gt; # Database: spark_connection\n#&gt;    dest  dest_name                     \n#&gt;    &lt;chr&gt; &lt;chr&gt;                         \n#&gt;  1 04G   Lansdowne Airport             \n#&gt;  2 06A   Moton Field Municipal Airport \n#&gt;  3 06C   Schaumburg Regional           \n#&gt;  4 06N   Randall Airport               \n#&gt;  5 09J   Jekyll Island Airport         \n#&gt;  6 0A9   Elizabethton Municipal Airport\n#&gt;  7 0G6   Williams County Airport       \n#&gt;  8 0G7   Finger Lakes Regional Airport \n#&gt;  9 0P2   Shoestring Aviation Airfield  \n#&gt; 10 0S9   Jefferson County Intl         \n#&gt; # ℹ more rows\n\n\nFrom tbl_airports, select “faa” and “name”. Rename the variables to “origin” and “origin_name” respectively. Assign it to a variable called origin_airports\n\n\norigin_airports &lt;- tbl_airports |&gt; \n  select(origin = faa, origin_name = name)\n\n\nPreview origin_airports\n\n\norigin_airports\n#&gt; # Source:   SQL [?? x 2]\n#&gt; # Database: spark_connection\n#&gt;    origin origin_name                   \n#&gt;    &lt;chr&gt;  &lt;chr&gt;                         \n#&gt;  1 04G    Lansdowne Airport             \n#&gt;  2 06A    Moton Field Municipal Airport \n#&gt;  3 06C    Schaumburg Regional           \n#&gt;  4 06N    Randall Airport               \n#&gt;  5 09J    Jekyll Island Airport         \n#&gt;  6 0A9    Elizabethton Municipal Airport\n#&gt;  7 0G6    Williams County Airport       \n#&gt;  8 0G7    Finger Lakes Regional Airport \n#&gt;  9 0P2    Shoestring Aviation Airfield  \n#&gt; 10 0S9    Jefferson County Intl         \n#&gt; # ℹ more rows\n\n\nSelect “tailnum”, “dest”, “origin”, and “distance” from flights_mapped. Assign it to a variable called flights_select\n\n\nflights_select &lt;- flights_mapped |&gt; \n  select(tailnum, dest, origin, distance) \n\n\nPreview flights_select\n\n\nflights_select\n#&gt; # Source:   SQL [?? x 4]\n#&gt; # Database: spark_connection\n#&gt;    tailnum dest  origin distance\n#&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n#&gt;  1 N626VA  LAX   JFK        2475\n#&gt;  2 N3760C  SJU   JFK        1598\n#&gt;  3 N712TW  LAX   JFK        2475\n#&gt;  4 N914DL  TPA   JFK        1005\n#&gt;  5 N823AY  ORF   LGA         296\n#&gt;  6 N3AXAA  ORD   LGA         733\n#&gt;  7 N218WN  HOU   EWR        1411\n#&gt;  8 N284JB  IAD   JFK         228\n#&gt;  9 N3FSAA  MIA   LGA        1096\n#&gt; 10 N12163  JAX   EWR         820\n#&gt; # ℹ more rows\n\n\nPipe flights_select into head()\n\n\nflights_select |&gt; \n  head()\n#&gt; # Source:   SQL [6 x 4]\n#&gt; # Database: spark_connection\n#&gt;   tailnum dest  origin distance\n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;\n#&gt; 1 N626VA  LAX   JFK        2475\n#&gt; 2 N3760C  SJU   JFK        1598\n#&gt; 3 N712TW  LAX   JFK        2475\n#&gt; 4 N914DL  TPA   JFK        1005\n#&gt; 5 N823AY  ORF   LGA         296\n#&gt; 6 N3AXAA  ORD   LGA         733\n\n\nAdd a left_join() step, after head(). Join to the dest_airports variable, using “dest” as the joining field\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") \n#&gt; # Source:   SQL [6 x 5]\n#&gt; # Database: spark_connection\n#&gt;   tailnum dest  origin distance dest_name         \n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;             \n#&gt; 1 N626VA  LAX   JFK        2475 Los Angeles Intl  \n#&gt; 2 N3760C  SJU   JFK        1598 &lt;NA&gt;              \n#&gt; 3 N712TW  LAX   JFK        2475 Los Angeles Intl  \n#&gt; 4 N914DL  TPA   JFK        1005 Tampa Intl        \n#&gt; 5 N823AY  ORF   LGA         296 Norfolk Intl      \n#&gt; 6 N3AXAA  ORD   LGA         733 Chicago Ohare Intl\n\n\nAdd another left_join() step. Join to the origin_airports variable, using “origin” as the joining field\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") \n#&gt; # Source:   SQL [6 x 6]\n#&gt; # Database: spark_connection\n#&gt;   tailnum dest  origin distance dest_name          origin_name        \n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;              \n#&gt; 1 N626VA  LAX   JFK        2475 Los Angeles Intl   John F Kennedy Intl\n#&gt; 2 N3760C  SJU   JFK        1598 &lt;NA&gt;               John F Kennedy Intl\n#&gt; 3 N712TW  LAX   JFK        2475 Los Angeles Intl   John F Kennedy Intl\n#&gt; 4 N914DL  TPA   JFK        1005 Tampa Intl         John F Kennedy Intl\n#&gt; 5 N823AY  ORF   LGA         296 Norfolk Intl       La Guardia         \n#&gt; 6 N3AXAA  ORD   LGA         733 Chicago Ohare Intl La Guardia\n\n\nAdd another left_join() step. Join to the tbl_planes variable, using “tailnum” as the joining field\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") |&gt; \n  left_join(tbl_planes, by = \"tailnum\")\n#&gt; # Source:   SQL [6 x 14]\n#&gt; # Database: spark_connection\n#&gt;   tailnum dest  origin distance dest_name   origin_name  year type  manufacturer\n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n#&gt; 1 N626VA  LAX   JFK        2475 Los Angele… John F Ken…  2006 Fixe… AIRBUS      \n#&gt; 2 N3760C  SJU   JFK        1598 &lt;NA&gt;        John F Ken…  2001 Fixe… BOEING      \n#&gt; 3 N712TW  LAX   JFK        2475 Los Angele… John F Ken…  1997 Fixe… BOEING      \n#&gt; 4 N914DL  TPA   JFK        1005 Tampa Intl  John F Ken…  1988 Fixe… MCDONNELL D…\n#&gt; 5 N823AY  ORF   LGA         296 Norfolk In… La Guardia   2005 Fixe… BOMBARDIER …\n#&gt; 6 N3AXAA  ORD   LGA         733 Chicago Oh… La Guardia    NaN &lt;NA&gt;  &lt;NA&gt;        \n#&gt; # ℹ 5 more variables: model &lt;chr&gt;, engines &lt;dbl&gt;, seats &lt;dbl&gt;, speed &lt;dbl&gt;,\n#&gt; #   engine &lt;chr&gt;\n\n\nAdd a mutate() step. Use ifelse() to modify “dest_name”. If “dest_name” is NA, then change its value to “Unknown”, if not NA, leave alone\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") |&gt; \n  left_join(tbl_planes, by = \"tailnum\") |&gt; \n  mutate(dest_name = ifelse(is.na(dest_name), \"Unknown\", dest_name))\n#&gt; # Source:   SQL [6 x 14]\n#&gt; # Database: spark_connection\n#&gt;   tailnum dest  origin distance dest_name   origin_name  year type  manufacturer\n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n#&gt; 1 N626VA  LAX   JFK        2475 Los Angele… John F Ken…  2006 Fixe… AIRBUS      \n#&gt; 2 N3760C  SJU   JFK        1598 Unknown     John F Ken…  2001 Fixe… BOEING      \n#&gt; 3 N712TW  LAX   JFK        2475 Los Angele… John F Ken…  1997 Fixe… BOEING      \n#&gt; 4 N914DL  TPA   JFK        1005 Tampa Intl  John F Ken…  1988 Fixe… MCDONNELL D…\n#&gt; 5 N823AY  ORF   LGA         296 Norfolk In… La Guardia   2005 Fixe… BOMBARDIER …\n#&gt; 6 N3AXAA  ORD   LGA         733 Chicago Oh… La Guardia    NaN &lt;NA&gt;  &lt;NA&gt;        \n#&gt; # ℹ 5 more variables: model &lt;chr&gt;, engines &lt;dbl&gt;, seats &lt;dbl&gt;, speed &lt;dbl&gt;,\n#&gt; #   engine &lt;chr&gt;\n\n\nInside the mutate() step, add another ifelse() to modify “origin_name”. If “origin_name” is NA, then change its value to “Unknown”, if not NA, leave alone\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") |&gt; \n  left_join(tbl_planes, by = \"tailnum\") |&gt; \n  mutate(\n    dest_name = ifelse(is.na(dest_name), \"Unknown\", dest_name),\n    origin_name = ifelse(is.na(origin_name), \"Unknown\", origin_name)\n    )\n#&gt; # Source:   SQL [6 x 14]\n#&gt; # Database: spark_connection\n#&gt;   tailnum dest  origin distance dest_name   origin_name  year type  manufacturer\n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n#&gt; 1 N626VA  LAX   JFK        2475 Los Angele… John F Ken…  2006 Fixe… AIRBUS      \n#&gt; 2 N3760C  SJU   JFK        1598 Unknown     John F Ken…  2001 Fixe… BOEING      \n#&gt; 3 N712TW  LAX   JFK        2475 Los Angele… John F Ken…  1997 Fixe… BOEING      \n#&gt; 4 N914DL  TPA   JFK        1005 Tampa Intl  John F Ken…  1988 Fixe… MCDONNELL D…\n#&gt; 5 N823AY  ORF   LGA         296 Norfolk In… La Guardia   2005 Fixe… BOMBARDIER …\n#&gt; 6 N3AXAA  ORD   LGA         733 Chicago Oh… La Guardia    NaN &lt;NA&gt;  &lt;NA&gt;        \n#&gt; # ℹ 5 more variables: model &lt;chr&gt;, engines &lt;dbl&gt;, seats &lt;dbl&gt;, speed &lt;dbl&gt;,\n#&gt; #   engine &lt;chr&gt;\n\n\nInside the mutate() step, add another ifelse() to modify “tailnum”. If “tailnum” is NA, then change its value to “Unknown”, if not NA, leave alone\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") |&gt; \n  left_join(tbl_planes, by = \"tailnum\") |&gt; \n  mutate(\n    dest_name = ifelse(is.na(dest_name), \"Unknown\", dest_name),\n    origin_name = ifelse(is.na(origin_name), \"Unknown\", origin_name),\n    tailnum = ifelse(is.na(tailnum), \"Unknown\", tailnum)\n    )\n#&gt; # Source:   SQL [6 x 14]\n#&gt; # Database: spark_connection\n#&gt;   tailnum dest  origin distance dest_name   origin_name  year type  manufacturer\n#&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       \n#&gt; 1 N626VA  LAX   JFK        2475 Los Angele… John F Ken…  2006 Fixe… AIRBUS      \n#&gt; 2 N3760C  SJU   JFK        1598 Unknown     John F Ken…  2001 Fixe… BOEING      \n#&gt; 3 N712TW  LAX   JFK        2475 Los Angele… John F Ken…  1997 Fixe… BOEING      \n#&gt; 4 N914DL  TPA   JFK        1005 Tampa Intl  John F Ken…  1988 Fixe… MCDONNELL D…\n#&gt; 5 N823AY  ORF   LGA         296 Norfolk In… La Guardia   2005 Fixe… BOMBARDIER …\n#&gt; 6 N3AXAA  ORD   LGA         733 Chicago Oh… La Guardia    NaN &lt;NA&gt;  &lt;NA&gt;        \n#&gt; # ℹ 5 more variables: model &lt;chr&gt;, engines &lt;dbl&gt;, seats &lt;dbl&gt;, speed &lt;dbl&gt;,\n#&gt; #   engine &lt;chr&gt;\n\n\nAdd a show_query() step to see the resulting query\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") |&gt; \n  left_join(tbl_planes, by = \"tailnum\") |&gt; \n  mutate(\n    dest_name = ifelse(is.na(dest_name), \"Unknown\", dest_name),\n    origin_name = ifelse(is.na(origin_name), \"Unknown\", origin_name),\n    tailnum = ifelse(is.na(tailnum), \"Unknown\", tailnum)\n    ) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   IF(ISNULL((`tailnum` IS NULL)), NULL, IF((`tailnum` IS NULL), \"Unknown\", `tailnum`)) AS `tailnum`,\n#&gt;   `dest`,\n#&gt;   `origin`,\n#&gt;   `distance`,\n#&gt;   IF(ISNULL((`dest_name` IS NULL)), NULL, IF((`dest_name` IS NULL), \"Unknown\", `dest_name`)) AS `dest_name`,\n#&gt;   IF(ISNULL((`origin_name` IS NULL)), NULL, IF((`origin_name` IS NULL), \"Unknown\", `origin_name`)) AS `origin_name`,\n#&gt;   `year`,\n#&gt;   `type`,\n#&gt;   `manufacturer`,\n#&gt;   `model`,\n#&gt;   `engines`,\n#&gt;   `seats`,\n#&gt;   `speed`,\n#&gt;   `engine`\n#&gt; FROM (\n#&gt;   SELECT\n#&gt;     `...1`.*,\n#&gt;     `airports_csv...2`.`name` AS `dest_name`,\n#&gt;     `airports_csv...3`.`name` AS `origin_name`,\n#&gt;     `year`,\n#&gt;     `type`,\n#&gt;     `manufacturer`,\n#&gt;     `model`,\n#&gt;     `engines`,\n#&gt;     `seats`,\n#&gt;     `speed`,\n#&gt;     `engine`\n#&gt;   FROM (\n#&gt;     SELECT `tailnum`, `dest`, `origin`, `distance`\n#&gt;     FROM `mapped`\n#&gt;     LIMIT 6\n#&gt;   ) `...1`\n#&gt;   LEFT JOIN `airports_csv` `airports_csv...2`\n#&gt;     ON (`...1`.`dest` = `airports_csv...2`.`faa`)\n#&gt;   LEFT JOIN `airports_csv` `airports_csv...3`\n#&gt;     ON (`...1`.`origin` = `airports_csv...3`.`faa`)\n#&gt;   LEFT JOIN `planes`\n#&gt;     ON (`...1`.`tailnum` = `planes`.`tailnum`)\n#&gt; ) `q01`\n\n\nRemove the head() step, and run again to see the new query\n\n\nflights_select |&gt; \n  head() |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") |&gt; \n  left_join(tbl_planes, by = \"tailnum\") |&gt; \n  mutate(\n    dest_name = ifelse(is.na(dest_name), \"Unknown\", dest_name),\n    origin_name = ifelse(is.na(origin_name), \"Unknown\", origin_name),\n    tailnum = ifelse(is.na(tailnum), \"Unknown\", tailnum)\n    ) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   IF(ISNULL((`tailnum` IS NULL)), NULL, IF((`tailnum` IS NULL), \"Unknown\", `tailnum`)) AS `tailnum`,\n#&gt;   `dest`,\n#&gt;   `origin`,\n#&gt;   `distance`,\n#&gt;   IF(ISNULL((`dest_name` IS NULL)), NULL, IF((`dest_name` IS NULL), \"Unknown\", `dest_name`)) AS `dest_name`,\n#&gt;   IF(ISNULL((`origin_name` IS NULL)), NULL, IF((`origin_name` IS NULL), \"Unknown\", `origin_name`)) AS `origin_name`,\n#&gt;   `year`,\n#&gt;   `type`,\n#&gt;   `manufacturer`,\n#&gt;   `model`,\n#&gt;   `engines`,\n#&gt;   `seats`,\n#&gt;   `speed`,\n#&gt;   `engine`\n#&gt; FROM (\n#&gt;   SELECT\n#&gt;     `...1`.*,\n#&gt;     `airports_csv...2`.`name` AS `dest_name`,\n#&gt;     `airports_csv...3`.`name` AS `origin_name`,\n#&gt;     `year`,\n#&gt;     `type`,\n#&gt;     `manufacturer`,\n#&gt;     `model`,\n#&gt;     `engines`,\n#&gt;     `seats`,\n#&gt;     `speed`,\n#&gt;     `engine`\n#&gt;   FROM (\n#&gt;     SELECT `tailnum`, `dest`, `origin`, `distance`\n#&gt;     FROM `mapped`\n#&gt;     LIMIT 6\n#&gt;   ) `...1`\n#&gt;   LEFT JOIN `airports_csv` `airports_csv...2`\n#&gt;     ON (`...1`.`dest` = `airports_csv...2`.`faa`)\n#&gt;   LEFT JOIN `airports_csv` `airports_csv...3`\n#&gt;     ON (`...1`.`origin` = `airports_csv...3`.`faa`)\n#&gt;   LEFT JOIN `planes`\n#&gt;     ON (`...1`.`tailnum` = `planes`.`tailnum`)\n#&gt; ) `q01`\n\n\nAssign to a variable called tbl_prep, swap the show_query() step with compute()\n\n\ntbl_prep &lt;- flights_select |&gt; \n  left_join(dest_airports, by = \"dest\") |&gt; \n  left_join(origin_airports, by = \"origin\") |&gt; \n  left_join(tbl_planes, by = \"tailnum\") |&gt; \n  mutate(\n    dest_name = ifelse(is.na(dest_name), \"Unknown\", dest_name),\n    origin_name = ifelse(is.na(origin_name), \"Unknown\", origin_name),\n    tailnum = ifelse(is.na(tailnum), \"Unknown\", tailnum)\n    ) |&gt; \n  compute()\n\n\nPipe tbl_prep to show_query() to see the new query\n\n\ntbl_prep |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM `table_46ab746a_e8ea_4ab6_b409_2c3da7056e34`",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Databricks Connect</span>"
    ]
  },
  {
    "objectID": "r-udfs.html",
    "href": "r-udfs.html",
    "title": "6  Intro to R UDFs",
    "section": "",
    "text": "Catch up\nlibrary(sparklyr)\nlibrary(dplyr)\nsc &lt;- spark_connect(method = \"databricks_connect\")\n#&gt; ! Changing host URL to: https://rstudio-partner-posit-default.cloud.databricks.com\n#&gt; ℹ Retrieving info for cluster:'1026-175310-7cpsh3g8'\n#&gt; ✔ Cluster: '1026-175310-7cpsh3g8' | DBR: '14.1' [432ms]\n#&gt; \n#&gt; ℹ Attempting to load 'r-sparklyr-databricks-14.1'\n#&gt; ✔ Python environment: 'r-sparklyr-databricks-14.1' [991ms]\n#&gt; \n#&gt; ℹ Connecting to '14.1 cluster'\n#&gt; ✔ Connected to: '14.1 cluster' [6ms]\n#&gt;",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to R UDFs</span>"
    ]
  },
  {
    "objectID": "r-udfs.html#simple-operations",
    "href": "r-udfs.html#simple-operations",
    "title": "6  Intro to R UDFs",
    "section": "6.1 Simple operations",
    "text": "6.1 Simple operations\nTrying out very simple operation to become familiar with the process\n\nUse copy_to() to send mtcars to the cluster. Load it to a variable called tbl_mtcars\n\n\ntbl_mtcars &lt;- copy_to(sc, mtcars)\n\n\nPipe tbl_mtcars to spark_apply(). Use nrow as the function to run\n\n\ntbl_mtcars |&gt; \n  spark_apply(nrow)\n#&gt; Some features are not enabled in this build of Arrow. Run `arrow_info()` for more information.\n#&gt; The repository you retrieved Arrow from did not include all of Arrow's features.\n#&gt; You can install a fully-featured version by running:\n#&gt; `install.packages('arrow', repos = 'https://apache.r-universe.dev')`.\n#&gt; \n#&gt; Attaching package: 'arrow'\n#&gt; The following object is masked from 'package:lubridate':\n#&gt; \n#&gt;     duration\n#&gt; The following object is masked from 'package:utils':\n#&gt; \n#&gt;     timestamp\n#&gt; To increase performance, use the following schema:\n#&gt; columns = \"x long\"\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_867dadbd_1724_452e_a30b_0aea96ec858f`&gt; [4 x 1]\n#&gt; # Database: spark_connection\n#&gt;       x\n#&gt;   &lt;dbl&gt;\n#&gt; 1     8\n#&gt; 2     8\n#&gt; 3     8\n#&gt; 4     8\n\n\nSwitch the function to use in spark_apply() to dim. Notice how it returns more rows, because coercing the size 2 vector creates a 2 row data frame\n\n\ntbl_mtcars |&gt; \n  spark_apply(dim)\n#&gt; To increase performance, use the following schema:\n#&gt; columns = \"x long\"\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_816793c9_b4cc_420b_a8a8_d2c20a5b07e5`&gt; [8 x 1]\n#&gt; # Database: spark_connection\n#&gt;       x\n#&gt;   &lt;dbl&gt;\n#&gt; 1     8\n#&gt; 2    11\n#&gt; 3     8\n#&gt; 4    11\n#&gt; 5     8\n#&gt; 6    11\n#&gt; 7     8\n#&gt; 8    11",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to R UDFs</span>"
    ]
  },
  {
    "objectID": "r-udfs.html#group-by-variable",
    "href": "r-udfs.html#group-by-variable",
    "title": "6  Intro to R UDFs",
    "section": "6.2 Group by variable",
    "text": "6.2 Group by variable\nWrite and run simple grouping commands\n\nGo back to using nrow again for spark_apply(). Remember to pass columns = \"x long\"\n\n\ntbl_mtcars |&gt; \n  spark_apply(nrow, columns = \"x long\")\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_067200fb_e916_4f85_888b_cb302e55e1d9`&gt; [4 x 1]\n#&gt; # Database: spark_connection\n#&gt;       x\n#&gt;   &lt;dbl&gt;\n#&gt; 1     8\n#&gt; 2     8\n#&gt; 3     8\n#&gt; 4     8\n\n\nAdd the group_by argument, with the value of \"am\". There should be an error. This is because there are 2 variables in the result, instead of one, and we defined x only in columns\n\n\ntbl_mtcars |&gt; \n  spark_apply(nrow, group_by = \"am\", columns = \"x long\")\n\n\nInsert am long, at the beginning of columns\n\n\ntbl_mtcars |&gt; \n  spark_apply(nrow, group_by = \"am\", columns = \"am long, x long\")\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_4321bb30_d09b_4b23_8f37_6dadd518ce53`&gt; [2 x 2]\n#&gt; # Database: spark_connection\n#&gt;      am     x\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0    19\n#&gt; 2     1    13\n\n\nTo see how the name we pass does not have to match the variable name, change am to notam in columns\n\n\ntbl_mtcars |&gt; \n  spark_apply(nrow, group_by = \"am\", columns = \"notam long, x long\")\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_48a8b521_0011_4622_99b8_94b701463acf`&gt; [2 x 2]\n#&gt; # Database: spark_connection\n#&gt;   notam     x\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0    19\n#&gt; 2     1    13\n\n\nChange the grouping variable to “cyl”, make sure to update that in the columns argument as well\n\n\ntbl_mtcars |&gt; \n  spark_apply(nrow, group_by = \"cyl\", columns = \"cyl long, x long\")\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_7b98282a_0246_4786_b252_4b0cfa037313`&gt; [3 x 2]\n#&gt; # Database: spark_connection\n#&gt;     cyl     x\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     4    11\n#&gt; 2     6     7\n#&gt; 3     8    14",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to R UDFs</span>"
    ]
  },
  {
    "objectID": "r-udfs.html#custom-functions",
    "href": "r-udfs.html#custom-functions",
    "title": "6  Intro to R UDFs",
    "section": "6.3 Custom functions",
    "text": "6.3 Custom functions\nCreate simple custom functions to send to Spark\n\nIn spark_apply(), pass function(x) x as the function. This will return the entire mtcars data set\n\n\ntbl_mtcars |&gt; \n  spark_apply(function(x) x)\n#&gt; To increase performance, use the following schema:\n#&gt; columns = \"mpg double, cyl double, disp double, hp double, drat double, wt\n#&gt; double, qsec double, vs double, am double, gear double, carb double\"\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_0f95d585_c5a5_4b6a_b4dd_afeacd7812c5`&gt; [?? x 11]\n#&gt; # Database: spark_connection\n#&gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#&gt;  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#&gt;  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n#&gt;  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#&gt;  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n#&gt;  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#&gt;  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n#&gt;  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n#&gt;  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n#&gt; 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#&gt; # ℹ more rows\n\n\nModify the function to return only the “mpg”, “cyl”, and “disp” variables\n\n\ntbl_mtcars |&gt; \n  spark_apply(function(x) x[, c(\"mpg\", \"cyl\", \"disp\")])\n#&gt; To increase performance, use the following schema:\n#&gt; columns = \"mpg double, cyl double, disp double\"\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_6d668022_b215_4b66_a904_b8df93f0814c`&gt; [?? x 3]\n#&gt; # Database: spark_connection\n#&gt;      mpg   cyl  disp\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160 \n#&gt;  2  21       6  160 \n#&gt;  3  22.8     4  108 \n#&gt;  4  21.4     6  258 \n#&gt;  5  18.7     8  360 \n#&gt;  6  18.1     6  225 \n#&gt;  7  14.3     8  360 \n#&gt;  8  24.4     4  147.\n#&gt;  9  22.8     4  141.\n#&gt; 10  19.2     6  168.\n#&gt; # ℹ more rows\n\n\nAdd the recommended columns spec from\n\n\ntbl_mtcars |&gt; \n  spark_apply(\n    function(x) x[, c(\"mpg\", \"cyl\", \"disp\")], \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_384a32ad_fcb8_468f_886b_142130ee6656`&gt; [?? x 3]\n#&gt; # Database: spark_connection\n#&gt;      mpg   cyl  disp\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160 \n#&gt;  2  21       6  160 \n#&gt;  3  22.8     4  108 \n#&gt;  4  21.4     6  258 \n#&gt;  5  18.7     8  360 \n#&gt;  6  18.1     6  225 \n#&gt;  7  14.3     8  360 \n#&gt;  8  24.4     4  147.\n#&gt;  9  22.8     4  141.\n#&gt; 10  19.2     6  168.\n#&gt; # ℹ more rows\n\n\nMake your custom function into a ‘multi-line’ function\n\n\ntbl_mtcars |&gt; \n  spark_apply(\n    function(x){ \n      x[, c(\"mpg\", \"cyl\", \"disp\")]\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_9ad6389e_7f88_455d_bb44_26373948e69a`&gt; [?? x 3]\n#&gt; # Database: spark_connection\n#&gt;      mpg   cyl  disp\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160 \n#&gt;  2  21       6  160 \n#&gt;  3  22.8     4  108 \n#&gt;  4  21.4     6  258 \n#&gt;  5  18.7     8  360 \n#&gt;  6  18.1     6  225 \n#&gt;  7  14.3     8  360 \n#&gt;  8  24.4     4  147.\n#&gt;  9  22.8     4  141.\n#&gt; 10  19.2     6  168.\n#&gt; # ℹ more rows\n\n\nAssign the data selection step to a variable called out, and then use it as the output of the function\n\n\ntbl_mtcars |&gt; \n  spark_apply(\n    function(x){ \n      out &lt;- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_729c736f_c544_4c37_81b4_3067a08c321a`&gt; [?? x 3]\n#&gt; # Database: spark_connection\n#&gt;      mpg   cyl  disp\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6  160 \n#&gt;  2  21       6  160 \n#&gt;  3  22.8     4  108 \n#&gt;  4  21.4     6  258 \n#&gt;  5  18.7     8  360 \n#&gt;  6  18.1     6  225 \n#&gt;  7  14.3     8  360 \n#&gt;  8  24.4     4  147.\n#&gt;  9  22.8     4  141.\n#&gt; 10  19.2     6  168.\n#&gt; # ℹ more rows\n\n\nAdd a filter step that returns the highest “mpg”. Notice that instead of 1 record, it returns several. That is because the filter is being processed per partition.\n\n\ntbl_mtcars |&gt; \n  spark_apply(\n    function(x){ \n      out &lt;- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out &lt;- out[out$mpg == max(out$mpg), ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_bfa02dd9_be66_4637_bf95_a699570246f3`&gt; [4 x 3]\n#&gt; # Database: spark_connection\n#&gt;     mpg   cyl  disp\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  24.4     4 147. \n#&gt; 2  22.8     4 141. \n#&gt; 3  33.9     4  71.1\n#&gt; 4  30.4     4  95.1\n\n\nChange the filter to display any records with an “mpg” over 25\n\n\ntbl_mtcars |&gt; \n  spark_apply(\n    function(x){ \n      out &lt;- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out &lt;- out[out$mpg &gt; 25, ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_ec9b86e6_1803_4e7a_90d8_fe9fb7910e9f`&gt; [6 x 3]\n#&gt; # Database: spark_connection\n#&gt;     mpg   cyl  disp\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  32.4     4  78.7\n#&gt; 2  30.4     4  75.7\n#&gt; 3  33.9     4  71.1\n#&gt; 4  27.3     4  79  \n#&gt; 5  26       4 120. \n#&gt; 6  30.4     4  95.1\n\n\nInsert a step that modifies cyl. It should make it add 1 to the value.\n\n\ntbl_mtcars |&gt; \n  spark_apply(\n    function(x){ \n      out &lt;- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out$cyl &lt;- out$cyl + 1\n      out &lt;- out[out$mpg &gt; 25, ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_7121314e_2bb9_4de1_ba18_6926a6c9cb6a`&gt; [6 x 3]\n#&gt; # Database: spark_connection\n#&gt;     mpg   cyl  disp\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1  32.4     5  78.7\n#&gt; 2  30.4     5  75.7\n#&gt; 3  33.9     5  71.1\n#&gt; 4  27.3     5  79  \n#&gt; 5  26       5 120. \n#&gt; 6  30.4     5  95.1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to R UDFs</span>"
    ]
  },
  {
    "objectID": "r-udfs.html#r-packages",
    "href": "r-udfs.html#r-packages",
    "title": "6  Intro to R UDFs",
    "section": "6.4 R packages",
    "text": "6.4 R packages\nSimple example that uses an R package\n\nLoad the broom package into your R session\n\n\nlibrary(broom)\n\n\nCreate a function that, creates an lm model against the one, and only, argument passed to the function. Then use tidy() to return the results of the model as a data frame. The lm() call should assume that the data will always have the same columns as mtcars, and it will create a linear model of the “mpg” against all the other variables. Name it model_function\n\n\nmodel_function &lt;- function(x) {\n  model &lt;- lm(mpg ~ ., x)\n  tidy(model)\n}\n\n\nTest model_function by passing mtcars to it\n\n\nmodel_function(mtcars)\n#&gt; # A tibble: 11 × 5\n#&gt;    term        estimate std.error statistic p.value\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 (Intercept)  12.3      18.7        0.657  0.518 \n#&gt;  2 cyl          -0.111     1.05      -0.107  0.916 \n#&gt;  3 disp          0.0133    0.0179     0.747  0.463 \n#&gt;  4 hp           -0.0215    0.0218    -0.987  0.335 \n#&gt;  5 drat          0.787     1.64       0.481  0.635 \n#&gt;  6 wt           -3.72      1.89      -1.96   0.0633\n#&gt;  7 qsec          0.821     0.731      1.12   0.274 \n#&gt;  8 vs            0.318     2.10       0.151  0.881 \n#&gt;  9 am            2.52      2.06       1.23   0.234 \n#&gt; 10 gear          0.655     1.49       0.439  0.665 \n#&gt; 11 carb         -0.199     0.829     -0.241  0.812\n\n\nPass model_function to spark_apply(), against tbl_mtcars. The call should fail, because broom is not explicitly referred to in model_function\n\n\ntbl_mtcars |&gt; \n  spark_apply(model_function)\n\n\nModify model_function by either adding a library() call, or using :: to explicitly refer to broom inside it\n\n\nmodel_function &lt;- function(x) {\n  model &lt;- lm(mpg ~ ., x)\n  broom::tidy(model)\n}\n\n\nTest model_function again against tbl_mtcars\n\n\ntbl_mtcars |&gt; \n  spark_apply(model_function)\n#&gt; To increase performance, use the following schema:\n#&gt; columns = \"term string, estimate double, std_error double, statistic double,\n#&gt; p_value double\"\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_7147ff59_4759_4f01_a920_3184b005d1f3`&gt; [?? x 5]\n#&gt; # Database: spark_connection\n#&gt;    term        estimate std_error statistic p_value\n#&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 (Intercept) -18.6          NaN       NaN     NaN\n#&gt;  2 cyl           0.517        NaN       NaN     NaN\n#&gt;  3 disp          0.0356       NaN       NaN     NaN\n#&gt;  4 hp           -0.0579       NaN       NaN     NaN\n#&gt;  5 drat          7.98         NaN       NaN     NaN\n#&gt;  6 wt           -1.24         NaN       NaN     NaN\n#&gt;  7 qsec          0.565        NaN       NaN     NaN\n#&gt;  8 vs            2.51         NaN       NaN     NaN\n#&gt;  9 am          NaN            NaN       NaN     NaN\n#&gt; 10 gear        NaN            NaN       NaN     NaN\n#&gt; # ℹ more rows\n\n\nAdd a group_by argument, use “am” as the grouping variable\n\n\ntbl_mtcars |&gt; \n  spark_apply(model_function, group_by = \"am\")\n#&gt; To increase performance, use the following schema:\n#&gt; columns = \"am double, term string, estimate double, std_error double, statistic\n#&gt; double, p_value double\"\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_b2796b5a_7ab9_4279_b1be_54b3268fb568`&gt; [?? x 6]\n#&gt; # Database: spark_connection\n#&gt;       am term        estimate std_error statistic  p_value\n#&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     0 (Intercept)   8.64     21.5        0.402   0.697 \n#&gt;  2     0 cyl          -0.534     1.13      -0.474   0.647 \n#&gt;  3     0 disp         -0.0203    0.0174    -1.16    0.275 \n#&gt;  4     0 hp            0.0622    0.0461     1.35    0.210 \n#&gt;  5     0 drat          0.592     3.01       0.196   0.849 \n#&gt;  6     0 wt            1.95      2.23       0.876   0.404 \n#&gt;  7     0 qsec         -0.884     0.758     -1.17    0.274 \n#&gt;  8     0 vs            0.739     2.51       0.294   0.775 \n#&gt;  9     0 am          NaN       NaN        NaN     NaN     \n#&gt; 10     0 gear          8.65      3.90       2.22    0.0534\n#&gt; # ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to R UDFs</span>"
    ]
  },
  {
    "objectID": "r-udfs-modeling.html",
    "href": "r-udfs-modeling.html",
    "title": "7  Modeling",
    "section": "",
    "text": "Catch up\nlibrary(sparklyr)\nlibrary(dplyr)\nsc &lt;- spark_connect(method = \"databricks_connect\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "r-udfs-modeling.html#get-sample-of-data",
    "href": "r-udfs-modeling.html#get-sample-of-data",
    "title": "7  Modeling",
    "section": "7.1 Get sample of data",
    "text": "7.1 Get sample of data\nDownload a sampled data set locally to R\n\nCreate a pointer to the lendingclub data. It is in the samples schema\n\n\nlendingclub_dat &lt;- tbl(sc, I(\"workshops.samples.lendingclub\"))\n\n\nUsing slice_sample(), download 2K records, and name it lendingclub_sample\n\n\nlendingclub_sample &lt;- lendingclub_dat |&gt;  \n  slice_sample(n = 2000) |&gt; \n  collect()\n\n\nPreview the data using the View() command\n\n\nView(lendingclub_sample)\n\n\nKeep only int_rate, term, bc_util, bc_open_to_buy and all_util fields. Remove the percent sign out of int_rate, and coerce it to numeric. Save resulting table to a new variable called lendingclub_prep\n\n\nlendingclub_prep &lt;- lendingclub_sample |&gt; \n  select(int_rate, term, bc_util, bc_open_to_buy, all_util) |&gt; \n  mutate(\n    int_rate = as.numeric(stringr::str_remove(int_rate, \"%\"))\n    )\n\n\nPreview the data using glimpse()\n\n\nglimpse(lendingclub_prep)\n#&gt; Rows: 2,000\n#&gt; Columns: 5\n#&gt; $ int_rate       &lt;dbl&gt; 14.07, 12.61, 10.90, 9.58, 7.96, 5.31, 10.56, 18.45, 5.…\n#&gt; $ term           &lt;chr&gt; \"36 months\", \"60 months\", \"36 months\", \"36 months\", \"36…\n#&gt; $ bc_util        &lt;dbl&gt; 8.5, 84.2, 95.4, 0.4, 16.6, 25.0, 28.3, 32.7, 53.6, 10.…\n#&gt; $ bc_open_to_buy &lt;dbl&gt; 21232, 835, 278, 20919, 37946, 41334, 17721, 64335, 343…\n#&gt; $ all_util       &lt;dbl&gt; 9, 61, 80, 61, 44, 29, 62, 33, 44, 19, 38, 37, 69, 29, …\n\n\nDisconnect from Spark\n\n\nspark_disconnect(sc)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "r-udfs-modeling.html#create-model-using-tidymodels",
    "href": "r-udfs-modeling.html#create-model-using-tidymodels",
    "title": "7  Modeling",
    "section": "7.2 Create model using tidymodels",
    "text": "7.2 Create model using tidymodels\n\nRun the following code to create a workflow that contains the pre-processing steps, and a linear regression model\n\n\nlibrary(tidymodels)\n\nlendingclub_rec &lt;- recipe(int_rate ~ ., data = lendingclub_prep) |&gt; \n  step_mutate(term = trimws(substr(term, 1,2))) |&gt; \n  step_mutate(across(everything(), as.numeric)) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt;\n  step_impute_mean(all_of(c(\"bc_open_to_buy\", \"bc_util\"))) |&gt;   \n  step_filter(!if_any(everything(), is.na))\n\n\nlendingclub_lr &lt;- linear_reg()\n\nlendingclub_wf &lt;- workflow() |&gt; \n  add_model(lendingclub_lr) |&gt; \n  add_recipe(lendingclub_rec)\n\nlendingclub_wf\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; 5 Recipe Steps\n#&gt; \n#&gt; • step_mutate()\n#&gt; • step_mutate()\n#&gt; • step_normalize()\n#&gt; • step_impute_mean()\n#&gt; • step_filter()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nFit the model in the workflow, now in a variable called lendingclub_wf, with the lendingclub_prep data\n\n\nlendingclub_fit &lt;- lendingclub_wf |&gt; \n  fit(data = lendingclub_prep)\n\n\nMeasure the performance of the model using metrics(). Make sure to use augment() to add the predictions first\n\n\nlendingclub_fit |&gt; \n  augment(lendingclub_prep) |&gt; \n  metrics(int_rate, .pred)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard       4.18 \n#&gt; 2 rsq     standard       0.307\n#&gt; 3 mae     standard       3.26\n\n\nRun a histogram over the predictions\n\n\nlibrary(ggplot2)\n\npredict(lendingclub_fit, lendingclub_sample) |&gt; \n  ggplot() +\n  geom_histogram(aes(.pred))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "r-udfs-modeling.html#using-vetiver",
    "href": "r-udfs-modeling.html#using-vetiver",
    "title": "7  Modeling",
    "section": "7.3 Using Vetiver",
    "text": "7.3 Using Vetiver\nConvert the workflow into a vetiver model\n\nLoad the vetiver package\n\n\nlibrary(vetiver)\n\n\nConvert to Vetiver using vetiver_model(). Name the variable lendingclub_vetiver\n\n\nlendingclub_vetiver &lt;- vetiver_model(lendingclub_fit, \"lendingclub_model\")\n\n\nSave lendingclub_vetiver as “lendingclub.rds”\n\n\nsaveRDS(lendingclub_vetiver, \"lendingclub.rds\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "r-udfs-modeling.html#create-prediction-function",
    "href": "r-udfs-modeling.html#create-prediction-function",
    "title": "7  Modeling",
    "section": "7.4 Create prediction function",
    "text": "7.4 Create prediction function\nCreating a self-contained prediction function that will read the model, and then run the predictions\n\nCreate a very simple function that takes x, and it assumes it will be a data frame. Inside the function, it will read the “lendingclub.rds” file, and then use it to predict against x. Name the function predict_vetiver\n\n\npredict_vetiver &lt;- function(x) {\n  model &lt;- readRDS(\"lendingclub.rds\")  \n  predict(model, x)\n}\n\n\nTest the predict_vetiver function against lendingclub_prep\n\n\npredict_vetiver(lendingclub_prep)\n#&gt; # A tibble: 2,000 × 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  8.35\n#&gt;  2 17.3 \n#&gt;  3 14.2 \n#&gt;  4 10.1 \n#&gt;  5  8.77\n#&gt;  6  8.22\n#&gt;  7 10.9 \n#&gt;  8  7.14\n#&gt;  9 11.8 \n#&gt; 10  8.00\n#&gt; # ℹ 1,990 more rows\n\n\nModify the function, so that it will add the predictions back to x. The new variable should be named ret_pred. The function should output the modified x\n\n\npredict_vetiver &lt;- function(x) {\n  model &lt;- readRDS(\"lendingclub.rds\")  \n  preds &lt;- predict(model, x)\n  x$rate_pred &lt;- preds$`.pred`\n  x\n}\n\n\nTest the predict_vetiver function against lendingclub_prep\n\n\npredict_vetiver(lendingclub_prep)\n#&gt; # A tibble: 2,000 × 6\n#&gt;    int_rate term      bc_util bc_open_to_buy all_util rate_pred\n#&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1    14.1  36 months     8.5          21232        9      8.35\n#&gt;  2    12.6  60 months    84.2            835       61     17.3 \n#&gt;  3    10.9  36 months    95.4            278       80     14.2 \n#&gt;  4     9.58 36 months     0.4          20919       61     10.1 \n#&gt;  5     7.96 36 months    16.6          37946       44      8.77\n#&gt;  6     5.31 36 months    25            41334       29      8.22\n#&gt;  7    10.6  36 months    28.3          17721       62     10.9 \n#&gt;  8    18.4  36 months    32.7          64335       33      7.14\n#&gt;  9     5.31 36 months    53.6           3434       44     11.8 \n#&gt; 10     7.46 36 months    10.9          33668       19      8.00\n#&gt; # ℹ 1,990 more rows\n\n\nAt the beginning of the function, load the workflows and vetiver libraries\n\n\npredict_vetiver &lt;- function(x) {\n  library(workflows)\n  library(vetiver)\n  model &lt;- readRDS(\"lendingclub.rds\")  \n  preds &lt;- predict(model, x)\n  x$rate_pred &lt;- preds$`.pred`\n  x\n}\n\n\nTest the predict_vetiver function against lendingclub_prep\n\n\npredict_vetiver(lendingclub_prep)\n#&gt; # A tibble: 2,000 × 6\n#&gt;    int_rate term      bc_util bc_open_to_buy all_util rate_pred\n#&gt;       &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1    14.1  36 months     8.5          21232        9      8.35\n#&gt;  2    12.6  60 months    84.2            835       61     17.3 \n#&gt;  3    10.9  36 months    95.4            278       80     14.2 \n#&gt;  4     9.58 36 months     0.4          20919       61     10.1 \n#&gt;  5     7.96 36 months    16.6          37946       44      8.77\n#&gt;  6     5.31 36 months    25            41334       29      8.22\n#&gt;  7    10.6  36 months    28.3          17721       62     10.9 \n#&gt;  8    18.4  36 months    32.7          64335       33      7.14\n#&gt;  9     5.31 36 months    53.6           3434       44     11.8 \n#&gt; 10     7.46 36 months    10.9          33668       19      8.00\n#&gt; # ℹ 1,990 more rows",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "r-udfs-modeling.html#predict-in-spark",
    "href": "r-udfs-modeling.html#predict-in-spark",
    "title": "7  Modeling",
    "section": "7.5 Predict in Spark",
    "text": "7.5 Predict in Spark\nRun the predictions in Spark against the entire data set\n\nAdd conditional statement that reads the RDS file if it’s available locally, and if not, read it from: “/Volumes/workshops/models/vetiver/lendingclub.rds”\n\n\npredict_vetiver &lt;- function(x) {\n  library(workflows)\n  library(vetiver)\n  if(file.exists(\"lendingclub.rds\")) {\n    model &lt;- readRDS(\"lendingclub.rds\")  \n  } else {\n    model &lt;- readRDS(\"/Volumes/workshops/models/vetiver/lendingclub.rds\")\n  }\n  preds &lt;- predict(model, x)\n  x$rate_pred &lt;- preds$`.pred`\n  x\n}\n\n\nRe-connect to your Spark cluster\n\n\nsc &lt;- spark_connect(method = \"databricks_connect\")\n\n\nRe-create a pointer to the lendingclub data. It is in the samples schema\n\n\nlendingclub_dat &lt;- tbl(sc, I(\"workshops.samples.lendingclub\"))\n\n\nSelect the int_rate, term, bc_util, bc_open_to_buy, and all_util fields from lendingclub_dat. And then pass just the top rows (using head()) to spark_apply(). Use the updated predict_vetiver to run the model.\n\n\nlendingclub_dat |&gt; \n  select(int_rate, term, bc_util, bc_open_to_buy, all_util) |&gt; \n  head() |&gt; \n  spark_apply(predict_vetiver)\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_ccf6db3c_0348_4246_8a93_765344c7f00a`&gt; [6 x 6]\n#&gt; # Database: spark_connection\n#&gt;   int_rate term      bc_util bc_open_to_buy all_util rate_pred\n#&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 20.39%   36 months    94              133       82     14.9 \n#&gt; 2 13.06%   60 months    82            10021       70     17.2 \n#&gt; 3 10.56%   60 months    34.5          41570       54     13.4 \n#&gt; 4 6.83%    36 months     7.9          23119       47      9.43\n#&gt; 5 17.47%   60 months    62.1          11686       57     16.0 \n#&gt; 6 16.46%   36 months    92.2            380       75     14.6\n\n\nAdd the columns specification to the spark_apply() call\n\n\nlendingclub_dat |&gt; \n  select(int_rate, term, bc_util, bc_open_to_buy, all_util) |&gt; \n  head() |&gt; \n  spark_apply(\n    predict_vetiver, \n    columns = \"int_rate string, term string, bc_util double, bc_open_to_buy double, all_util double, pred double\"\n    )\n#&gt; # Source:   table&lt;`sparklyr_tmp_table_63178bba_c8ea_4f0b_aa19_c3377c947124`&gt; [6 x 6]\n#&gt; # Database: spark_connection\n#&gt;   int_rate term      bc_util bc_open_to_buy all_util  pred\n#&gt;   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 20.39%   36 months    94              133       82 14.9 \n#&gt; 2 13.06%   60 months    82            10021       70 17.2 \n#&gt; 3 10.56%   60 months    34.5          41570       54 13.4 \n#&gt; 4 6.83%    36 months     7.9          23119       47  9.43\n#&gt; 5 17.47%   60 months    62.1          11686       57 16.0 \n#&gt; 6 16.46%   36 months    92.2            380       75 14.6\n\n\nAppend compute() to the end of the code, remove head(), and save the results into a variable called lendingclub_predictions\n\n\nlendingclub_predictions &lt;- lendingclub_dat |&gt; \n  select(int_rate, term, bc_util, bc_open_to_buy, all_util) |&gt; \n  spark_apply(\n    predict_vetiver,\n    columns = \"int_rate string, term string, bc_util double, bc_open_to_buy double, all_util double, pred double\"\n    ) |&gt; \n  compute()\n\n\nPreview the lendingclub_predictions table\n\n\nlendingclub_predictions\n#&gt; # Source:   table&lt;`table_6ba184f8_9933_4bdd_88da_cc5f27fc4e91`&gt; [?? x 6]\n#&gt; # Database: spark_connection\n#&gt;    int_rate term      bc_util bc_open_to_buy all_util  pred\n#&gt;    &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 20.39%   36 months    94              133       82 14.9 \n#&gt;  2 13.06%   60 months    82            10021       70 17.2 \n#&gt;  3 10.56%   60 months    34.5          41570       54 13.4 \n#&gt;  4 6.83%    36 months     7.9          23119       47  9.43\n#&gt;  5 17.47%   60 months    62.1          11686       57 16.0 \n#&gt;  6 16.46%   36 months    92.2            380       75 14.6 \n#&gt;  7 19.42%   60 months    21.5           1099       12 13.5 \n#&gt;  8 22.90%   60 months    36.1          30777       62 14.3 \n#&gt;  9 5.31%    36 months    54.9           8576       71 12.7 \n#&gt; 10 6.83%    36 months    12.4          53356       17  6.96\n#&gt; # ℹ more rows",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "working-with-llms.html",
    "href": "working-with-llms.html",
    "title": "8  Working with LLMs in Databricks",
    "section": "",
    "text": "Catch up\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\n\ncon &lt;- dbConnect(\n  odbc::databricks(),\n  HTTPPath = \"/sql/1.0/warehouses/300bd24ba12adf8e\"\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with LLMs in Databricks</span>"
    ]
  },
  {
    "objectID": "working-with-llms.html#accessing-ai-functions",
    "href": "working-with-llms.html#accessing-ai-functions",
    "title": "8  Working with LLMs in Databricks",
    "section": "8.1 Accessing AI functions",
    "text": "8.1 Accessing AI functions\nUse the sentiment classification function\n\nCreate a quick review table using the following code:\n\n\nreviews &lt;- tribble(\n  ~name,     ~review, \n  \"adam\",    \"This is the best toaster I have ever bought\",\n  \"berry\",   \"Toaster arrived broken, waiting for replancement\",\n  \"charles\", \"The washing machine is as advertised, can't wait to use it\",\n  \"dan\",     \"Not sure how to feel about this tevelision, nice brightness but bad definition\"\n) |&gt; \n  select(review) \n\n\nCopy the reviews data frame to your SQL session. Assign it to a variable called tbl_reviews\n\n\ntbl_reviews &lt;- copy_to(con, reviews, overwrite = TRUE)\n\n\nCreate a new field called “sentiment”, use ai_analyze_sentiment() to analyze the “review” field\n\n\ntbl_reviews |&gt; \n  mutate(sentiment = ai_analyze_sentiment(review))\n#&gt; # Source:   SQL [4 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                             sentiment\n#&gt;   &lt;chr&gt;                                                              &lt;chr&gt;    \n#&gt; 1 This is the best toaster I have ever bought                        positive \n#&gt; 2 Toaster arrived broken, waiting for replancement                   negative \n#&gt; 3 The washing machine is as advertised, cant wait to use it          positive \n#&gt; 4 Not sure how to feel about this tevelision, nice brightness but b… mixed",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with LLMs in Databricks</span>"
    ]
  },
  {
    "objectID": "working-with-llms.html#specify-array",
    "href": "working-with-llms.html#specify-array",
    "title": "8  Working with LLMs in Databricks",
    "section": "8.2 Specify array",
    "text": "8.2 Specify array\nUsing array() to run the classification function\n\nUse ai_classify() to find out if we need to follow up with customer. The two options should be: ‘order complete’, and ‘need follow up’. Use array() as if you would be using the c() function. Name the new field “follow_up”\n\n\ntbl_reviews |&gt; \n  mutate(\n    follow_up = ai_classify(review, array(\"order complete\", \"need follow up\"))\n    )\n#&gt; # Source:   SQL [4 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                        follow_up     \n#&gt;   &lt;chr&gt;                                                         &lt;chr&gt;         \n#&gt; 1 This is the best toaster I have ever bought                   order complete\n#&gt; 2 Toaster arrived broken, waiting for replancement              need follow up\n#&gt; 3 The washing machine is as advertised, cant wait to use it     order complete\n#&gt; 4 Not sure how to feel about this tevelision, nice brightness … need follow up\n\n\nAdd a step that keeps only those orders that need follow up\n\n\ntbl_reviews |&gt; \n  mutate(\n    follow_up = ai_classify(review, array(\"order complete\", \"need follow up\"))\n    ) |&gt; \n  filter(follow_up == \"need follow up\")\n#&gt; # Source:   SQL [2 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                        follow_up     \n#&gt;   &lt;chr&gt;                                                         &lt;chr&gt;         \n#&gt; 1 Toaster arrived broken, waiting for replancement              need follow up\n#&gt; 2 Not sure how to feel about this tevelision, nice brightness … need follow up",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with LLMs in Databricks</span>"
    ]
  },
  {
    "objectID": "working-with-llms.html#process-complex-output",
    "href": "working-with-llms.html#process-complex-output",
    "title": "8  Working with LLMs in Databricks",
    "section": "8.3 Process complex output",
    "text": "8.3 Process complex output\nWorking STRUCT output from an ‘ai’ function\n\nUse ai_extract() to pull the type of product being referred to in the review. Pass ‘product’ as the extract argument, and pass it inside an array() call. Name the new field “product”\n\n\ntbl_reviews |&gt; \n  mutate(product = ai_extract(review, array(\"product\")))\n#&gt; # Source:   SQL [4 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                  product             \n#&gt;   &lt;chr&gt;                                                   &lt;chr&gt;               \n#&gt; 1 This is the best toaster I have ever bought             \"{\\\"product\\\":\\\"toa…\n#&gt; 2 Toaster arrived broken, waiting for replancement        \"{\\\"product\\\":\\\"Toa…\n#&gt; 3 The washing machine is as advertised, cant wait to use… \"{\\\"product\\\":\\\"was…\n#&gt; 4 Not sure how to feel about this tevelision, nice brigh… \"{\\\"product\\\":\\\"tel…\n\n\nAppend a compute() step, and assign to a new variable called tbl_review\n\n\ntbl_review &lt;- tbl_reviews |&gt; \n  mutate(product = ai_extract(review, array(\"product\"))) |&gt; \n  compute()\n\n\nPreview tbl_review\n\n\ntbl_review \n#&gt; # Source:   table&lt;`dbplyr_MjqcmuPoJy`&gt; [4 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                  product             \n#&gt;   &lt;chr&gt;                                                   &lt;chr&gt;               \n#&gt; 1 This is the best toaster I have ever bought             \"{\\\"product\\\":\\\"toa…\n#&gt; 2 Toaster arrived broken, waiting for replancement        \"{\\\"product\\\":\\\"Toa…\n#&gt; 3 The washing machine is as advertised, cant wait to use… \"{\\\"product\\\":\\\"was…\n#&gt; 4 Not sure how to feel about this tevelision, nice brigh… \"{\\\"product\\\":\\\"tel…\n\n\nPass tbl_review to show_query() to confirm that it is pulling from a new temporary table\n\n\ntbl_review |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT *\n#&gt; FROM `dbplyr_MjqcmuPoJy`\n\n\nCoerce “product” to a character\n\n\ntbl_review |&gt; \n  mutate(product = as.character(product))\n#&gt; # Source:   SQL [4 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                     product          \n#&gt;   &lt;chr&gt;                                                      &lt;chr&gt;            \n#&gt; 1 This is the best toaster I have ever bought                {toaster}        \n#&gt; 2 Toaster arrived broken, waiting for replancement           {Toaster}        \n#&gt; 3 The washing machine is as advertised, cant wait to use it  {washing machine}\n#&gt; 4 Not sure how to feel about this tevelision, nice brightne… {television}\n\n\nWrap the as.character() call, inside a tolower() call\n\n\ntbl_review |&gt; \n  mutate(product = tolower(as.character(product)))\n#&gt; # Source:   SQL [4 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   review                                                     product          \n#&gt;   &lt;chr&gt;                                                      &lt;chr&gt;            \n#&gt; 1 This is the best toaster I have ever bought                {toaster}        \n#&gt; 2 Toaster arrived broken, waiting for replancement           {toaster}        \n#&gt; 3 The washing machine is as advertised, cant wait to use it  {washing machine}\n#&gt; 4 Not sure how to feel about this tevelision, nice brightne… {television}\n\n\nAdd a count step, that breaks down the reviews by product\n\n\ntbl_review |&gt; \n  mutate(product = tolower(as.character(product))) |&gt; \n  count(product)\n#&gt; # Source:   SQL [3 x 2]\n#&gt; # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#&gt;   product                 n\n#&gt;   &lt;chr&gt;             &lt;int64&gt;\n#&gt; 1 {toaster}               2\n#&gt; 2 {washing machine}       1\n#&gt; 3 {television}            1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with LLMs in Databricks</span>"
    ]
  },
  {
    "objectID": "working-with-llms.html#introducing-chattr",
    "href": "working-with-llms.html#introducing-chattr",
    "title": "8  Working with LLMs in Databricks",
    "section": "8.4 Introducing chattr",
    "text": "8.4 Introducing chattr\nRequest a simple example from LLM\n\nLoad the chattr library\n\n\nlibrary(chattr)\n\n\nCall chattr_app() and then select the first model (DBRX)\n\n\nchattr_app()\n\n\nIn the app type, and run: “show me a simple example of a ggplot using the mtcars data set”\nCopy the code into a script and test",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Working with LLMs in Databricks</span>"
    ]
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "9  Dashboards",
    "section": "",
    "text": "9.1 Preview app\nTry out the finalized Shiny app",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#preview-app",
    "href": "dashboards.html#preview-app",
    "title": "9  Dashboards",
    "section": "",
    "text": "Open the example-app.R file",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#catch-up",
    "href": "dashboards.html#catch-up",
    "title": "9  Dashboards",
    "section": "Catch up",
    "text": "Catch up\n\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\n\ncon &lt;- dbConnect(\n  odbc::databricks(),\n  HTTPPath = \"/sql/1.0/warehouses/300bd24ba12adf8e\"\n)\n\norders &lt;- tbl(con, I(\"workshops.tpch.orders\"))\ncustomers &lt;- tbl(con, I(\"workshops.tpch.customer\"))\nnation &lt;- tbl(con, I(\"workshops.tpch.nation\"))\n\nprep_orders &lt;- orders |&gt; \n  left_join(customers, by = c(\"o_custkey\" = \"c_custkey\")) |&gt; \n  left_join(nation, by = c(\"c_nationkey\" = \"n_nationkey\")) |&gt; \n  mutate(\n    order_year = year(o_orderdate), \n    order_month = month(o_orderdate)\n  ) |&gt; \n  rename(customer = o_custkey) |&gt; \n  select(-ends_with(\"comment\"),  -ends_with(\"key\"))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#data-driven-dropdown",
    "href": "dashboards.html#data-driven-dropdown",
    "title": "9  Dashboards",
    "section": "9.2 Data driven dropdown",
    "text": "9.2 Data driven dropdown\n\nLoad the shiny library\n\n\nlibrary(shiny)\n\n\nRun the following code to see the “stub” Shiny app\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = \"FRANCE\", selected = \"FRANCE\"),\n    plotOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderPlot({\n      # Your code here\n    })\n  },\n  options = list(height = 200)\n)\n\n\nRetrieve the country names from nation into a character vector. Save the values to a variable called countries\n\n\ncountries &lt;- nation |&gt; \n  pull(n_name)\n\n\nPreview countries\n\n\ncountries\n#&gt;  [1] \"ALGERIA\"        \"ARGENTINA\"      \"BRAZIL\"         \"CANADA\"        \n#&gt;  [5] \"EGYPT\"          \"ETHIOPIA\"       \"FRANCE\"         \"GERMANY\"       \n#&gt;  [9] \"INDIA\"          \"INDONESIA\"      \"IRAN\"           \"IRAQ\"          \n#&gt; [13] \"JAPAN\"          \"JORDAN\"         \"KENYA\"          \"MOROCCO\"       \n#&gt; [17] \"MOZAMBIQUE\"     \"PERU\"           \"CHINA\"          \"ROMANIA\"       \n#&gt; [21] \"SAUDI ARABIA\"   \"VIETNAM\"        \"RUSSIA\"         \"UNITED KINGDOM\"\n#&gt; [25] \"UNITED STATES\"\n\n\nReplace the values of choices in the Shiny app. Now use countries as its source. Preview the app, the drop-down should now have the 25 countries as options\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    plotOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderPlot({\n      # Your code here\n    })\n  },\n  options = list(height = 200)\n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#adding-the-plot",
    "href": "dashboards.html#adding-the-plot",
    "title": "9  Dashboards",
    "section": "9.3 Adding the plot",
    "text": "9.3 Adding the plot\nPort the code that creates the plot from the database\n\nLoad the ggplot2 library\n\n\nlibrary(ggplot2)\n\n\nInsert the full code that makes the country sales plot, the one in previous section. It should include the code that creates the local data, and the one that creates the plot itself. Place them in the area that says # Your code here. Make sure to replace, country, with input$country. Preview the app.\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    plotOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderPlot({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      sales_by_year |&gt;\n        ggplot() +\n        geom_col(aes(order_year, total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n    })\n  },\n  options = list(height = 200)\n)\n\n\nPrefix !! to the input$country entry where we create the sales_by_year variable. Preview the app.\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    plotOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderPlot({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      sales_by_year |&gt;\n        ggplot() +\n        geom_col(aes(order_year, total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n    })\n  },\n  options = list(height = 200)\n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#interactive-plots",
    "href": "dashboards.html#interactive-plots",
    "title": "9  Dashboards",
    "section": "9.4 Interactive plots",
    "text": "9.4 Interactive plots\nUsing ggiraph to add plot interactivity\n\nLoad the ggiraph library\n\n\nlibrary(ggiraph)\n\n\nIn the Shiny app’s code, replace:\n\n\nplotOUtput() with girafeOutput()\nrenderPlot() with renderGirafe()\ngeom_col() with geom_col_interactive()\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n    })\n  },\n  options = list(height = 200)\n)\n\n\nLoad the ggplot code to a variable called g, and then insert girafe(ggobj = g) as the last code in the output’s code\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      g &lt;- sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n\n      girafe(ggobj = g)\n    })\n  },\n  options = list(height = 200)\n)\n\n\nIn geom_col_interactive() add the following arguments:\n\ndata_id = order_year\ntooltip = total_price\n\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      g &lt;- sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price, data_id = order_year, tooltip = total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n\n      girafe(ggobj = g)\n    })\n  },\n  options = list(height = 200)\n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dashboards</span>"
    ]
  },
  {
    "objectID": "dashboards.html#plot-drill-down",
    "href": "dashboards.html#plot-drill-down",
    "title": "9  Dashboards",
    "section": "9.5 Plot drill-down",
    "text": "9.5 Plot drill-down\nShows how to go a level deeper in to what has been clicked\n\nAdd a showModal() that reacts when a column in the plot is clicked: observeEvent(input$sales_plot_selected, {showModal(modalDialog())})\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      g &lt;- sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price, data_id = order_year, tooltip = total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n\n      girafe(ggobj = g)\n    })\n    \n    observeEvent(input$sales_plot_selected, {\n      showModal(modalDialog())\n      })\n  },\n  options = list(height = 200)\n)\n\n\nAdd an options argument to the girafe() function call: options = list(opts_selection(type = \"single\")). Notice how only one column is highlighted at a time.\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      g &lt;- sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price, data_id = order_year, tooltip = total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n\n      girafe(ggobj = g, options = list(opts_selection(type = \"single\")))\n    })\n    \n    observeEvent(input$sales_plot_selected, {\n      showModal(modalDialog())\n      })\n  },\n  options = list(height = 200)\n)\n\n\nAdd a title argument to the modalDialog() call. Use combination of the country and sales_plot_selected values from input\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      g &lt;- sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price, data_id = order_year, tooltip = total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n\n      girafe(ggobj = g, options = list(opts_selection(type = \"single\")))\n    })\n\n    observeEvent(input$sales_plot_selected, {\n      showModal(\n        modalDialog(\n          title = paste0(input$country, \" - \", input$sales_plot_selected)\n        )\n      )\n    })\n  },\n  options = list(height = 200)\n)\n\n\nAdd a new renderPLot() function, and name it output$montly_sales. Inside, add the full code from the Plot data by month section in the previous unit. Make sure to replace country and year, with input$country, and input$sales_plot_selected respectively. Make sure to prefix !! when adding them to the filter() call\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      g &lt;- sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price, data_id = order_year, tooltip = total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n\n      girafe(ggobj = g, options = list(opts_selection(type = \"single\")))\n    })\n\n    output$monthly_sales &lt;- renderPlot({\n      sales_by_month &lt;- prep_orders |&gt;\n        filter(\n          n_name == !!input$country, \n          order_year == !!input$sales_plot_selected\n          ) |&gt;\n        group_by(order_month) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_month$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      sales_by_month |&gt;\n        ggplot() +\n        geom_col(aes(order_month, total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_month$order_month)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(\n          title = \"Sales by month\", \n          subtitle = paste0(input$country, \" - \", input$sales_plot_selected)\n          ) +\n        theme_light()\n    })\n\n    observeEvent(input$sales_plot_selected, {\n      showModal(\n        modalDialog(\n          title = paste0(input$country, \" - \", input$sales_plot_selected)\n        )\n      )\n    })\n  },\n  options = list(height = 200)\n)\n\n\nFinally, add plotOutput(\"monthly_sales\") to the modelDialog() call\n\n\nshinyApp(\n  ui = fluidPage(\n    selectInput(\"country\", \"Country:\", choices = countries, selected = \"FRANCE\"),\n    girafeOutput(\"sales_plot\")\n  ),\n  server = function(input, output) {\n    output$sales_plot &lt;- renderGirafe({\n      sales_by_year &lt;- prep_orders |&gt;\n        filter(n_name == !!input$country) |&gt;\n        group_by(order_year) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_year$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      g &lt;- sales_by_year |&gt;\n        ggplot() +\n        geom_col_interactive(aes(order_year, total_price, data_id = order_year, tooltip = total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_year$order_year)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(title = \"Sales by year\", subtitle = input$country) +\n        theme_light()\n\n      girafe(ggobj = g, options = list(opts_selection(type = \"single\")))\n    })\n\n    output$monthly_sales &lt;- renderPlot({\n      sales_by_month &lt;- prep_orders |&gt;\n        filter(\n          n_name == !!input$country, \n          order_year == !!input$sales_plot_selected\n          ) |&gt;\n        group_by(order_month) |&gt;\n        summarise(\n          total_price = sum(o_totalprice, na.rm = TRUE)\n        ) |&gt;\n        collect()\n\n      breaks &lt;- as.double(quantile(c(0, max(sales_by_month$total_price))))\n      breaks_labels &lt;- paste(round(breaks / 1000000000, 1), \"B\")\n\n      sales_by_month |&gt;\n        ggplot() +\n        geom_col(aes(order_month, total_price)) +\n        scale_x_continuous(breaks = unique(sales_by_month$order_month)) +\n        scale_y_continuous(breaks = breaks, labels = breaks_labels) +\n        xlab(\"Year\") +\n        ylab(\"Total Sales\") +\n        labs(\n          title = \"Sales by month\", \n          subtitle = paste0(input$country, \" - \", input$sales_plot_selected)\n          ) +\n        theme_light()\n    })\n\n    observeEvent(input$sales_plot_selected, {\n      showModal(\n        modalDialog(\n          title = paste0(input$country, \" - \", input$sales_plot_selected),\n          plotOutput(\"monthly_sales\")\n        )\n      )\n    })\n  },\n  options = list(height = 200)\n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Dashboards</span>"
    ]
  }
]