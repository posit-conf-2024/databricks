{
  "hash": "7550d40cbe853e30c3d3fdf179f1f499",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"dplyr basics\"\nexecute: \n  eval: true\n  freeze: true\n---\n\n\n\n\n## Create a table variable\n\n*Basics to how to point a variable in R to a table or view inside the database*\n\n\n1. Load the `dplyr`, `DBI` and `dbplyr` libraries\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(DBI)\n```\n:::\n\n\n2. *(Optional)* Open a connection to the database if it's currently closed\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- dbConnect(odbc::odbc(), \"warehouse\")\n```\n:::\n\n\n\n3. Use the `tbl()` and `in_catalog()` functions to create a reference to a table\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl(con, in_catalog(\"samples\", \"tpch\", \"customer\"))\n#> # Source:   table<`samples`.`tpch`.`customer`> [?? x 8]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>    c_custkey c_name         c_address c_nationkey c_phone c_acctbal c_mktsegment\n#>      <int64> <chr>          <chr>         <int64> <chr>       <dbl> <chr>       \n#>  1    412445 Customer#0004… \"0QAB3Oj…          21 31-421…     5358. BUILDING    \n#>  2    412446 Customer#0004… \"5u8MSby…          20 30-487…     9442. MACHINERY   \n#>  3    412447 Customer#0004… \"HC4ZT62…           7 17-797…     7869. AUTOMOBILE  \n#>  4    412448 Customer#0004… \"hJok1MM…           6 16-541…     6061. MACHINERY   \n#>  5    412449 Customer#0004… \"zAt1nZN…          14 24-710…     4974. HOUSEHOLD   \n#>  6    412450 Customer#0004… \"fUD6IoG…          20 30-293…     4406. BUILDING    \n#>  7    412451 Customer#0004… \"W2Ge0Qd…          20 30-590…     2290. BUILDING    \n#>  8    412452 Customer#0004… \"Ij4xiPI…          10 20-492…     3427. AUTOMOBILE  \n#>  9    412453 Customer#0004… \"4DmSxDP…          21 31-480…     4592. MACHINERY   \n#> 10    412454 Customer#0004… \"ZQfKDMU…           9 19-898…     2036. FURNITURE   \n#> # ℹ more rows\n#> # ℹ 1 more variable: c_comment <chr>\n```\n:::\n\n\n4. Load the reference, not the table data, into a variable\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers <- tbl(con, in_catalog(\"samples\", \"tpch\", \"customer\"))\n```\n:::\n\n\n\n5. Call the variable to see preview the data in the table\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers\n#> # Source:   table<`samples`.`tpch`.`customer`> [?? x 8]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>    c_custkey c_name         c_address c_nationkey c_phone c_acctbal c_mktsegment\n#>      <int64> <chr>          <chr>         <int64> <chr>       <dbl> <chr>       \n#>  1    412445 Customer#0004… \"0QAB3Oj…          21 31-421…     5358. BUILDING    \n#>  2    412446 Customer#0004… \"5u8MSby…          20 30-487…     9442. MACHINERY   \n#>  3    412447 Customer#0004… \"HC4ZT62…           7 17-797…     7869. AUTOMOBILE  \n#>  4    412448 Customer#0004… \"hJok1MM…           6 16-541…     6061. MACHINERY   \n#>  5    412449 Customer#0004… \"zAt1nZN…          14 24-710…     4974. HOUSEHOLD   \n#>  6    412450 Customer#0004… \"fUD6IoG…          20 30-293…     4406. BUILDING    \n#>  7    412451 Customer#0004… \"W2Ge0Qd…          20 30-590…     2290. BUILDING    \n#>  8    412452 Customer#0004… \"Ij4xiPI…          10 20-492…     3427. AUTOMOBILE  \n#>  9    412453 Customer#0004… \"4DmSxDP…          21 31-480…     4592. MACHINERY   \n#> 10    412454 Customer#0004… \"ZQfKDMU…           9 19-898…     2036. FURNITURE   \n#> # ℹ more rows\n#> # ℹ 1 more variable: c_comment <chr>\n```\n:::\n\n\n6. Set up the pointers to the other of the tables\n\n::: {.cell}\n\n```{.r .cell-code}\norders <- tbl(con, in_catalog(\"samples\", \"tpch\", \"orders\"))\nregions <- tbl(con, in_catalog(\"samples\", \"tpch\", \"region\"))\n```\n:::\n\n\n## Under the hood \n*Use `show_query()` to preview the SQL statement that will be sent to the database*\n\n1. SQL statement that actually runs when we ran `customers` as a command\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_query(customers)\n#> <SQL>\n#> SELECT *\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n2. Easily view the resulting query by adding `show_query()` in another piped command\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  show_query()\n#> <SQL>\n#> SELECT *\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n3. Insert `head()` in between the two statements to see how the SQL changes\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  head() %>%\n  show_query()\n#> <SQL>\n#> SELECT `customer`.*\n#> FROM `samples`.`tpch`.`customer`\n#> LIMIT 6\n```\n:::\n\n\n4. Use `sql_render()` and `simulate_mssql()` to see how the SQL statement changes from vendor to vendor\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  head() %>%\n  sql_render(con = simulate_mssql()) \n#> <SQL> SELECT TOP 6 `customer`.*\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n5. Use `explain()` to explore the query plan\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>% \n  head() %>% \n  explain()\n#> <SQL>\n#> SELECT `customer`.*\n#> FROM `samples`.`tpch`.`customer`\n#> LIMIT 6\n#> \n#> <PLAN>\n#>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                plan\n#> 1 == Physical Plan ==\\nCollectLimit 6\\n+- ColumnarToRow\\n   +- PhotonResultStage\\n      +- PhotonScan parquet samples.tpch.customer[c_custkey#8548L,c_name#8549,c_address#8550,c_nationkey#8551L,c_phone#8552,c_acctbal#8553,c_mktsegment#8554,c_comment#8555] DataFilters: [], DictionaryFilters: [], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/databricks-datasets/tpch/delta-001/customer], PartitionFilters: [], ReadSchema: struct<c_custkey:bigint,c_name:string,c_address:string,c_nationkey:bigint,c_phone:string,c_acctba..., RequiredDataFilters: []\\n\\n\\n== Photon Explanation ==\\nPhoton does not fully support the query because:\\n\\t\\tUnsupported node: CollectLimit 6.\\n\\nReference node:\\n\\tCollectLimit 6\\n\n```\n:::\n\n\n## Un-translated R commands\n*Review of how `dbplyr` handles R commands that have not been translated into a like-SQL command*\n\n1. Preview how `Sys.time()` is translated\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  mutate(today = Sys.time()) %>%\n  select(today) %>% \n  show_query()\n#> <SQL>\n#> SELECT Sys.time() AS `today`\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n2. Use Databricks's native commands, in this case `current_date()` \n(https://docs.databricks.com/en/sql/language-manual/functions/current_date.html)\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  mutate(today = current_date()) %>%\n  select(today) %>% \n  show_query()\n#> <SQL>\n#> SELECT current_date() AS `today`\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n3. Run the `dplyr` code to confirm it works\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  mutate(today = current_date()) %>%\n  select(today) %>%\n  head()\n#> # Source:   SQL [6 x 1]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>   today     \n#>   <date>    \n#> 1 2024-05-09\n#> 2 2024-05-09\n#> 3 2024-05-09\n#> 4 2024-05-09\n#> 5 2024-05-09\n#> 6 2024-05-09\n```\n:::\n\n\n## Using bang-bang\n*Intro on passing unevaluated code to a dplyr verb*\n\n1. Preview how `Sys.time()` is translated\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  mutate(today = Sys.time()) %>%\n  show_query()\n#> <SQL>\n#> SELECT `customer`.*, Sys.time() AS `today`\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n2. Preview how `Sys.time()` is translated when prefixing `!!`\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  mutate(today = !! Sys.time()) %>%\n  show_query()\n#> <SQL>\n#> SELECT `customer`.*, '2024-05-09T19:35:20Z' AS `today`\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n3. Preview how `Sys.time()` is translated when prefixing `!!`\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  mutate(today = !!Sys.time()) %>%\n  select(today) %>%\n  head()\n#> # Source:   SQL [6 x 1]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>   today               \n#>   <chr>               \n#> 1 2024-05-09T19:35:20Z\n#> 2 2024-05-09T19:35:20Z\n#> 3 2024-05-09T19:35:20Z\n#> 4 2024-05-09T19:35:20Z\n#> 5 2024-05-09T19:35:20Z\n#> 6 2024-05-09T19:35:20Z\n```\n:::\n\n\n## knitr SQL engine\n\n1. Copy the result of the latest `show_query()` exercise\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  mutate(today = !!Sys.time()) %>%\n  select(today) %>% \n  show_query()\n#> <SQL>\n#> SELECT '2024-05-09T19:35:21Z' AS `today`\n#> FROM `samples`.`tpch`.`customer`\n```\n:::\n\n\n2. Paste the result in this SQL chunk\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT '2024-05-09T19:35:08Z' AS `today`\nFROM `samples`.`tpch`.`customer`\n```\n\n\n<div class=\"knitsql-table\">\n\n\nTable: Displaying records 1 - 10\n\n|today                |\n|:--------------------|\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n|2024-05-09T19:35:08Z |\n\n</div>\n:::\n\n\n\n## Basic aggregation\n*A couple of `dplyr` commands that run in-database*\n\n1. How many records are in the **airport** table?\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers  %>%\n  count()\n#> # Source:   SQL [1 x 1]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>         n\n#>   <int64>\n#> 1  750000\n```\n:::\n\n\n2. What is the average character length of the airport codes? How many characters is the longest and the shortest airport name?\n\n::: {.cell}\n\n```{.r .cell-code}\ncustomers %>%\n  summarise(\n    avg_customer_balance = mean(c_acctbal, na.rm = TRUE),\n    max_customer_balance = max(c_acctbal, na.rm = TRUE),\n    min_customer_balance = min(c_acctbal, na.rm = TRUE),\n    total_records = n()\n  )\n#> # Source:   SQL [1 x 4]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>   avg_customer_balance max_customer_balance min_customer_balance total_records\n#>                  <dbl>                <dbl>                <dbl>       <int64>\n#> 1                4501.               10000.               -1000.        750000\n```\n:::\n\n\n3. How many records are in the **orders** table?\n\n::: {.cell}\n\n```{.r .cell-code}\norders %>%\n  count()\n#> # Source:   SQL [1 x 1]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>         n\n#>   <int64>\n#> 1 7500000\n```\n:::\n\n\n4. What is the highest total price of an order?\n\n::: {.cell}\n\n```{.r .cell-code}\norders %>%\n  summarise(x = max(o_totalprice, na.rm = TRUE))\n#> # Source:   SQL [1 x 1]\n#> # Database: Spark SQL 3.1.1[token@Spark SQL/hive_metastore]\n#>         x\n#>     <dbl>\n#> 1 569370.\n```\n:::\n\n\n5. What is the SQL statement sent in exercise 4?\n\n::: {.cell}\n\n```{.r .cell-code}\norders %>%\n  summarise(x = max(o_totalprice, na.rm = TRUE)) %>%\n  show_query()\n#> <SQL>\n#> SELECT MAX(`o_totalprice`) AS `x`\n#> FROM `samples`.`tpch`.`orders`\n```\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}