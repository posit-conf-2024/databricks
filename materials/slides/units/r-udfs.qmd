---
engine: knitr
---

```{r}
#| include: false
unit_no <- 6
```

# {background-image="assets/background/content-slide.svg" background-size="1700px" background-color="#2a7070"}

:::{.content-slide-title}
Unit `r unit_no`
:::

:::{.content-slide}
Intro to <br/> 
R UDFs
:::

## [What is an UDF?]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

<br/>

:::{.custom}
:::{.incremental1}
- Stands for **"User Defined Function"**
- Enables operations not built-in Spark
- Can be written in Scala, Python, or R
:::
:::

## [Ok, but what does that mean?!]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

## [Ok, but what does that mean?!]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

[
<br/><br/><br/>
We can run R code *inside* Spark! ðŸŽ‰ ðŸŽ‰
]{style="font-size:100px;line-height:0.5;font-weight:600;color:#579297;"}

## [But first... parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="25%"}
:::
:::{.column width="75%"}
[Spark partitions the data logically]{style="font-size:60px;line-height:0.5;font-weight:600;color:#579297;"}
:::
:::

![](assets/r-udfs/spark-1.png){.absolute top="200" left="400" width="400"}
![](assets/r-udfs/spark-2.png){.absolute top="190" left="900" width="400"}
![](assets/r-udfs/spark-3.png){.absolute top="390" left="900" width="400"}
![](assets/r-udfs/spark-4.png){.absolute top="590" left="900" width="400"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="20%"}
:::
:::{.column width="70%"}
[Each node gets one, or several partitions]{style="font-size:60px;line-height:0.5;font-weight:600;color:#579297;"}
:::
:::

![](assets/r-udfs/spark-6.png){.absolute top="250" left="320" width="200"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="100" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="420" left="100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="250" left="1320" width="200"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="1100" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="420" left="1100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="1180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="500" left="770" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="550" left="550" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="670" left="550" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="800" left="660" width="200"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="5%"}
:::
:::{.column width="95%"}
[The cluster runs jobs that process each partition in parallel]{style="font-size:55px;line-height:0.5;font-weight:600;color:#579297;"}
:::
:::

![](assets/r-udfs/spark-6.png){.absolute top="250" left="320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="293" left="90" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="100" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="420" left="100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="250" left="1320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="413" left="1090" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="1100" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="420" left="1100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="1180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="500" left="770" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="542" left="540" width="220"}
![](assets/r-udfs/spark-3.png){.absolute top="550" left="550" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="670" left="550" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="800" left="660" width="200"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="10%"}
:::
:::{.column width="90%"}
[Your R function runs on each partition independently]{style="font-size:55px;line-height:0.5;font-weight:600;color:#579297;"}
:::
:::

![](assets/r-udfs/spark-6.png){.absolute top="250" left="320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="293" left="90" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="100" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="420" left="100" width="200"}
![](assets/r-udfs/r-logo.png){.absolute top="310" left="150" width="100"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="250" left="1320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="413" left="1090" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="1100" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="420" left="1100" width="200"}
![](assets/r-udfs/r-logo.png){.absolute top="430" left="1150" width="100"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="1180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="500" left="770" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="542" left="540" width="220"}
![](assets/r-udfs/spark-3.png){.absolute top="550" left="550" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="670" left="550" width="200"}
![](assets/r-udfs/r-logo.png){.absolute top="560" left="590" width="100"}
![](assets/r-udfs/spark.svg){.absolute top="800" left="660" width="200"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="1%"}
:::
:::{.column width="99%"}
[Results are appended, and returned to user as a single table]{style="font-size:60px;line-height:0.5;font-weight:600;color:#579297;"}
:::
:::

![](assets/r-udfs/spark-2.png){.absolute top="210" left="600" width="400"}
![](assets/r-udfs/spark-3.png){.absolute top="400" left="600" width="400"}
![](assets/r-udfs/spark-4.png){.absolute top="590" left="600" width="400"}

## [Accessing in R]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.columns}
:::{.column width="42%"}

:::{.incremental1}
:::: {style="text-align: left; float:left;"}
[
`spark_apply()` enables acces to the R runtime installed in the cluster. 
The R function will run over each individual partition.
]{style="color:#666; font-weight:500;font-size:52px;"} 
<br><br>
[
In this case, 4 partitions of 8 rows each. The results from each partition are 
merged in a single table.
]{style="color:#666; font-weight:500;font-size:52px;"} 


:::
:::

:::
:::{.column width="58%"}
:::{.code-slim-45}
```r
tbl_mtcars <- copy_to(sc, mtcars)

tbl_mtcars |> 
  spark_apply(nrow)
#> # Source:   table<`sparklyr_tmp_table_d83f7f26`> [4 x 1]
#> # Database: spark_connection
#>       x
#>   <dbl>
#> 1     8
#> 2     8
#> 3     8
#> 4     8
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r unit_no`.1
:::

## [Group by variable]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}


:::{.columns}
:::{.column width="2%"}
:::
:::{.column width="95%"}
[
Use `group_by` to override the partitions, and divide data by a column
<br> 
]{style="color:#666; font-weight:500;font-size:52px;"} 

:::
:::

:::{.code-slim-45}

```r
tbl_mtcars |> 
  spark_apply(nrow, group_by = "am")
#> # Source:   table<`sparklyr_tmp_table_c30d3aba_515f`> [2 x 2]
#> # Database: spark_connection
#>      am     x
#>   <dbl> <dbl>
#> 1     0    19
#> 2     1    13
```
:::


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r unit_no`.2
:::


## [Custom functions]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/r-udfs/expectations.png){.absolute top="250" left="50" width="1800"}

:::{.columns}
:::{.column width="4%"}
:::
:::{.column width="95%"}
[Create custom R functions that expects and outputs tables]{style="font-size:58px;line-height:0.5;font-weight:600;color:#579297;"}
:::
:::

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r unit_no`.3
:::

## [R packages]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.columns}
:::{.column width="42%"}

:::{.incremental1}
:::: {style="text-align: left; float:left;"}
[
R packages are available for use, but they have to be referenced from within
your function's code. 
<br/><br/>
You can reference them using `library()` or by directly calling them using `::`,
for example: `broom::tidy()`
]{style="color:#666; font-weight:500;font-size:52px;"} 


:::
:::

:::
:::{.column width="58%"}
:::{.code-slim-35}
```r
tbl_mtcars |> 
  spark_apply(function(x) {
      library(broom)
      model <- lm(mpg ~ ., x)
      tidy(model)[1,]
      },
    group_by = "am"
    )
#> # Source:   table<`sparklyr_tmp_table_f2ddbd6d`> [2 x 6]
#> # Database: spark_connection
#>      am term        estimate std_error statistic p_value
#>   <dbl> <chr>          <dbl>     <dbl>     <dbl>   <dbl>
#> 1     0 (Intercept)     8.64      21.5     0.402   0.697
#> 2     1 (Intercept)  -138.        69.1    -1.99    0.140
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r unit_no`.4
:::

## [Dependencies in Databricks]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

[In Databricks, Spark clusters are tied to a Databricks Runtime version. And each
version of DBR contain [specific versions](https://docs.databricks.com/en/release-notes/runtime/15.3.html#system-environment) of:]{style="color:#666; font-weight:500;font-size:52px;"} 

:::{.columns}
:::{.column width="40%"}
:::
:::{.column width="40%"}
- Spark
- Java
- Scala
- Python
- R
:::
:::

[A set of Python and R packages are also pre-installed in the cluster.]{style="color:#666; font-weight:500;font-size:52px;"} 


:::{.footer}
https://docs.databricks.com/en/release-notes/runtime/15.3.html#installed-r-libraries
:::

## [Missing packages]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="40%"}
- Additional packages can be installed 
- In the Databricks portal, use the `Libraries` tab of the cluster
- Python packages will install from PyPi
- R packages will install from CRAN
:::
:::{.column width="0%"}
:::
:::

![](assets/r-udfs/install.png){.absolute top="140" left="700" width="900"}

## [Key considerations]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="55%"}
:::{.incremental1}
- You are dealing with **two** different environments, your machine and DBR
- Your Python version **must** match that of your Spark cluster
- R version can be different, but could be source of errors (a.k.a. *"works locally, but not in the cluster"*)
- Same for Python, and R packages. OK if mismatch, but could be errors
:::
:::
:::{.column width="45%"}
:::{.code-slim-35}
```r
tbl_mtcars <- copy_to(sc, mtcars)
my_func <- function(x) R.Version()$minor
# Local output
my_func(mtcars)
#> [1] "4.0"
# Remote output
tbl_mtcars |> 
  spark_apply(my_func) |> 
  head(1)
#> # Source:   SQL [1 x 1]
#> # Database: spark_connection
#>   x    
#>   <chr>
#> 1 3.1
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r unit_no`.5
:::

## [Python UDFs]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/r-udfs/python-udfs.png){.absolute top="270" left="50" width="1800"}

:::{.columns}
:::{.column width="7%"}
:::
:::{.column width="91%"}
[Python code is packaged and sent to Databricks which runs it]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

## [R UDFs]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

![](assets/r-udfs/r-udfs.png){.absolute top="230" left="50" width="1800"}

:::{.columns}
:::{.column width="0%"}
:::
:::{.column width="100%"}
[`sparklyr` inserts your R code in a Python script. Uses `rpy2` to access R]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

:::{.footer}
https://spark.posit.co/deployment/databricks-connect-udfs.html
:::