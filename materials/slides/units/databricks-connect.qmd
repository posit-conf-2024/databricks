---
engine: knitr
---

```{r}
#| include: false
if(!("no_databricks" %in% ls())) no_databricks <- 1
```

# {background-image="assets/background/content-slide.svg" background-size="1700px" background-color="#2a7070"}

:::{.content-slide-title}
Unit `r no_databricks`
:::

:::{.content-slide}
Databricks <br/> Connect
:::

## [Databricks Connect]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="50%"}

:::{.custom2}
:::{.incremental1}
- Spark Connect, offers **true** remote connectivity
- Uses **gRPC** to as the communication interface
- **Databricks Connect 'v2'** is based on Spark Connect (DBR 13+)
:::
:::

:::
:::{.column width="45%"}
:::
:::

![](assets/databricks-connect/grpc){.absolute top="264" left="900" width="670"}


## [Databricks Connect]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="37%"}

:::{.custom2}
:::{.incremental1}
- `databricks-connect` integrates with gRPC, by wrapping `pyspark`
- `pyspark` is the most developed **Spark Connect** interface
:::
:::

:::
:::{.column width="60%"}
:::
:::

![](assets/databricks-connect/python.png){.absolute top="264" left="572" width="998"}

## [Databricks Connect]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.columns}
:::{.column width="4%"}
:::
:::{.column width="96%"}
[`sparklyr` integrates with `databricks-connect` via `reticulate`]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::



![](assets/databricks-connect/db-connect.png){.absolute top="200" left="70" width="1500"}

## [Why not just use 'reticulate'?]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

[**sparklyr** extends functionality and user experience:]{style="font-size:65px;line-height:1;font-weight:400;color:#666;"}


:::{.columns}
:::{.column width="45%"}

:::{.custom2}
:::{.incremental1}
  - `dplyr` back-end
  - `DBI` back-end
  - R UDFs 
  - **RStudio**, and **Positron**, Connections Pane integration
:::
:::

:::
:::{.column width="55%"}
:::{.code-slim-35}
```r
library(sparklyr) 
sc <- spark_connect(method = "databricks_connect") 

trips <- tbl(sc, I("samples.nyctaxi.trips")) 

trips |>  
  group_by(pickup_zip) |> 
  summarise( 
    count = n(), 
    avg_distance = mean(trip_distance) 
    )
```
:::
:::
:::

## [Getting started]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.columns}
:::{.column width="42%"}

:::{.custom2}
:::{.incremental1}
- Python 3.10+ 
- A Python environment with `databricks-connect` and its dependencies
- `pysparklyr` extension
:::
:::

:::
:::{.column width="58%"}
:::{.code-slim-35}
```r
install.packages("pysparklyr")
library(sparklyr) 
sc <- spark_connect(
  cluster_id = "1026-175310-7cpsh3g8",
  method = "databricks_connect"
  )
```
:::
:::
:::

## [Getting started]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.columns}
:::{.column width="42%"}

:::{.incremental1}
:::: {style="text-align: left; float:left;"}
[
`pysparklyr` automatically, checks for, and installs the needed Python packages. <br/>
<br/>
Once you confirm, it will create a new virtual environment, and installs the 
packages.
]{style="color:#666; font-weight:500;font-size:52px;"} 
:::
:::

:::
:::{.column width="58%"}
:::{.code-slim-35}
```r
install.packages("pysparklyr")
library(sparklyr) 
sc <- spark_connect(
  cluster_id = "1026-175310-7cpsh3g8",
  method = "databricks_connect"
  )
#> ! Retrieving version from cluster '1026-175310-7cpsh3g8'
#> Cluster version: '14.1' 
#> ! No viable Python Environment was identified for 
#> Databricks Connect version 14.1 
#> Do you wish to install Databricks Connect version 14.1? 
#> 1: Yes
#> 2: No
#> 3: Cancel
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r no_databricks`.1
:::

## {background-image="assets/background/boxed-white.svg" background-size="1700px" background-color="#fff"}

<br/><br/><br/><br/><br/>

:::{.columns}
:::{.column width="10%"}
:::
:::{.column width="90%"}
:::{.content-slide}
[Understanding Spark's data caching]{style="font-size:130px; line-height: 0.5;"}
:::
:::
:::

## [Spark's data capabilities]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="50%"}
:::{.custom2}
:::{.incremental2}
- Spark has the ability to cache large amounts of data
- Amount of data is limited by the size of the cluster
- Data in cache is the fastest way to access data in Spark
:::
:::
:::
:::{.column width="50%"}
![](assets/databricks-connect/spark-data-2.png)
:::
:::

## [Default approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="20%"}
:::
:::{.column width="70%"}
[Data is read and processed. Results go to R.]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/warehouse-r.png){.absolute top="200" left="220" width="1100"}

## [About this approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom2}
:::{.incremental1}
- [Well suited when exploring the entirety of the data. Usually to find relevant variables]{style="font-size:75px;"}
- [Not efficient when accessing the same fields and rows over and over]{style="font-size:75px;"}

:::
:::

## [Uploading data from R]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="2%"}
:::
:::{.column width="98%"}
[`copy_to()` to upload data to Spark. Use for "enrichment" purposes]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/r-ram.png){.absolute top="200" left="220" width="1100"}


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r no_databricks`.2
:::

## [Caching data]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="12%"}
:::
:::{.column width="80%"}
[2 step process. first, cache all or some data in memory]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/warehouse-ram.png){.absolute top="200" left="220" width="1100"}

## [Caching data]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="15%"}
:::
:::{.column width="80%"}
[Second, read and process from memory. *Much faster*]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/ram-r.png){.absolute top="200" left="220" width="1100"}

## [About this approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom2}
:::{.incremental1}
- [Well suited when accessing the same fields and rows over and over]{style="font-size:75px;"}
- [Best when running models and UDFs *(More about this later)*]{style="font-size:75px;"}

:::
:::


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r no_databricks`.3
:::

## [Reading files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="20%"}
:::
:::{.column width="80%"}
[By default, files are read and saved to memory]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/files-ram.png){.absolute top="200" left="220" width="1100"}

## [Reading files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="10%"}
:::
:::{.column width="90%"}
[Afterwards, the data is read from memory for processing]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/ram-r.png){.absolute top="200" left="220" width="1100"}

## [About this approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom2}
:::{.incremental1}
- Read files using the `spark_read...` family of functions
- The file path needs to be relative to your Databricks environment
- [Databricks Volumes](https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html) are ideal for this approach
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r no_databricks`.4
:::

## ["Mapping" files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="15%"}
:::
:::{.column width="85%"}
[The files can be mapped but not imported to memory]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/files-map.png){.absolute top="200" left="220" width="1100"}

## ["Mapping" files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="20%"}
:::
:::{.column width="80%"}
[Data is read and processed. Results sent to R.]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/files-r.png){.absolute top="200" left="220" width="1100"}

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r no_databricks`.5
:::

## [Partial cache]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="12%"}
:::
:::{.column width="80%"}
[Alternatively, you can cache specific data from the files]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/files-ram-partial.png){.absolute top="200" left="220" width="1100"}

## [Partial cache]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="10%"}
:::
:::{.column width="90%"}
[Afterwards, the data is read from memory for processing]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/ram-r.png){.absolute top="200" left="220" width="1100"}

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r no_databricks`.6
:::

## [Very large files, read or map]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom2}
:::{.incremental1}
- [Reading, you "pay" in time at the beginning]{style="font-size:65px;"}
- [Mapping, you "pay" in time as you access the data]{style="font-size:65px;"}
- [Extended EDA, reading would be better]{style="font-size:65px;"}
- [EDA of targeted data (specific days or variables), partial caching would be better]{style="font-size:65px;"}
- [Jobs that pull a predetermined set of data, mapping would be better]{style="font-size:65px;"}
:::
:::

## [End game]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="2%"}
:::
:::{.column width="98%"}
[Combine the data from any approach. Cache the resulting table]{style="font-size:54px;line-height:1;font-weight:400;color:#666;"}
:::
:::

![](assets/databricks-connect/my-data-set.png){.absolute top="250" left="100" width="1300"}

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.green-slide}
Exercise `r no_databricks`.7
:::
