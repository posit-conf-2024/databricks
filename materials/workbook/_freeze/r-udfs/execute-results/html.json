{
  "hash": "4801c433d59fdfc625794d165295a6af",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Intro to R UDFs\"\nexecute: \n  eval: true\n  freeze: true\n  warning: true\n---\n\n\n\n\n\n\n## Catch up {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(dplyr)\nsc <- spark_connect(method = \"databricks_connect\")\n#> ! Changing host URL to: https://rstudio-partner-posit-default.cloud.databricks.com\n#> ℹ Retrieving info for cluster:'1026-175310-7cpsh3g8'\n#> ✔ Cluster: '1026-175310-7cpsh3g8' | DBR: '14.1' [336ms]\n#> \n#> ℹ Attempting to load 'r-sparklyr-databricks-14.1'\n#> ✔ Python environment: 'r-sparklyr-databricks-14.1' [998ms]\n#> \n#> ℹ Connecting to '14.1 cluster'\n#> ✔ Connected to: '14.1 cluster' [6ms]\n#> \n```\n:::\n\n\n\n\n## Simple operations\n*Trying out very simple operation to become familiar with the process*\n\n1. Use `copy_to()` to send `mtcars` to the cluster. Load it to a variable called\n`tbl_mtcars`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars <- copy_to(sc, mtcars)\n```\n:::\n\n\n\n\n2. Pipe `tbl_mtcars` to `spark_apply()`. Use `nrow` as the function to run\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow)\n#> Some features are not enabled in this build of Arrow. Run `arrow_info()` for more information.\n#> The repository you retrieved Arrow from did not include all of Arrow's features.\n#> You can install a fully-featured version by running:\n#> `install.packages('arrow', repos = 'https://apache.r-universe.dev')`.\n#> \n#> Attaching package: 'arrow'\n#> The following object is masked from 'package:lubridate':\n#> \n#>     duration\n#> The following object is masked from 'package:utils':\n#> \n#>     timestamp\n#> To increase performance, use the following schema:\n#> columns = \"x long\"\n#> # Source:   table<`sparklyr_tmp_table_72305a16_a3a0_4a42_b6bc_4ec898913d25`> [4 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2     8\n#> 3     8\n#> 4     8\n```\n:::\n\n\n\n\n3. Switch the function to use in `spark_apply()` to `dim`. Notice how it\nreturns more rows, because coercing the size 2 vector creates a 2 row data frame\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(dim)\n#> To increase performance, use the following schema:\n#> columns = \"x long\"\n#> # Source:   table<`sparklyr_tmp_table_4a33c1ee_581e_4aba_be6f_250a300c8dc4`> [8 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2    11\n#> 3     8\n#> 4    11\n#> 5     8\n#> 6    11\n#> 7     8\n#> 8    11\n```\n:::\n\n\n\n\n## Group by variable\n*Write and run simple grouping commands*\n\n\n1. Go back to using `nrow` again for `spark_apply()`. Remember to \npass `columns = \"x long\"`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, columns = \"x long\")\n#> # Source:   table<`sparklyr_tmp_table_944dce94_e1e0_4bd2_aa66_08e8a1ff2588`> [4 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2     8\n#> 3     8\n#> 4     8\n```\n:::\n\n\n\n\n2. Add the `group_by` argument, with the value of `\"am\"`. There should be an \nerror. This is because there are 2 variables in the result, instead of one, and\nwe defined `x` only in `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"x long\")\n```\n:::\n\n\n\n\n3. Insert `am long,` at the beginning of `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"am long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_c25deba7_828f_41cf_8258_096304f86027`> [2 x 2]\n#> # Database: spark_connection\n#>      am     x\n#>   <dbl> <dbl>\n#> 1     0    19\n#> 2     1    13\n```\n:::\n\n\n\n4. To see how the name we pass does not have to match the variable name, change\n`am` to `notam` in `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"notam long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_b8582c21_4d0c_4524_8157_b04420b5e9cc`> [2 x 2]\n#> # Database: spark_connection\n#>   notam     x\n#>   <dbl> <dbl>\n#> 1     0    19\n#> 2     1    13\n```\n:::\n\n\n\n\n5. Change the grouping variable to \"cyl\", make sure to update that in the `columns`\nargument as well\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"cyl\", columns = \"cyl long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_dfbe0b4c_3309_4ca4_8ddf_cae59efc5385`> [3 x 2]\n#> # Database: spark_connection\n#>     cyl     x\n#>   <dbl> <dbl>\n#> 1     4    11\n#> 2     6     7\n#> 3     8    14\n```\n:::\n\n\n\n\n\n## Custom functions\n*Create simple custom functions to send to Spark*\n\n1. In `spark_apply()`, pass `function(x) x` as the function. This will \nreturn the entire `mtcars` data set \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(function(x) x)\n#> To increase performance, use the following schema:\n#> columns = \"mpg double, cyl double, disp double, hp double, drat double, wt\n#> double, qsec double, vs double, am double, gear double, carb double\"\n#> # Source:   table<`sparklyr_tmp_table_b4d7de85_c6f8_4671_bd3a_e03f3fc41a2b`> [?? x 11]\n#> # Database: spark_connection\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#> # ℹ more rows\n```\n:::\n\n\n\n2. Modify the function to return only the \"mpg\", \"cyl\", and \"disp\" variables\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(function(x) x[, c(\"mpg\", \"cyl\", \"disp\")])\n#> To increase performance, use the following schema:\n#> columns = \"mpg double, cyl double, disp double\"\n#> # Source:   table<`sparklyr_tmp_table_0b4eeebe_cf55_43ff_86ac_64883794a3fb`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n3. Add the recommended `columns` spec from \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x) x[, c(\"mpg\", \"cyl\", \"disp\")], \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_fa57dd09_f2e6_4f81_aac3_7db572b7295c`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n4.  Add a filter that returns the highest \"mpg\". Notice that instead of 1\nrecord, it returns several. That is because the filter is being\nprocessed per partition. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x) x[x$mpg == max(x$mpg), c(\"mpg\", \"cyl\", \"disp\")], \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_564176df_8bce_4be0_9d80_f39381e87366`> [4 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  24.4     4 147. \n#> 2  22.8     4 141. \n#> 3  33.9     4  71.1\n#> 4  30.4     4  95.1\n```\n:::\n\n\n\n\n\n5. Change the filter to display any records with an \"mpg\" over 25\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x) x[x$mpg > 25, c(\"mpg\", \"cyl\", \"disp\")], \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_6b82fbb9_7476_462b_8a40_04bcb1f69811`> [6 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  32.4     4  78.7\n#> 2  30.4     4  75.7\n#> 3  33.9     4  71.1\n#> 4  27.3     4  79  \n#> 5  26       4 120. \n#> 6  30.4     4  95.1\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}