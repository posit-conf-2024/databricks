{
  "hash": "bcad148814b56c6ca139c16c548348ee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Intro to R UDFs\"\nexecute: \n  eval: true\n  freeze: true\n  warning: true\n---\n\n\n\n\n\n\n## Catch up {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(dplyr)\nsc <- spark_connect(method = \"databricks_connect\")\n#> ! Changing host URL to: https://rstudio-partner-posit-default.cloud.databricks.com\n#> ℹ Retrieving info for cluster:'1026-175310-7cpsh3g8'\n#> ✔ Cluster: '1026-175310-7cpsh3g8' | DBR: '14.1' [453ms]\n#> \n#> ℹ Attempting to load 'r-sparklyr-databricks-14.1'\n#> ✔ Python environment: 'r-sparklyr-databricks-14.1' [1.1s]\n#> \n#> ℹ Connecting to '14.1 cluster'\n#> ✔ Connected to: '14.1 cluster' [6ms]\n#> \n```\n:::\n\n\n\n\n## Simple operations\n*Trying out very simple operation to become familiar with the process*\n\n1. Use `copy_to()` to send `mtcars` to the cluster. Load it to a variable called\n`tbl_mtcars`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars <- copy_to(sc, mtcars)\n```\n:::\n\n\n\n\n2. Pipe `tbl_mtcars` to `spark_apply()`. Use `nrow` as the function to run\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow)\n#> Some features are not enabled in this build of Arrow. Run `arrow_info()` for more information.\n#> The repository you retrieved Arrow from did not include all of Arrow's features.\n#> You can install a fully-featured version by running:\n#> `install.packages('arrow', repos = 'https://apache.r-universe.dev')`.\n#> \n#> Attaching package: 'arrow'\n#> The following object is masked from 'package:lubridate':\n#> \n#>     duration\n#> The following object is masked from 'package:utils':\n#> \n#>     timestamp\n#> To increase performance, use the following schema:\n#> columns = \"x long\"\n#> # Source:   table<`sparklyr_tmp_table_68074d46_d958_49e9_960b_7de08000d1b0`> [4 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2     8\n#> 3     8\n#> 4     8\n```\n:::\n\n\n\n\n3. Switch the function to use in `spark_apply()` to `dim`. Notice how it\nreturns more rows, because coercing the size 2 vector creates a 2 row data frame\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(dim)\n#> To increase performance, use the following schema:\n#> columns = \"x long\"\n#> # Source:   table<`sparklyr_tmp_table_7f127689_e675_4a90_b494_783f017a1c9c`> [8 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2    11\n#> 3     8\n#> 4    11\n#> 5     8\n#> 6    11\n#> 7     8\n#> 8    11\n```\n:::\n\n\n\n\n## Group by variable\n*Write and run simple grouping commands*\n\n\n1. Go back to using `nrow` again for `spark_apply()`. Remember to \npass `columns = \"x long\"`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, columns = \"x long\")\n#> # Source:   table<`sparklyr_tmp_table_3cee338f_b9c1_4728_8e06_17819b7e5944`> [4 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2     8\n#> 3     8\n#> 4     8\n```\n:::\n\n\n\n\n2. Add the `group_by` argument, with the value of `\"am\"`. There should be an \nerror. This is because there are 2 variables in the result, instead of one, and\nwe defined `x` only in `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"x long\")\n```\n:::\n\n\n\n\n3. Insert `am long,` at the beginning of `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"am long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_62e1d962_d23a_41cb_8244_f42a603522c0`> [2 x 2]\n#> # Database: spark_connection\n#>      am     x\n#>   <dbl> <dbl>\n#> 1     0    19\n#> 2     1    13\n```\n:::\n\n\n\n4. To see how the name we pass does not have to match the variable name, change\n`am` to `notam` in `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"notam long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_96655c7b_49a0_4444_9094_62fd1f353c59`> [2 x 2]\n#> # Database: spark_connection\n#>   notam     x\n#>   <dbl> <dbl>\n#> 1     0    19\n#> 2     1    13\n```\n:::\n\n\n\n\n5. Change the grouping variable to \"cyl\", make sure to update that in the `columns`\nargument as well\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"cyl\", columns = \"cyl long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_eeca85d8_82b9_4136_9945_5ff3010a0f14`> [3 x 2]\n#> # Database: spark_connection\n#>     cyl     x\n#>   <dbl> <dbl>\n#> 1     4    11\n#> 2     6     7\n#> 3     8    14\n```\n:::\n\n\n\n\n\n## Custom functions\n*Create simple custom functions to send to Spark*\n\n1. In `spark_apply()`, pass `function(x) x` as the function. This will \nreturn the entire `mtcars` data set \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(function(x) x)\n#> To increase performance, use the following schema:\n#> columns = \"mpg double, cyl double, disp double, hp double, drat double, wt\n#> double, qsec double, vs double, am double, gear double, carb double\"\n#> # Source:   table<`sparklyr_tmp_table_99a4f56e_d0f4_4be8_8f13_f3d0f90c90fd`> [?? x 11]\n#> # Database: spark_connection\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#> # ℹ more rows\n```\n:::\n\n\n\n2. Modify the function to return only the \"mpg\", \"cyl\", and \"disp\" variables\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(function(x) x[, c(\"mpg\", \"cyl\", \"disp\")])\n#> To increase performance, use the following schema:\n#> columns = \"mpg double, cyl double, disp double\"\n#> # Source:   table<`sparklyr_tmp_table_636b677b_54eb_470f_b402_ce2a2c1a5999`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n3. Add the recommended `columns` spec from \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x) x[, c(\"mpg\", \"cyl\", \"disp\")], \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_21c17e7d_d626_44ef_8471_906d1c5f3e31`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n4. Make your custom function into a 'multi-line' function\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      x[, c(\"mpg\", \"cyl\", \"disp\")]\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_0a4af34a_7090_4db6_b3bf_cacdf4356000`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n5. Assign the data selection step to a variable called `out`, and then use it as \nthe output of the function\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_f7e2a28e_84ee_4bf6_879e_c65d13db9145`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n6. Add a filter step that returns the highest \"mpg\". Notice that instead of 1\nrecord, it returns several. That is because the filter is being\nprocessed per partition. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out <- out[out$mpg == max(out$mpg), ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_6521d013_7d2b_404a_b54f_cf1275df16b1`> [4 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  24.4     4 147. \n#> 2  22.8     4 141. \n#> 3  33.9     4  71.1\n#> 4  30.4     4  95.1\n```\n:::\n\n\n\n\n\n7. Change the filter to display any records with an \"mpg\" over 25\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out <- out[out$mpg > 25, ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_fb583493_f93f_4516_a907_38596e865523`> [6 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  32.4     4  78.7\n#> 2  30.4     4  75.7\n#> 3  33.9     4  71.1\n#> 4  27.3     4  79  \n#> 5  26       4 120. \n#> 6  30.4     4  95.1\n```\n:::\n\n\n\n\n8. Insert a step that modifies `cyl`. It should make it add 1 to the value. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out$cyl <- out$cyl + 1\n      out <- out[out$mpg > 25, ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_ea184401_fafd_4f21_84ef_f032ed8955c1`> [6 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  32.4     5  78.7\n#> 2  30.4     5  75.7\n#> 3  33.9     5  71.1\n#> 4  27.3     5  79  \n#> 5  26       5 120. \n#> 6  30.4     5  95.1\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}