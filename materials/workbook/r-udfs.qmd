---
title: "Intro to R UDFs"
execute: 
  eval: true
  freeze: true
  warning: true
---

```{r, setup}
#| include: false

library(dplyr)
library(dbplyr)
library(sparklyr)
library(tidymodels)
library(tidyverse)
```

## Catch up {.unnumbered}

```{r}
library(sparklyr)
library(dplyr)
sc <- spark_connect(method = "databricks_connect")
```

## Simple operations
*Trying out very simple operation to become familiar with the process*

1. Use `copy_to()` to send `mtcars` to the cluster. Load it to a variable called
`tbl_mtcars`

```{r}
tbl_mtcars <- copy_to(sc, mtcars)
```

2. Pipe `tbl_mtcars` to `spark_apply()`. Use `nrow` as the function to run

```{r}
tbl_mtcars |> 
  spark_apply(nrow)
```

3. Switch the function to use in `spark_apply()` to `dim`. Notice how it
returns more rows, because coercing the size 2 vector creates a 2 row data frame

```{r}
tbl_mtcars |> 
  spark_apply(dim)
```

## Group by variable
*Write and run simple grouping commands*


1. Go back to using `nrow` again for `spark_apply()`. Remember to 
pass `columns = "x long"`

```{r}
tbl_mtcars |> 
  spark_apply(nrow, columns = "x long")
```

2. Add the `group_by` argument, with the value of `"am"`. There should be an 
error. This is because there are 2 variables in the result, instead of one, and
we defined `x` only in `columns`

```{r}
#| eval: false
tbl_mtcars |> 
  spark_apply(nrow, group_by = "am", columns = "x long")
```

3. Insert `am long,` at the beginning of `columns`

```{r}
tbl_mtcars |> 
  spark_apply(nrow, group_by = "am", columns = "am long, x long")
```
4. To see how the name we pass does not have to match the variable name, change
`am` to `notam` in `columns`

```{r}
tbl_mtcars |> 
  spark_apply(nrow, group_by = "am", columns = "notam long, x long")
```

5. Change the grouping variable to "cyl", make sure to update that in the `columns`
argument as well

```{r}
tbl_mtcars |> 
  spark_apply(nrow, group_by = "cyl", columns = "cyl long, x long")
```


## Custom functions
*Create simple custom functions to send to Spark*

1. In `spark_apply()`, pass `function(x) x` as the function. This will 
return the entire `mtcars` data set 

```{r}
tbl_mtcars |> 
  spark_apply(function(x) x)
```
2. Modify the function to return only the "mpg", "cyl", and "disp" variables

```{r}
tbl_mtcars |> 
  spark_apply(function(x) x[, c("mpg", "cyl", "disp")])
```

3. Add the recommended `columns` spec from 

```{r}
tbl_mtcars |> 
  spark_apply(
    function(x) x[, c("mpg", "cyl", "disp")], 
    columns = "mpg double, cyl double, disp double"
    )
```

4. Make your custom function into a 'multi-line' function

```{r}
tbl_mtcars |> 
  spark_apply(
    function(x){ 
      x[, c("mpg", "cyl", "disp")]
      }, 
    columns = "mpg double, cyl double, disp double"
    )
```

5. Assign the data selection step to a variable called `out`, and then use it as 
the output of the function

```{r}
tbl_mtcars |> 
  spark_apply(
    function(x){ 
      out <- x[, c("mpg", "cyl", "disp")]
      out
      }, 
    columns = "mpg double, cyl double, disp double"
    )
```

6. Add a filter step that returns the highest "mpg". Notice that instead of 1
record, it returns several. That is because the filter is being
processed per partition. 

```{r}
tbl_mtcars |> 
  spark_apply(
    function(x){ 
      out <- x[, c("mpg", "cyl", "disp")]
      out <- out[out$mpg == max(out$mpg), ]
      out
      }, 
    columns = "mpg double, cyl double, disp double"
    )
```


7. Change the filter to display any records with an "mpg" over 25

```{r}
tbl_mtcars |> 
  spark_apply(
    function(x){ 
      out <- x[, c("mpg", "cyl", "disp")]
      out <- out[out$mpg > 25, ]
      out
      }, 
    columns = "mpg double, cyl double, disp double"
    )
```

8. Insert a step that modifies `cyl`. It should make it add 1 to the value. 

```{r}
tbl_mtcars |> 
  spark_apply(
    function(x){ 
      out <- x[, c("mpg", "cyl", "disp")]
      out$cyl <- out$cyl + 1
      out <- out[out$mpg > 25, ]
      out
      }, 
    columns = "mpg double, cyl double, disp double"
    )
```
