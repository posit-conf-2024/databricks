---
title: "Databricks Connect"
execute: 
  eval: true
  freeze: true
  warning: false
---

```{r, setup}
#| include: false

library(dplyr)
library(dbplyr)
library(sparklyr)
library(tidymodels)
library(tidyverse)
```

## Connect to Databricks Connect cluster


```{r}
library(sparklyr)
library(dplyr)
sc <- spark_connect(method = "databricks_connect")
```

## Uploading data from R

```{r}
library(nycflights13)
```

```{r}
tbl_planes <- copy_to(sc, planes)
```

```{r}
tbl_planes |> 
  glimpse()
```

```{r}
tbl_planes |> 
  show_query()
```

## Caching data

```{r}
tbl_diamonds <- tbl(sc, "diamonds")
```

```{r}
tbl_temp <- tbl_diamonds |> 
  select(cut, color, clarity, price) |> 
  compute()
```

```{r}
tbl_temp
```

```{r}
tbl_temp |> 
  show_query()
```

## Reading files

```{r}
tbl_airports <- spark_read_csv(
  sc = sc,
  name = "airports_csv",
  path = "/Volumes/workshops/nycflights/2013/airports.csv"
  )
```

```{r}
tbl_airports
```

```{r}
tbl_airports |> 
  show_query()
```

## "Mapping" files


```{r}
flights_mapped <- spark_read_csv(
  sc = sc,
  name = "mapped",
  path = "/Volumes/workshops/nycflights/2013/nycflights.csv",
  memory = FALSE
  )
```

```{r}
flights_mapped |> 
  glimpse()
```

```{r}
flights_mapped |> 
  count(carrier)
```

```{r}
sdf_sql(sc, "select * from mapped limit 10")
```

## Partial cache

```{r}
flights_ord <- flights_mapped |> 
  filter(dest == "ORD") |> 
  compute()
```

```{r}
flights_ord
```


```{r}
flights_ord |> 
  show_query()
```

## End game

```{r}
flights_mapped |> 
  select(tailnum, dest, origin, distance) |> 
  head() |> 
  left_join(tbl_planes, by = "tailnum")
```

