{
  "hash": "ff2725b5fc6c3a0acf76a1678f6f5d52",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Intro to R UDFs\"\nexecute: \n  eval: true\n  freeze: true\n  warning: true\n---\n\n\n\n\n\n\n## Catch up {.unnumbered}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nlibrary(dplyr)\nsc <- spark_connect(method = \"databricks_connect\")\n#> ! Changing host URL to: https://rstudio-partner-posit-default.cloud.databricks.com\n#> ℹ Retrieving info for cluster:'1026-175310-7cpsh3g8'\n#> ✔ Cluster: '1026-175310-7cpsh3g8' | DBR: '14.1' [432ms]\n#> \n#> ℹ Attempting to load 'r-sparklyr-databricks-14.1'\n#> ✔ Python environment: 'r-sparklyr-databricks-14.1' [991ms]\n#> \n#> ℹ Connecting to '14.1 cluster'\n#> ✔ Connected to: '14.1 cluster' [6ms]\n#> \n```\n:::\n\n\n\n\n## Simple operations\n*Trying out very simple operation to become familiar with the process*\n\n1. Use `copy_to()` to send `mtcars` to the cluster. Load it to a variable called\n`tbl_mtcars`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars <- copy_to(sc, mtcars)\n```\n:::\n\n\n\n\n2. Pipe `tbl_mtcars` to `spark_apply()`. Use `nrow` as the function to run\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow)\n#> Some features are not enabled in this build of Arrow. Run `arrow_info()` for more information.\n#> The repository you retrieved Arrow from did not include all of Arrow's features.\n#> You can install a fully-featured version by running:\n#> `install.packages('arrow', repos = 'https://apache.r-universe.dev')`.\n#> \n#> Attaching package: 'arrow'\n#> The following object is masked from 'package:lubridate':\n#> \n#>     duration\n#> The following object is masked from 'package:utils':\n#> \n#>     timestamp\n#> To increase performance, use the following schema:\n#> columns = \"x long\"\n#> # Source:   table<`sparklyr_tmp_table_867dadbd_1724_452e_a30b_0aea96ec858f`> [4 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2     8\n#> 3     8\n#> 4     8\n```\n:::\n\n\n\n\n3. Switch the function to use in `spark_apply()` to `dim`. Notice how it\nreturns more rows, because coercing the size 2 vector creates a 2 row data frame\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(dim)\n#> To increase performance, use the following schema:\n#> columns = \"x long\"\n#> # Source:   table<`sparklyr_tmp_table_816793c9_b4cc_420b_a8a8_d2c20a5b07e5`> [8 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2    11\n#> 3     8\n#> 4    11\n#> 5     8\n#> 6    11\n#> 7     8\n#> 8    11\n```\n:::\n\n\n\n\n## Group by variable\n*Write and run simple grouping commands*\n\n\n1. Go back to using `nrow` again for `spark_apply()`. Remember to \npass `columns = \"x long\"`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, columns = \"x long\")\n#> # Source:   table<`sparklyr_tmp_table_067200fb_e916_4f85_888b_cb302e55e1d9`> [4 x 1]\n#> # Database: spark_connection\n#>       x\n#>   <dbl>\n#> 1     8\n#> 2     8\n#> 3     8\n#> 4     8\n```\n:::\n\n\n\n\n2. Add the `group_by` argument, with the value of `\"am\"`. There should be an \nerror. This is because there are 2 variables in the result, instead of one, and\nwe defined `x` only in `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"x long\")\n```\n:::\n\n\n\n\n3. Insert `am long,` at the beginning of `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"am long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_4321bb30_d09b_4b23_8f37_6dadd518ce53`> [2 x 2]\n#> # Database: spark_connection\n#>      am     x\n#>   <dbl> <dbl>\n#> 1     0    19\n#> 2     1    13\n```\n:::\n\n\n\n4. To see how the name we pass does not have to match the variable name, change\n`am` to `notam` in `columns`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"am\", columns = \"notam long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_48a8b521_0011_4622_99b8_94b701463acf`> [2 x 2]\n#> # Database: spark_connection\n#>   notam     x\n#>   <dbl> <dbl>\n#> 1     0    19\n#> 2     1    13\n```\n:::\n\n\n\n\n5. Change the grouping variable to \"cyl\", make sure to update that in the `columns`\nargument as well\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(nrow, group_by = \"cyl\", columns = \"cyl long, x long\")\n#> # Source:   table<`sparklyr_tmp_table_7b98282a_0246_4786_b252_4b0cfa037313`> [3 x 2]\n#> # Database: spark_connection\n#>     cyl     x\n#>   <dbl> <dbl>\n#> 1     4    11\n#> 2     6     7\n#> 3     8    14\n```\n:::\n\n\n\n\n\n## Custom functions\n*Create simple custom functions to send to Spark*\n\n1. In `spark_apply()`, pass `function(x) x` as the function. This will \nreturn the entire `mtcars` data set \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(function(x) x)\n#> To increase performance, use the following schema:\n#> columns = \"mpg double, cyl double, disp double, hp double, drat double, wt\n#> double, qsec double, vs double, am double, gear double, carb double\"\n#> # Source:   table<`sparklyr_tmp_table_0f95d585_c5a5_4b6a_b4dd_afeacd7812c5`> [?? x 11]\n#> # Database: spark_connection\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n#> # ℹ more rows\n```\n:::\n\n\n\n2. Modify the function to return only the \"mpg\", \"cyl\", and \"disp\" variables\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(function(x) x[, c(\"mpg\", \"cyl\", \"disp\")])\n#> To increase performance, use the following schema:\n#> columns = \"mpg double, cyl double, disp double\"\n#> # Source:   table<`sparklyr_tmp_table_6d668022_b215_4b66_a904_b8df93f0814c`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n3. Add the recommended `columns` spec from \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x) x[, c(\"mpg\", \"cyl\", \"disp\")], \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_384a32ad_fcb8_468f_886b_142130ee6656`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n4. Make your custom function into a 'multi-line' function\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      x[, c(\"mpg\", \"cyl\", \"disp\")]\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_9ad6389e_7f88_455d_bb44_26373948e69a`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n5. Assign the data selection step to a variable called `out`, and then use it as \nthe output of the function\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_729c736f_c544_4c37_81b4_3067a08c321a`> [?? x 3]\n#> # Database: spark_connection\n#>      mpg   cyl  disp\n#>    <dbl> <dbl> <dbl>\n#>  1  21       6  160 \n#>  2  21       6  160 \n#>  3  22.8     4  108 \n#>  4  21.4     6  258 \n#>  5  18.7     8  360 \n#>  6  18.1     6  225 \n#>  7  14.3     8  360 \n#>  8  24.4     4  147.\n#>  9  22.8     4  141.\n#> 10  19.2     6  168.\n#> # ℹ more rows\n```\n:::\n\n\n\n\n6. Add a filter step that returns the highest \"mpg\". Notice that instead of 1\nrecord, it returns several. That is because the filter is being\nprocessed per partition. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out <- out[out$mpg == max(out$mpg), ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_bfa02dd9_be66_4637_bf95_a699570246f3`> [4 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  24.4     4 147. \n#> 2  22.8     4 141. \n#> 3  33.9     4  71.1\n#> 4  30.4     4  95.1\n```\n:::\n\n\n\n\n\n7. Change the filter to display any records with an \"mpg\" over 25\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out <- out[out$mpg > 25, ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_ec9b86e6_1803_4e7a_90d8_fe9fb7910e9f`> [6 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  32.4     4  78.7\n#> 2  30.4     4  75.7\n#> 3  33.9     4  71.1\n#> 4  27.3     4  79  \n#> 5  26       4 120. \n#> 6  30.4     4  95.1\n```\n:::\n\n\n\n\n8. Insert a step that modifies `cyl`. It should make it add 1 to the value. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(\n    function(x){ \n      out <- x[, c(\"mpg\", \"cyl\", \"disp\")]\n      out$cyl <- out$cyl + 1\n      out <- out[out$mpg > 25, ]\n      out\n      }, \n    columns = \"mpg double, cyl double, disp double\"\n    )\n#> # Source:   table<`sparklyr_tmp_table_7121314e_2bb9_4de1_ba18_6926a6c9cb6a`> [6 x 3]\n#> # Database: spark_connection\n#>     mpg   cyl  disp\n#>   <dbl> <dbl> <dbl>\n#> 1  32.4     5  78.7\n#> 2  30.4     5  75.7\n#> 3  33.9     5  71.1\n#> 4  27.3     5  79  \n#> 5  26       5 120. \n#> 6  30.4     5  95.1\n```\n:::\n\n\n\n\n## R packages\n*Simple example that uses an R package*\n\n1. Load the `broom` package into your R session\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\n```\n:::\n\n\n\n\n2. Create a function that, creates an `lm` model against the one, and only, \nargument passed to the function. Then use `tidy()` to return the results of the\nmodel as a data frame. The `lm()` call should assume that the data will always have\nthe same columns as `mtcars`, and it will create a linear model of the \"mpg\" against\nall the other variables. Name it `model_function`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_function <- function(x) {\n  model <- lm(mpg ~ ., x)\n  tidy(model)\n}\n```\n:::\n\n\n\n\n3. Test `model_function` by passing `mtcars` to it\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_function(mtcars)\n#> # A tibble: 11 × 5\n#>    term        estimate std.error statistic p.value\n#>    <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#>  1 (Intercept)  12.3      18.7        0.657  0.518 \n#>  2 cyl          -0.111     1.05      -0.107  0.916 \n#>  3 disp          0.0133    0.0179     0.747  0.463 \n#>  4 hp           -0.0215    0.0218    -0.987  0.335 \n#>  5 drat          0.787     1.64       0.481  0.635 \n#>  6 wt           -3.72      1.89      -1.96   0.0633\n#>  7 qsec          0.821     0.731      1.12   0.274 \n#>  8 vs            0.318     2.10       0.151  0.881 \n#>  9 am            2.52      2.06       1.23   0.234 \n#> 10 gear          0.655     1.49       0.439  0.665 \n#> 11 carb         -0.199     0.829     -0.241  0.812\n```\n:::\n\n\n\n\n4. Pass `model_function` to `spark_apply()`, against `tbl_mtcars`. The call should\nfail, because `broom` is not explicitly referred to in `model_function`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(model_function)\n```\n:::\n\n\n\n\n5. Modify `model_function` by either adding a `library()` call, or using `::`\nto explicitly refer to `broom` inside it\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_function <- function(x) {\n  model <- lm(mpg ~ ., x)\n  broom::tidy(model)\n}\n```\n:::\n\n\n\n\n6. Test `model_function` again against `tbl_mtcars`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(model_function)\n#> To increase performance, use the following schema:\n#> columns = \"term string, estimate double, std_error double, statistic double,\n#> p_value double\"\n#> # Source:   table<`sparklyr_tmp_table_7147ff59_4759_4f01_a920_3184b005d1f3`> [?? x 5]\n#> # Database: spark_connection\n#>    term        estimate std_error statistic p_value\n#>    <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#>  1 (Intercept) -18.6          NaN       NaN     NaN\n#>  2 cyl           0.517        NaN       NaN     NaN\n#>  3 disp          0.0356       NaN       NaN     NaN\n#>  4 hp           -0.0579       NaN       NaN     NaN\n#>  5 drat          7.98         NaN       NaN     NaN\n#>  6 wt           -1.24         NaN       NaN     NaN\n#>  7 qsec          0.565        NaN       NaN     NaN\n#>  8 vs            2.51         NaN       NaN     NaN\n#>  9 am          NaN            NaN       NaN     NaN\n#> 10 gear        NaN            NaN       NaN     NaN\n#> # ℹ more rows\n```\n:::\n\n\n\n\n7. Add a `group_by` argument, use \"am\" as the grouping variable\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl_mtcars |> \n  spark_apply(model_function, group_by = \"am\")\n#> To increase performance, use the following schema:\n#> columns = \"am double, term string, estimate double, std_error double, statistic\n#> double, p_value double\"\n#> # Source:   table<`sparklyr_tmp_table_b2796b5a_7ab9_4279_b1be_54b3268fb568`> [?? x 6]\n#> # Database: spark_connection\n#>       am term        estimate std_error statistic  p_value\n#>    <dbl> <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#>  1     0 (Intercept)   8.64     21.5        0.402   0.697 \n#>  2     0 cyl          -0.534     1.13      -0.474   0.647 \n#>  3     0 disp         -0.0203    0.0174    -1.16    0.275 \n#>  4     0 hp            0.0622    0.0461     1.35    0.210 \n#>  5     0 drat          0.592     3.01       0.196   0.849 \n#>  6     0 wt            1.95      2.23       0.876   0.404 \n#>  7     0 qsec         -0.884     0.758     -1.17    0.274 \n#>  8     0 vs            0.739     2.51       0.294   0.775 \n#>  9     0 am          NaN       NaN        NaN     NaN     \n#> 10     0 gear          8.65      3.90       2.22    0.0534\n#> # ℹ more rows\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}