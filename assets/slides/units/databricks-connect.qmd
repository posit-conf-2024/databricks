---
engine: knitr
---

```{r}
#| include: false
if(!("no_databricks" %in% ls())) no_databricks <- 1
```

# {background-image="assets/background/content-slide.svg" background-size="1700px" background-color="#2a7070"}

:::{.custom-unit-number}
Unit `r no_databricks`
:::

:::{.custom-unit-title}
Databricks <br/> Connect
:::

## [Databricks Connect]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="50%"}

:::{.custom-smaller}
:::{.incremental1}
- Spark Connect, offers **true** remote connectivity
- Uses **gRPC** to as the communication interface
- **Databricks Connect 'v2'** is based on Spark Connect (DBR 13+)
:::
:::

:::
:::{.column width="45%"}
:::
:::

![](assets/databricks-connect/grpc){.absolute top="264" left="900" width="670"}

## [Databricks Connect]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle .custom-smaller}
`databricks-connect` integrates with gRPC via `pyspark`
:::

![](assets/databricks-connect/python.png){.absolute top="264" left="572" width="998"}

## [Databricks Connect]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.custom-subtitle}
`sparklyr` integrates with `databricks-connect` via `reticulate`
:::

![](assets/databricks-connect/db-connect.png){.absolute top="200" left="70" width="1500"}

## [Why not just use 'reticulate'?]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
`sparklyr` extends functionality and user experience
:::

:::{.columns}
:::{.column width="20%"}
:::
:::{.column width="70%"}
:::{.custom-smaller}
:::{.incremental1}
  - `dplyr` back-end
  - `DBI` back-end
  - R UDFs 
  - **RStudio**, and **Positron**, Connections Pane integration
:::
:::

:::
:::

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

## [Getting started]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.columns}
:::{.column width="42%"}

:::{.custom-smaller}
:::{.incremental1}
- Python 3.10+ 
- A Python environment with `databricks-connect` and its dependencies
- `pysparklyr` extension
:::
:::

:::
:::{.column width="58%"}
:::{.custom-smaller}
<br/>
```r
install.packages("pysparklyr")

library(sparklyr) 

sc <- spark_connect(
 cluster_id = "[cluster's id]",
 method = "databricks_connect"
 )
```
:::
:::
:::

## [Getting started]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.custom-subtitle .custom-smaller}
Automatically, checks for, and installs the Python packages
:::

:::{.columns}
:::{.column width="8%"}
:::
:::{.column width="90%"}
:::{.custom-smallest}
```r
install.packages("pysparklyr")
library(sparklyr) 
sc <- spark_connect(
  cluster_id = "[cluster's id]", 
  method = "databricks_connect"
  )
#> ! Retrieving version from cluster '1026-175310-7cpsh3g8'
#> Cluster version: '14.1' 
#> ! No viable Python Environment was identified for 
#> Databricks Connect version 14.1 
#> Do you wish to install Databricks Connect version 14.1? 
#> 1: Yes
#> 2: No
#> 3: Cancel
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_databricks`.1
:::

## {background-image="assets/background/boxed-white.svg" background-size="1700px" background-color="#fff"}

<br/><br/>

:::{.columns}
:::{.column width="10%"}
:::
:::{.column width="90%"}
:::{.custom-unit-title}
[Understanding Spark's data caching]{style="font-size:130px; line-height: 0.5;"}
:::
:::
:::

## [Spark's data capabilities]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="50%"}
:::{.custom-smaller}
:::{.incremental2}
- Spark has the ability to cache large amounts of data
- Amount of data is limited by the size of the cluster
- Data in cache is the fastest way to access data in Spark
:::
:::
:::
:::{.column width="50%"}
![](assets/databricks-connect/spark-data-2.png)
:::
:::

## [Default approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Data is read and processed. Results go to R.
:::

![](assets/databricks-connect/warehouse-r.png){.absolute top="200" left="220" width="1100"}

## [About this approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.incremental1}
- [Well suited when exploring the entirety of the data. Usually to find relevant variables]{style="font-size:75px;"}
- [Not efficient when accessing the same fields and rows over and over]{style="font-size:75px;"}

:::

## [Uploading data from R]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle .custom-smaller}
`copy_to()` to upload data to Spark
:::

![](assets/databricks-connect/r-ram.png){.absolute top="200" left="220" width="1100"}


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_databricks`.2
:::

## [Caching data]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
2 step process. first, cache all or some data in memory
:::

![](assets/databricks-connect/warehouse-ram.png){.absolute top="200" left="220" width="1100"}

## [Caching data]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Second, read and process from memory. *Much faster*
:::

![](assets/databricks-connect/ram-r.png){.absolute top="200" left="220" width="1100"}

## [About this approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
:::{.incremental1}
- [Well suited when accessing the same fields and rows over and over]{style="font-size:75px;"}
- [Best when running models and UDFs *(More about this later)*]{style="font-size:75px;"}

:::
:::


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_databricks`.3
:::

## [Reading files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
By default, files are read and saved to memory
:::

![](assets/databricks-connect/files-ram.png){.absolute top="200" left="220" width="1100"}

## [Reading files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Afterwards, the data is read from memory for processing
:::

![](assets/databricks-connect/ram-r.png){.absolute top="200" left="220" width="1100"}

## [About this approach]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-closer}
:::{.incremental1}
- Read files using the `spark_read...` family of functions
- The file path needs to be relative to your Databricks environment
- [Databricks Volumes](https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html) are ideal for this approach
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_databricks`.4
:::

## ["Mapping" files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
The files can be mapped but not imported to memory
:::

![](assets/databricks-connect/files-map.png){.absolute top="200" left="220" width="1100"}

## ["Mapping" files]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Data is read and processed. Results sent to R.
:::


![](assets/databricks-connect/files-r.png){.absolute top="200" left="220" width="1100"}

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_databricks`.5
:::

## [Partial cache]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Alternatively, you can cache specific data from the files
:::


![](assets/databricks-connect/files-ram-partial.png){.absolute top="200" left="220" width="1100"}

## [Partial cache]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Afterwards, the data is read from memory for processing
:::

![](assets/databricks-connect/ram-r.png){.absolute top="200" left="220" width="1100"}

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_databricks`.6
:::

## [Very large files, read or map]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-closer}
:::{.incremental1}
- [Reading, you "pay" in time at the beginning]{style="font-size:65px;"}
- [Mapping, you "pay" in time as you access the data]{style="font-size:65px;"}
- [Extended EDA, reading would be better]{style="font-size:65px;"}
- [EDA of targeted data, partial caching would be better]{style="font-size:65px;"}
- [Jobs that pull a predetermined set of data, mapping would be better]{style="font-size:65px;"}
:::
:::

## [End game]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Combine the data from any approach. Cache the resulting table
:::

![](assets/databricks-connect/my-data-set.png){.absolute top="270" left="100" width="1250"}

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_databricks`.7
:::
