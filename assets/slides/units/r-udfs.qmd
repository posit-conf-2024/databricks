---
engine: knitr
---

```{r}
#| include: false
if(!("no_udfs" %in% ls())) no_udfs <- 1
```

# {background-image="assets/background/content-slide.svg" background-size="1700px" background-color="#2a7070"}

:::{.custom-unit-number}
Unit `r no_udfs`
:::

:::{.custom-unit-title}
Intro to <br/> 
R UDFs
:::

 <br/> <br/>

## [What is an UDF?]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.incremental}
- Stands for **"User Defined Function"**
- Enables operations not built-in Spark
- Can be written in Scala, Python, or R
:::

## [Ok, but what does that mean?!]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

## [Ok, but what does that mean?!]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

[
<br/><br/>
We can run R code *inside* Spark! ðŸŽ‰ ðŸŽ‰
]{style="font-size:100px;line-height:0.5;font-weight:600;color:#579297;"}

## [But first... parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Spark partitions the data logically
:::

![](assets/r-udfs/spark-1.png){.absolute top="200" left="400" width="400"}
![](assets/r-udfs/spark-2.png){.absolute top="190" left="900" width="400"}
![](assets/r-udfs/spark-3.png){.absolute top="390" left="900" width="400"}
![](assets/r-udfs/spark-4.png){.absolute top="590" left="900" width="400"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Each node gets one, or several partitions
:::

![](assets/r-udfs/spark-6.png){.absolute top="250" left="320" width="200"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="100" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="420" left="100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="250" left="1320" width="200"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="1100" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="420" left="1100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="1180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="500" left="770" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="550" left="550" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="670" left="550" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="800" left="660" width="200"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
The cluster runs jobs that process each partition in parallel
:::

![](assets/r-udfs/spark-6.png){.absolute top="250" left="320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="293" left="90" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="100" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="420" left="100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="250" left="1320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="413" left="1090" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="1100" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="420" left="1100" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="1180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="500" left="770" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="542" left="540" width="220"}
![](assets/r-udfs/spark-3.png){.absolute top="550" left="550" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="670" left="550" width="200"}
![](assets/r-udfs/spark.svg){.absolute top="800" left="660" width="200"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Your R function runs on each partition independently
:::

![](assets/r-udfs/spark-6.png){.absolute top="250" left="320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="293" left="90" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="100" width="200"}
![](assets/r-udfs/spark-3.png){.absolute top="420" left="100" width="200"}
![](assets/r-udfs/r-logo.png){.absolute top="310" left="150" width="100"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="250" left="1320" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="413" left="1090" width="220"}
![](assets/r-udfs/spark-2.png){.absolute top="300" left="1100" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="420" left="1100" width="200"}
![](assets/r-udfs/r-logo.png){.absolute top="430" left="1150" width="100"}
![](assets/r-udfs/spark.svg){.absolute top="550" left="1180" width="200"}

![](assets/r-udfs/spark-6.png){.absolute top="500" left="770" width="200"}
![](assets/r-udfs/spark-5.png){.absolute top="542" left="540" width="220"}
![](assets/r-udfs/spark-3.png){.absolute top="550" left="550" width="200"}
![](assets/r-udfs/spark-4.png){.absolute top="670" left="550" width="200"}
![](assets/r-udfs/r-logo.png){.absolute top="560" left="590" width="100"}
![](assets/r-udfs/spark.svg){.absolute top="800" left="660" width="200"}

## [Parallel processing]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Results are appended, and returned to user as a single table
:::

![](assets/r-udfs/spark-5.png){.absolute top="190" left="575" width="450" height="620"}

![](assets/r-udfs/spark-2.png){.absolute top="210" left="600" width="400"}
![](assets/r-udfs/spark-3.png){.absolute top="400" left="600" width="400"}
![](assets/r-udfs/spark-4.png){.absolute top="590" left="600" width="400"}



## [Accessing in R]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.columns}
:::{.column width="50%"}

:::{.custom-smaller}
- `spark_apply()` Enables access to the R run-time installed in the cluster
- The R function will run over each individual partition
:::

:::
:::{.column width="50%"}
:::{.custom-smaller}
```r
tbl_mtcars <- copy_to(sc, 
                      mtcars)

tbl_mtcars |> 
  spark_apply(nrow)
#>       x
#>   <dbl>
#> 1     8
#> 2     8
#> 3     8
#> 4     8
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_udfs`.1
:::

## [Group by variable]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}


:::{.custom-subtitle}
Use `group_by` to override the partitions, and divide data by a column
:::

:::{.custom-smaller}

```r
tbl_mtcars |> 
  spark_apply(nrow, group_by = "am")
#> # Source:   table<`sparklyr_tmp_table`> [2 x 2]
#> # Database: spark_connection
#>      am     x
#>   <dbl> <dbl>
#> 1     0    19
#> 2     1    13
```
:::


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_udfs`.2
:::


## [Custom functions]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/r-udfs/expectations.png){.absolute top="250" left="50" width="1800"}

:::{.custom-subtitle}
R function should expect and return a table
:::

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}


## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_udfs`.3
:::

## [R packages]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.custom-closer}
:::{.custom-subtitle .custom-smaller}
R packages have to be referenced from within
your function's code using `library()` or `::`
:::

:::{.columns}
:::{.column width="8%"}
:::
:::{.column width="90%"}
:::{.custom-smallest}
```r
tbl_mtcars |> 
  spark_apply(function(x) {
      library(broom)
      model <- lm(mpg ~ ., x)
      tidy(model)[1,]
      }, 
      group_by = "am"
      )
#> # Source:   table<`sparklyr_tmp`> [2 x 6]
#> # Database: spark_connection
#>      am term        estimate std_error statistic p_value
#>   <dbl> <chr>          <dbl>     <dbl>     <dbl>   <dbl>
#> 1     0 (Intercept)     8.64      21.5     0.402   0.697
#> 2     1 (Intercept)  -138.        69.1    -1.99    0.140
```
:::
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_udfs`.4
:::

## [Dependencies in Databricks]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle .custom-smaller}
Spark clusters are tied to a Databricks Runtime version. Each
version of DBR contains [specific versions](https://docs.databricks.com/en/release-notes/runtime/15.3.html#system-environment) of:
:::

:::{.columns}
:::{.column width="25%"}
:::
:::{.column width="70%"}
:::{.custom-smaller .custom-closer}
- Spark
- Java
- Scala
- Python
- R
- Set of Python and R packages
:::
:::
:::

:::{.footer}
https://docs.databricks.com/en/release-notes/runtime/15.3.html#installed-r-libraries
:::

## [Missing packages]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="45%"}
:::{.custom-smaller .custom-closer}
- Additional packages can be installed 
- In the Databricks portal, use the *Libraries* tab of the cluster
- Python packages will install from PyPi
- R packages will install from CRAN
:::
:::
:::{.column width="55%"}
![](assets/r-udfs/install.png)
:::
:::

## [Key considerations]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.incremental}
:::{.custom-smaller}
- You are dealing with **two** different environments
- Your Python version **must** match the cluster
- R version can be different, but could be source of errors 
- Same for Python, and R packages. OK if mismatch, but could be errors
:::
:::

## [Key considerations]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
An example of different R versions
:::

:::{.columns}
:::{.column width="15%"}
:::
:::{.column width="80%"}
:::{.custom-smaller}
```{r}
#| eval: false
#| echo: true

my_func <- function(x) R.Version()$minor
# Local output ------------------------
my_func(mtcars)
#> [1] "4.0"
# Remote output ----------------------
tbl_mtcars |> 
  spark_apply(my_func) |> 
  head(1)
#>   x    
#>   <chr>
#> 1 3.1
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_udfs`.5
:::

## {background-image="assets/background/boxed-white.svg" background-size="1700px" background-color="#fff"}

<br/><br/>

:::{.columns}
:::{.column width="15%"}
:::
:::{.column width="70%"}
:::{.custom-unit-title}
Solving for R UDFs
:::
:::
:::

## {background-image="assets/background/boxed-white.svg" background-size="1700px" background-color="#fff"}

<br/><br/>

:::{.columns}
:::{.column width="15%"}
:::
:::{.column width="70%"}
:::{.custom-unit-title}
Solving for R UDFs
:::
It wasn't easy...
:::
:::

## [Problem]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

[
<br/>
Spark Connect does not support R UDFs natively
]{style="font-size:77px;line-height:0.5;font-weight:600;color:#579297;"}

## [Problem]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

[
<br/>
Spark Connect does not support R UDFs natively
]{style="font-size:77px;line-height:0.5;font-weight:600;color:#579297;"}

[
(Databricks Connect v2, is based on Spark Connect)
]{style="font-size:73px;line-height:0.5;font-weight:200;color:#999;"}

## [In Python]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/r-udfs/python-udfs.png){.absolute top="270" left="50" width="1800"}

:::{.custom-subtitle}
Code sent through gRPC, code runs in remote Python environment through Spark
:::

## [R UDFs]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

![](assets/r-udfs/r-udfs.png){.absolute top="230" left="50" width="1800"}

:::{.custom-subtitle}
R code is inserted in a Python script, `rpy2` executes
:::

:::{.footer}
https://spark.posit.co/deployment/databricks-connect-udfs.html
:::

## [Wrapped Python functions]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}
![](assets/posit-databricks.png){.absolute top="-10" left="1430" width="180"}

:::{.custom-smaller}

R: 

```r
spark_apply(tbl, my_fun) 
```

Python:

```python
tbl.mapInPandas(r_objects.r(my_fun))
```

:::

<hr>

:::{.custom-smaller}
R:

```r
spark_apply(tbl, my_fun, group_by = "col1") 
```

Python:

```python
tbl.applyInPandas.group_by(col1).(r_objects.r(my_fun))
```
:::

<hr>


