---
engine: knitr
---

```{r}
#| include: false
if(!("no_modeling" %in% ls())) no_modeling <- 1
```

# {background-image="assets/background/content-slide.svg" background-size="1700px" background-color="#2a7070"}

:::{.custom-unit-number}
Unit `r no_modeling`
:::

:::{.custom-unit-title}
Modeling <br/> with R UDFs
:::

<br/><br/>

## [Run R models in Spark]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/modeling/topology.png){.absolute top="120" left="210" width="1200"}

## [1. Sample - Use *dplyr*]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/remote-processing/dplyr.png){.absolute top="0" left="1300" width="150"}
![](assets/remote-processing/dbplyr.png){.absolute top="0" left="1450" width="150"}



:::{.columns}
:::{.column width="40%"}

:::{.custom-closer}
:::{.incremental1}
- Use `dplyr`'s `slice_sample()` function
- Download enough data to be representative

:::
:::

:::
:::{.column width="60%"}
<br/><br/>

```r
local_df <- remote_df |>
 slice_sample(n = 100) |>
 collect()
```

:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_modeling`.1
:::

## [2. Train - Ideally, *tidymodels*]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/modeling/tidymodels.png){.absolute top="0" left="1450" width="150"}

:::{.custom-subtitle}
Fit the model locally in R
:::

:::{.columns}
:::{.column width="10%"}
:::
:::{.column width="90%"}
:::{.custom-smaller}
```r
library(tidymodels)
my_rec <- recipe(x ~ ., data = local_df) |> 
  step_normalize(all_numeric_predictors()) 
  
my_reg <- linear_reg()

my_wkf <- workflow() |> 
  add_model(my_reg) |> 
  add_recipe(my_rec)

model <- fit(my_wkf, local_df)
```
:::
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_modeling`.2
:::

## [3. Publish - Package with *vetiver*]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/modeling/vetiver.png){.absolute top="0" left="1450" width="150"}

:::{.columns}
:::{.column width="45%"}

:::{.custom-smaller}
:::{.incremental1}
- `vetiver` packages `tidymodels` workflows and models
- Model is automatically reduced in size
- Output is easy to load into another R session
:::
:::

:::
:::{.column width="55%"}
<br/>
```r
library(vetiver)
m <- vetiver_model(
 model = model,
 model_name = “model”
 )
saveRDS(m, “model.rds”)
```

:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_modeling`.3
:::

## [3. Publish - Transfer model file]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/modeling/uc-volume.png){.absolute top="90" left="800" width="680"}

:::{.columns}
:::{.column width="45%"}

:::{.custom-smaller}
:::{.incremental1}
- Copy the R model where Spark can access
- Location **has** to be secure
- Options: Databricks UC, s3 buckets, Posit Connect, others
:::
:::

:::
:::{.column width="10%"}

:::
:::

## [3. Publish - Databricks UC Volume]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/modeling/upload.png){.absolute top="120" left="70" width="1500"}

## [4. Predict - Create the function]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
It needs to read the model file, runs prediction, and process results
:::

:::{.columns}
:::{.column width="13%"}
:::
:::{.column width="80%"}
```r
my_function <- function(x) {
  library(workflows)
  library(vetiver)
  model <- readRDS("model.rds")
  preds <- predict(model, x)
  x$pred <- preds[,1][[1]]
  x
}
```
:::
:::


## [4. Predict - Test the function locally]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Good practice to test the function with local data
:::

<br/>

:::{.columns}
:::{.column width="25%"}
:::
:::{.column width="60%"}
```r
my_function(local_df)
```
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_modeling`.4
:::

## [4. Predict - Switch to published]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle .custom-smaller}
Function to use the copy of the model in Databricks
:::

:::{.custom-smaller}
```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "|5|4-8"
my_function <- function(x) {
  library(workflows)
  library(vetiver)
  if(file.exists("model.rds")) {
    model <- readRDS("model.rds")
  } else {
    model <- readRDS("/Volumes/[catalog]/[schema]/[volume]/model.rds")
  }
  preds <- predict(model, x)
  x$pred <- preds[,1][[1]]
  x
}
```
:::

## [4. Predict - Test remotely]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Use a sample or top rows to test the function in Spark
:::

<br/>

:::{.columns}
:::{.column width="20%"}
:::
:::{.column width="80%"}
```r
remote_df |> 
  head() |> 
  spark_apply(my_function)
```
:::
:::

## [4. Predict - Run predictions]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.custom-subtitle}
Run full set, make sure results are saved
:::

<br/>

:::{.columns}
:::{.column width="10%"}
:::
:::{.column width="80%"}
```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "|1|3"
pred_df <- remote_df |> 
  spark_apply(my_function) |> 
  compute()
```
:::
:::

## {background-image="assets/background/boxed-green.svg" background-size="1700px" background-color="#799857"}

:::{.custom-exercise}
Exercise `r no_modeling`.5
:::

## [4. Predict]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

:::{.columns}
:::{.column width="25%"}
:::
:::{.column width="75%"}
:::{.custom-closer}
:::{.incremental1}
1. [Create the function]{style="font-size:75px;"}
1. [Test the function locally]{style="font-size:75px;"}
1. [Switch to published model]{style="font-size:75px;"}
1. [Test function remotely]{style="font-size:75px;"}
1. [Run predictions]{style="font-size:75px;"}
:::
:::
:::
:::

## [Run R models in Spark]{style="color:#666;"} {background-image="assets/background/slide-light.svg" background-size="1700px" background-color="white"}

![](assets/modeling/topology.png){.absolute top="120" left="210" width="1200"}
